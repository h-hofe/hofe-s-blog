{"meta":{"title":"Hofe's blog","subtitle":"","description":"个人博客","author":"hofe","url":"http://hofe.work","root":"/"},"pages":[{"title":"关于","date":"2020-04-03T09:41:12.000Z","updated":"2020-05-18T03:33:42.019Z","comments":false,"path":"about/index.html","permalink":"http://hofe.work/about/index.html","excerpt":"","text":"Hofe 我 想 用 砂 砾 和 泥 土 堆 出 山 川 立志要用自己的方式推动社会进步的青年。摸索、实现的过程可能很漫长，最后也可能会无疾而终，但一定要时刻激励自己朝着这个目标前进。 联系方式 Email: hhf.work@foxmail.com Github: https://github.com/hhf443 CSDN: https://blog.csdn.net/qq_41011723 技能 掌握HTML、CSS、JavaScript、Vue等前端开发技术; 掌握Java语法，多线程、集合，JVM内存模型以及垃圾回收机制等知识; 掌握Spring，SpringMVC，MyBatis等开源框架技术; 熟悉SpringBoot+Dubbo+zookeeper微服务架构; 掌握计算机网络，熟悉TCP/IP协议、HTTP协议; 掌握MySql数据库及其优化，了解Oracle; 了解Redis基本数据类型，了解高并发下缓存穿透缓存雪崩解决方案; 了解Linux常用命令及阿里云部署项目; 了解Python基础，一些常见第三方库如numpy、pandas、requests，以及Web框架如Django、Flask; 熟悉Idea，Git，Maven，Docker等开发工具、项目管理和项目构建工具; 具有良好的编程习惯以及技术文档编写习惯，拥有个人技术博客(http://hhf443.github.io)； 技术栈 HTML+CSS+JSVue React Spring springbootMybatis HibernateDubbo+zookeeper SpringCloud Flask Django爬虫 数据分析 深度学习"},{"title":"相册","date":"2020-04-03T09:45:19.000Z","updated":"2020-04-04T08:31:35.402Z","comments":false,"path":"album/index.html","permalink":"http://hofe.work/album/index.html","excerpt":"","text":"作品 做过的一些项目 设计 看见过的好设计 生活 关于生活 旅行 遇见世界 美图 收藏图片 摄影 记录美好"},{"title":"分类","date":"2020-04-02T05:02:28.000Z","updated":"2020-04-04T12:01:21.745Z","comments":false,"path":"categories/index.html","permalink":"http://hofe.work/categories/index.html","excerpt":"","text":""},{"title":"留言板","date":"2020-04-03T09:44:21.000Z","updated":"2020-04-04T12:01:39.163Z","comments":true,"path":"messageboard/index.html","permalink":"http://hofe.work/messageboard/index.html","excerpt":"","text":"有什么问题可以留言~14 14.不一定会回就是了^_^"},{"title":"标签","date":"2020-04-02T05:01:27.000Z","updated":"2020-04-04T12:00:56.106Z","comments":false,"path":"tags/index.html","permalink":"http://hofe.work/tags/index.html","excerpt":"","text":""},{"title":"??","date":"2020-04-03T12:44:55.000Z","updated":"2020-04-04T08:16:29.275Z","comments":false,"path":"album/design/index.html","permalink":"http://hofe.work/album/design/index.html","excerpt":"","text":""},{"title":"生活","date":"2020-04-03T12:44:55.000Z","updated":"2020-04-04T08:16:56.872Z","comments":false,"path":"album/life/index.html","permalink":"http://hofe.work/album/life/index.html","excerpt":"","text":""},{"title":"摄影","date":"2020-04-03T12:44:55.000Z","updated":"2020-04-04T08:17:51.530Z","comments":false,"path":"album/photograph/index.html","permalink":"http://hofe.work/album/photograph/index.html","excerpt":"","text":""},{"title":"旅行","date":"2020-04-03T12:44:55.000Z","updated":"2020-04-04T08:28:00.578Z","comments":false,"path":"album/travel/index.html","permalink":"http://hofe.work/album/travel/index.html","excerpt":"","text":""},{"title":"美图","date":"2020-04-03T12:44:55.000Z","updated":"2020-04-04T08:27:56.452Z","comments":false,"path":"album/wallpaper/index.html","permalink":"http://hofe.work/album/wallpaper/index.html","excerpt":"","text":""},{"title":"作品","date":"2020-04-03T12:44:55.000Z","updated":"2020-04-04T08:28:01.998Z","comments":false,"path":"album/works/index.html","permalink":"http://hofe.work/album/works/index.html","excerpt":"","text":""}],"posts":[{"title":"进制转化题目总结","slug":"数据结构与算法/进制转换题目总结","date":"2020-06-07T15:33:11.000Z","updated":"2020-06-06T15:34:10.203Z","comments":true,"path":"2020/06/07/数据结构与算法/进制转换题目总结/","link":"","permalink":"http://hofe.work/2020/06/07/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/","excerpt":"","text":"二进制中1的个数 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 假设n为13二进制为 1011 1011 1011 1011 0001左移0010左移0100左移1000 0001 0010 0000 1000 这样可求得1的个数 public class Solution &#123; public int NumberOf1(int n) &#123; int flag = 1; int count = 0; while(flag != 0)&#123; //flag == 0表示遍历完int类型二进制表示的所有位了 if((n &amp; flag)!=0)&#123; count++; &#125; flag = flag &lt;&lt; 1; &#125; return count; &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"进制","slug":"进制","permalink":"http://hofe.work/tags/%E8%BF%9B%E5%88%B6/"}]},{"title":"微预约小程序","slug":"项目/微预约小程序","date":"2020-06-05T09:13:45.000Z","updated":"2020-06-06T15:49:45.937Z","comments":true,"path":"2020/06/05/项目/微预约小程序/","link":"","permalink":"http://hofe.work/2020/06/05/%E9%A1%B9%E7%9B%AE/%E5%BE%AE%E9%A2%84%E7%BA%A6%E5%B0%8F%E7%A8%8B%E5%BA%8F/","excerpt":"","text":"需求分析预约页：用户在预约页可选择具体要办理的业务类型 业务详情页：客户在预约首页选择需要办理的具体业务后，将会跳转至该业务详情页，显示已预约的人数、余号数量等。此外，用户根据自身需求，输入身份证号、手机号后、时间提交申请即可预约成功。 取号：具有预约取号和现场取号两大功能。现场取号功能根据选择的业务类型，提交预约后即可得到办事编号；预约取号时要求输入身份证号码，和预约时输入的身份证号比对，正确则得到办事编号 服务器端数据库设计详细设计减少余号数量需要注意不能低于0 API接口 预约页 URL HTTP 功能 /bussiness/selectOne/ POST 获取id为serviceId的业务类型 /bussiness/ /order/insertOne 开发过程出现的问题tabBar不显示问题欢迎页是pages/index/index，但list的第一项不是pages/index/index就会tabBar不显示 \"pages\":[ \"pages/index/index\", \"pages/reserve/index\", \"pages/take/index\", \"pages/record/index\", \"pages/logs/logs\" ],\"tabBar\": &#123; \"list\": [&#123; \"pagePath\": \"pages/reserve/index\", \"text\": \"预约\", \"iconPath\": \"icons/reserve_index.png\", \"selectedIconPath\": \"icons/reserve_index.png\" &#125;,&#123; \"pagePath\": \"pages/take/index\", \"text\": \"取号\", \"iconPath\": \"icons/take_index.png\", \"selectedIconPath\": \"icons/take_index.png\" &#125;,&#123; \"pagePath\": \"pages/record/index\", \"text\": \"预约历史\", \"iconPath\": \"icons/record_index.png\", \"selectedIconPath\": \"icons/record_index.png\" &#125;] IView的button组件无法绑定submit事件&lt;view&gt; &lt;form bindsubmit=\"formSubmit\"&gt; &lt;i-panel title=\"预约信息\"&gt; &lt;i-input value=\"&#123;&#123; IDcard &#125;&#125;\" name=\"IDcard\" type=\"number\" title=\"身份证号\" mode=\"wrapped\" placeholder=\"请输入身份证号\" /&gt; &lt;i-input value=\"&#123;&#123; phone &#125;&#125;\" name=\"phone\" type=\"number\" title=\"联系电话\" mode=\"wrapped\" placeholder=\"请输入手机号\" /&gt; &lt;i-input value=\"&#123;&#123; phone &#125;&#125;\" name=\"reserveTime\" type=\"number\" title=\"预约时间\" mode=\"wrapped\" placeholder=\"如2020-06-05 15:30\" /&gt; &lt;/i-panel&gt; &lt;i-button formType=\"submit\" type=\"info\" shape=\"circle\"&gt;提交&lt;/i-button&gt; &lt;!-- &lt;button form-type=\"submit\" type=\"info\" shape=\"circle\"&gt;提交&gt;&lt;/button&gt; --&gt; &lt;/form&gt;&lt;/view&gt; 第8行是iView中的组件按钮，点击的时候无法提交表单","categories":[{"name":"项目","slug":"项目","permalink":"http://hofe.work/categories/%E9%A1%B9%E7%9B%AE/"},{"name":"小程序","slug":"项目/小程序","permalink":"http://hofe.work/categories/%E9%A1%B9%E7%9B%AE/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}],"tags":[{"name":"小程序","slug":"小程序","permalink":"http://hofe.work/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"}]},{"title":"ArrayList实现原理及源码阅读","slug":"Java/ArrayList实现原理及源码阅读","date":"2020-05-30T06:47:53.000Z","updated":"2020-05-30T07:37:18.296Z","comments":true,"path":"2020/05/30/Java/ArrayList实现原理及源码阅读/","link":"","permalink":"http://hofe.work/2020/05/30/Java/ArrayList%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"","text":"扩容 发生在Add方法中，首先会调用ensureCapacityInternal确保size+1的位置可以存放元素，不够的话就调用ensureExplicitCapacity进行扩容，首先将原元素数组的长度增大1.5倍（oldCapacity + (oldCapacity &gt;&gt; 1)），然后对扩容后的容量与minCapacity进行比较：① 新容量小于minCapacity，则将新容量设为minCapacity；②新容量大于minCapacity，则指定新容量。最后将旧数组拷贝到扩容后的新数组中。 线程不安全的原因 对ArrayList进行添加元素的操作的时候是分两个步骤进行的，即第一步先在object[size]的位置上存放需要添加的元素；第二步将size的值增加1。由于这个过程在多线程的环境下是不能保证具有原子性的，因此ArrayList在多线程的环境下是线程不安全的。 通常有两种解决办法：第一，使用synchronized关键字；第二，可以用Collections类中的静态方法synchronizedList()，对ArrayList进行调用即可。 public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; // 版本号 private static final long serialVersionUID = 8683452581122892189L; // 缺省容量,当ArrayList的构造方法中没有显示指出ArrayList的数组长度时，类内部使用默认缺省时对象数组的容量大小，为10。 private static final int DEFAULT_CAPACITY = 10; // 空对象数组，当ArrayList的构造方法中显示指出ArrayList的数组长度为0时，类内部将EMPTY_ELEMENTDATA 这个空对象数组赋给elemetData数组。 private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; // 缺省空对象数组，当ArrayList的构造方法中没有显示指出ArrayList的数组长度时，类内部使用默认缺省时对象数组为DEFAULTCAPACITY_EMPTY_ELEMENTDATA。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; // 元素数组，ArrayList的底层数据结构，只是一个对象数组，用于存放实际元素，并且被标记为transient，也就意味着在序列化的时候此字段是不会被序列化的。 transient Object[] elementData; // 实际元素大小，默认为0 private int size; // 最大数组容量 private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;&#125; 构造函数ArrayList()无参构造函数。将DEFAULTCAPACITY_EMPTY_ELEMENTDATA赋值给elementData ArrayList(int initialCapacity)：initialCapacity &gt; 0则 新建一个object对象数组，并赋值给elementData；等于0则将EMPTY_ELEMENTDATA赋值给elementData ArrayList(Collection&lt;? extends E&gt; c) 将传进来的c转化为object类型赋值给elementData，如果c为空则将EMPTY_ELEMENTDATA赋值给elementData // ArrayList带容量大小的构造函数。 public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; // 初始容量大于0 this.elementData = new Object[initialCapacity]; // 初始化元素数组 &#125; else if (initialCapacity == 0) &#123; // 初始容量为0 this.elementData = EMPTY_ELEMENTDATA; // 为空对象数组 &#125; else &#123; // 初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125; // ArrayList无参构造函数。默认容量是10。 public ArrayList() &#123; // 无参构造函数，设置元素数组为空 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; // 创建一个包含collection的ArrayList public ArrayList(Collection&lt;? extends E&gt; c) &#123; // 集合参数构造函数 elementData = c.toArray(); // 转化为数组 if ((size = elementData.length) != 0) &#123; // 参数为非空集合 if (elementData.getClass() != Object[].class) // 是否成功转化为Object类型数组 elementData = Arrays.copyOf(elementData, size, Object[].class); // 不为Object数组的话就进行复制 &#125; else &#123; // 集合大小为空，则设置元素数组为空 this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; add方法（扩容机制）ensureCapacityInternal确保size+1的位置可以存放元素，不够的话就进行扩容 public boolean add(E e) &#123; // 添加元素 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; ArrayList的扩容主要发生在向ArrayList集合中添加元素的时候。由add()方法的分析可知添加前必须确保集合的容量能够放下添加的元素。主要经历了以下几个阶段： 第一，在add()方法中调用ensureCapacityInternal(size + 1)方法来确定集合确保添加元素成功的最小集合容量minCapacity的值。参数为size+1，代表的含义是如果集合添加元素成功后，集合中的实际元素个数。在ensureCapacityInternal方法中，首先判断elementData是否为默认的空数组，如果是，minCapacity为minCapacity与集合默认容量大小中的较大值。 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 判断元素数组是否为空数组 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); // 取较大值 &#125; ensureExplicitCapacity(minCapacity);&#125; 第二，调用ensureExplicitCapacity(minCapacity)方法来确定集合为了确保添加元素成功是否需要对现有的元素数组进行扩容。首先将结构性修改计数器加一；然后判断minCapacity与当前元素数组的长度的大小，如果minCapacity比当前元素数组的长度大的时候需要扩容，进入第三阶段 private void ensureExplicitCapacity(int minCapacity) &#123; // 结构性修改加1 modCount++; if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125; 第三，如果需要对现有的元素数组进行扩容，则调用grow(minCapacity)方法，参数minCapacity表示集合为了确保添加元素成功的最小容量。在扩容的时候，首先将原元素数组的长度增大1.5倍（oldCapacity + (oldCapacity &gt;&gt; 1)），然后对扩容后的容量与minCapacity进行比较：① 新容量小于minCapacity，则将新容量设为minCapacity；②新容量大于minCapacity，则指定新容量。最后将旧数组拷贝到扩容后的新数组中。 private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; // 旧容量 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 新容量为旧容量的1.5倍 if (newCapacity - minCapacity &lt; 0) // 新容量小于参数指定容量，修改新容量 newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) // 新容量大于最大容量 newCapacity = hugeCapacity(minCapacity); // 指定新容量 // 拷贝扩容 elementData = Arrays.copyOf(elementData, newCapacity);&#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"List","slug":"Java/List","permalink":"http://hofe.work/categories/Java/List/"}],"tags":[{"name":"ArrayList","slug":"ArrayList","permalink":"http://hofe.work/tags/ArrayList/"}]},{"title":"时间序列预测","slug":"项目/时间序列预测","date":"2020-05-28T12:13:45.000Z","updated":"2020-06-06T15:54:36.462Z","comments":true,"path":"2020/05/28/项目/时间序列预测/","link":"","permalink":"http://hofe.work/2020/05/28/%E9%A1%B9%E7%9B%AE/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B/","excerpt":"","text":"摘要本项目是基于时间序列算法的Web应用，通过上传数据集，选取时间序列算法，设定参数来完成数据的预测。系统分为使用Vue搭建的前端以及使用Java SpringBoot框架搭建的后端、使用Python实现算法。Java后端接收前端请求，后端调用相应算法进行计算，Java与Python之间通过RPC进行通信。算法部分使用statsmodels.tsa库下的一系列时间序列预测函数，如AR、ARMA、ARIMA等方法。对模型预测结果的评估采用了平均绝对误差、标准差、均方根误差来处理。 本文档侧重描述了ARMA、ARIMA算法的实现原理，如关于数据集的特征工程以及算法实现步骤、每个步骤对应的可视化。其余算法不做过多介绍，源码中会有注释说明。由于侧重点在于数据分析方法，所以没有对Web搭建过程做过多的介绍，Web应用的演示放在了实验结果可视化部分。 1 前言现代经济与生产活动中，往往需要预测未来某个时刻某一指标的值，从而更好地控制、优化经济与生产过程，时间序列分析应运而生。 时间序列是按照时间顺序取得的一系列观测值，时间序列典型的一个本质特征就是相邻观测值的依赖性。时间序列分析所论及的就是对这种依赖性进行分析的技巧，根据系统的有限长度的系列观测值，建立能够比较精确地反映序列中所包含的动态依存关系的数学模型，并借以对系统的未来进行预报。这种分析方法简单易行，便于掌握，但准确性差，一般只适用于短期预测。 时间序列包括：AR(自回归模型)，AR ( p) ，p阶的自回归模型 MA(移动平均模型)，MA(q)，q阶的移动平均模型 ARIMA(差分自回归移动平均模型) AR,MA,ARMA都是运用于原始数据是平稳的时间序列。ARIMA运用于原始数据差分后是平稳的时间序列。 2 问题描述对数据进行时间序列分析之前需要先完成数据预处理，包括缺失值的填充，异常值的删除等特征工程。之后选出符合要求的数据进行数据分析，如ARIMA比较适合预测有季节性特点的数据集；波动不大的数据就可以考虑使用ARMA这类算法。其次，使用ARMA、ARIMA算法的时候需要进行平稳性检验，非平稳数据通过一阶差分或多阶差分进行处理。最后，合理选择预测天数也是很重要的一点，算法对一两天的数据可以达到较高准确率，预测更久远的数据会增大误差。 3 数据预处理和特征工程以pollution.csv数据集为例，介绍数据的预处理 3.1 缺失值填充可以看出数据中有一些缺失值，需要进行填充 分别采用插值法、最近邻插补、均值法进行填充 3.2 数据筛选数据记录了365天，每天24小时，总共有365*24条数据。数据量比较大，故需将其聚合成以天为单位的数据集 对数据进行可视化，这里以pm和windspeed列数据为例。 pm数据大致情况 windspeed数据大致情况 下面实验均以pm为例 4 基本原理与算法描述1、朴素法 介绍：如果数据集在一段时间内都很稳定，我们想预测第二天的价格，可以取前面一天的价格，预测第二天的值。这种假设第一个预测点和上一个观察点相等的预测方法就叫朴素法。 适用场景：不适合变化很大的数据集，最适合稳定性很高的数据集 2、简单平均法 介绍：用之前全部已知的值计算出它们的平均值，将它作为要预测的下一个值。当然这不会很准确，但这种预测方法在某些情况下效果是最好的，比如在平均值上下波动的数据集。这种将预期值等同于之前所有观测点的平均值的预测方法就叫简单平均法。 使用场景：物品价格会随机上涨和下跌，平均价格会保持一致。我们经常会遇到一些数据集，虽然在一定时期内出现小幅变动，但每个时间段的平均值确实保持不变。这种情况下，我们可以预测出第二天的价格大致和过去天数的价格平均值一致。 3、滑动平均法(MA) 介绍：某段时间内，数据集有较大变化，这时如果计算整个时间段的平均值会导致误差较大，因此取最近几个时期的数据计算平均值。这种用某些窗口期计算平均值的预测方法就叫移动平均法。移动平均法能有效地消除预测中的随机波动，是非常有用的。 适用场景：物品价格在一段时间内大幅上涨，但后来又趋于平稳。我们也经常会遇到这种数据集，比如价格或销售额某段时间大幅上升或下降。 参数：p：移动窗口：加大p值会使平滑波动效果更好，但会使预测值对数据实际变动更不敏感 4、简单指数平滑法(SE) 介绍：通过加权平均值计算出预测值，其中权重随着观测值从早期到晚期的变化呈指数级下降。 适用场景：需要将所有数据考虑在内的同时也能给数据赋予不同非权重。例如，相比更早时期内的观测值，它会给近期的观测值赋予更大的权重。 参数： α：平滑指数： 当时间序列呈现较稳定的水平趋势时，应选较小的α值，一般可在0.05～0.20之间取值； 当时间序列有波动，但长期趋势变化不大时，可选稍大的α值，常在0.1～0.4之间取值； 当时间序列波动很大，长期趋势变化幅度较大，宜选择较大的α值，如可在0.6～0.8间选值； 当时间序列数据是上升（或下降）的发展趋势类型，α应取较大的值，在0.6~1之间。 5、霍尔特线性趋势法(Holt) 介绍：可以在无需假设的情况下，准确预测出数据变化趋势。这种考虑到数据集变化趋势的方法就叫做霍尔特线性趋势法。 适用场景：每个时序数据集可以分解为相应的几个部分：趋势（Trend），季节性(Seasonal)和残差(Residual)。任何呈现某种趋势的数据集都可以用霍尔特线性趋势法用于预测。 参数：α和γ是平滑系数，取值范围均为（0，1）之间：若时间序列的波动不大，比较平稳，则α应取小一些，如0.1 ～ 0.3 ；若时间序列具有迅速且明显的变动倾向， 则α应取大一些，如0.6 ～ 0.9。 6、自回归模型(AR) 介绍：自回归模型（英语：Autoregressive model，简称AR模型），是统计上一种处理时间序列的方法，用同一变数例如x的之前各期，亦即x1至xt-1来预测本期xt的表现，并假设它们为一线性关系。因为这是从回归分析中的线性回归发展而来，只是不用x预测y，而是用x预测 x（自己）；所以叫做自回归 7、自回归移动平均模型(ARMA) 介绍：研究时间序列的重要方法，由自回归模型（简称AR模型）与移动平均模型（简称MA模型）为基础“混合”构成。在市场研究中常用于长期追踪资料的研究，如：Panel研究中，用于消费行为模式变迁研究；在零售研究中，用于具有季节变动特征的销售量、市场规模的预测等。 8、差分自回归移动平均模型（ARIMA） 介绍：如果数据集在一定时间段内的固定区间内呈现相似的模式，那么该数据集就具有季节性。自回归移动平均模型的目标是描述数据中彼此之间的关系。 适用场景：例如夏季降雨量较多，冬季降雨量较低，而且每年都是这样，那么这种重复现象叫做“季节性”，ARIMA即适用于这种场景。 5 实验5.1 数据集描述和评价指标数据集采用的是pollution.csv环境数据集，包含pm、humidity(湿度)、temperature(温度)、pressure(压力)、windspeed(风力)、snowfall(降雪量)、rainfall(降雨量)字段。总共有8544行数据，每行为当前小时的检测数据，由于大量的数据会引起网页卡顿或增加计算量的缘故，会把其按天进行重采样，简化到365条数据。 评价指标采用MAE平均绝对误差来计算。 MAE平均绝对误差：平均绝对误差，又叫平均绝对离差，是所有单个观测值与算术平均值的偏差的绝对值的平均。平均绝对误差可以避免误差相互抵消的问题，因而可以准确反映实际预测误差的大小。 5.2 实验过程和实验结果这部分以ARMA算法和ARIMA算法为例介绍算法原理 5.2.1 ARMA分析1.提取部分数据进行分析由于ARMA都是运用于原始数据是平稳的时间序列，故我们筛选出数据比较平稳的6月到9月这段时间进行预测分析 plt.figure(figsize=(16, 5), dpi=256)data_part = data[(data.index &gt;= '2014-05-01') &amp; (data.index &lt;= '2014-9-30')] 2.进行平稳性检验时序图 plt.plot(data_part['pm'])plt.show() 从时序图可以看出数据还是较为平稳的 自相关图 偏自相关图 单位根检验 单位根检验主要是检验p值是否大于0.05，大于0.05的时间序列是非平稳的，需要进行差分。p值小于0.05的是平稳的时间序列。 print(\"ADF检验结果:\", ADF(data_part['pm'])) 这里的第二个值就是p值。这里的p值小于0.05。判断选取的数据是平稳的时间序列。 白噪声检验 白噪声检验主要是检验p值是否大于0.05，大于0.05的时间序列是平稳的白噪声时间序列，p值小于0.05的是平稳的非白噪声的时间序列，是平稳的非白噪声的时间序列才可以进行下一步的ARMA分析。 print(\"白噪声的检验结果:\", acorr_ljungbox(data_part['pm'], lags=1)) 这里明显小于0.05.判断选取的数据是平稳的非白噪声的时间序列。 3.ARMA模型的训练实际上就是要确定ARMA函数中p阶，q阶的最佳参数，通过计算最小的AIC值来寻找 ps = range(0, 4) qs = range(0, 4) parameters = product(ps, qs) parameters_list = list(parameters) best_aic = float('inf') results = [] for param in parameters_list: try: model = ARMA(data_part['pm'], order=(param[0], param[1])).fit() except ValueError: print(\"参数错误：\", param) continue aic = model.aic if aic &lt; best_aic: best_model = model best_aic = model.aic best_param = param results.append([param, model.aic]) results_table = pd.DataFrame(results) results_table.columns = ['parameters', 'aic'] print(\"最优模型\", best_model.summary()) 找到当p = 3, q = 1时有最优的模型 用该模型就可以预测未来5天的pm值，并且使用均值绝对误差MAE进行评估 pred_y = best_model.forecast(5)[0] test_y = data[(data.index &gt;= '2014-10-1') &amp; (data.index &lt;= '2014-10-5')]['pm'] print(\"预测的未来5天的pm: \", pred_y) print(\"真实的未来5天的pm: \", test_y) print(\"MAE平均绝对误差： %.2f\" %mean_absolute_error(test_y, pred_y)) x = data[(data.index &gt;= '2014-10-1') &amp; (data.index &lt;= '2014-10-5')] plt.plot(x.index, test_y, color='blue') plt.plot(x.index, pred_y, color='red') plt.title('预测值与真实值对比') plt.show() 红色为预测值，蓝色为真实值 5.2.2 ARIMA分析1.提取部分数据进行分析ARIMA都是运用于原始数据差分后是平稳的时间序列，故我们筛选出数据不是特别平稳的11月到12月这段时间进行预测分析 plt.figure(figsize=(16, 5), dpi=256)data_part = data[(data.index &gt;= '2014-6-01') &amp; (data.index &lt;= '2014-7-31')] 2.进行平稳性检验时序图 plt.plot(data_part['pm'])plt.show() 从时序图可以看出数据的波动还是挺大 自相关图 偏自相关图 单位根检验 单位根检验主要是检验p值是否大于0.05，大于0.05的时间序列是非平稳的，需要进行差分。p值小于0.05的是平稳的时间序列。 print(\"ADF检验结果:\", ADF(data_part['pm'])) 这里的第二个值就是p值。这里的p值大于0.05。判断选取的数据是非平稳的时间序列。 一阶差分 一阶差分后，ADF小于0.05，转换为平稳序列 数据差分后的时序图 白噪声检验 白噪声检验主要是检验p值是否大于0.05，大于0.05的时间序列是平稳的白噪声时间序列，p值小于0.05的是平稳的非白噪声的时间序列，是平稳的非白噪声的时间序列才可以进行下一步的ARMA分析。 print(\"白噪声的检验结果:\", acorr_ljungbox(data_part['pm'], lags=1)) 这里小于0.05，判断选取的数据是平稳的非白噪声的时间序列。 3.ARMA模型的训练由于只有一阶差分，故参数d = 1 D_data_day = data_part['pm'].diff().dropna() print(\"一阶差分检验结果：\", ADF(D_data_day)) 下面寻找最优的p和q ps = range(0, 4) qs = range(0, 4) parameters = product(ps, qs) parameters_list = list(parameters) best_aic = float('inf') results = [] for param in parameters_list: try: model = ARIMA(data_part['pm'], order=(param[0], 1, param[1])).fit() except ValueError: print(\"参数错误：\", param) continue aic = model.aic if aic &lt; best_aic: best_model = model best_aic = model.aic best_param = param results.append([param, model.aic]) results_table = pd.DataFrame(results) results_table.columns = ['parameters', 'aic'] print(\"最优模型\", best_model.summary()) pred_y = best_model.forecast(3)[0] test_y = data[(data.index &gt;= '2014-8-1') &amp; (data.index &lt;= '2014-8-3')]['pm'] print(\"预测的未来3天的pm: \", pred_y) print(\"真实的未来3天的pm: \", test_y) print(\"MAE平均绝对误差： %.2f\" % mean_absolute_error(test_y, pred_y)) x = data[(data.index &gt;= '2014-8-1') &amp; (data.index &lt;= '2014-8-3')] plt.plot(x.index, test_y, color='blue') plt.plot(x.index, pred_y, color='red') plt.title('预测值与真实值对比') plt.show() 找到最优参数 p = 1，q = 1。 用该模型预测未来3天的pm值，并且使用均值绝对误差MAE进行评估 红色为预测值，蓝色为真实值，总体趋势还是吻合的 5.3 实验结果可视化使用ARIMA预测train.csv数据集结果 数据集操作可以选择要预测的数据集，以及确定数据集的占比；预测方法部分实现了AR、MA、SES、Holt、ARIMA算法 6 总结本项目是基于Web的时间序列预测平台。通过上传数据集，选取时间序列算法，设定参数来完成数据的预测。系统分为使用Vue搭建的前端以及使用Java SpringBoot框架搭建的后端、使用Python实现算法。Java后端接收前端请求，后端调用相应算法进行计算，Java与Python之间通过RPC进行通信。算法部分使用statsmodels.tsa库下的一系列时间序列预测函数，如AR、ARMA、ARIMA等方法。对模型预测结果的评估采用了平均绝对误差、标准差、均方根误差来处理。 本文档侧重描述了ARMA、ARIMA算法的实现原理。如关于数据集的特征工程以及算法实现步骤、每个步骤对应的可视化。并没有对Web搭建做过多的介绍，Web应用的演示放在了实验结果可视化部分。 对于时间序列预测的算法，数据集的选取还是比较重要的，对于不同特点的数据集采用不同方法进行预测，会达到较高的预测精度，并不能期望一种算法能够适配所有数据。 参考文献[1]zhh_2017.统计学——时间序列预测[EB/OL].https://blog.csdn.net/i505224798/article/details/87310168,2019-02-15. [2]皮皮_blog.时间序列分析[EB/OL].http://blog.csdn.net/pipisorry/article/details/62053938,2017-3-22. [3]张良均.Python数据分析与挖掘实战[M].机械工业出版社:北京市,2018:119-132. https://blog.csdn.net/qq_33333002/article/details/106171234 https://mp.weixin.qq.com/s?__biz=MzU5Nzg5ODQ3NQ==&amp;mid=2247487503&amp;idx=2&amp;sn=3830a27122726bd8b43b28c0df34cd65&amp;chksm=fe4d358bc93abc9d161c7d9289e2680bf5a1c42491fa2e86e533021512985ca7a3c707f7fc0a&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1590235251941&amp;sharer_shareid=8eccc1546aceed0b542f3d42d319e5e1#rd","categories":[{"name":"Python","slug":"Python","permalink":"http://hofe.work/categories/Python/"},{"name":"数据分析","slug":"Python/数据分析","permalink":"http://hofe.work/categories/Python/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"时间序列","slug":"时间序列","permalink":"http://hofe.work/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"}]},{"title":"Java集合核心概念","slug":"Java/概念/Java集合核心概念","date":"2020-05-25T02:30:00.000Z","updated":"2020-05-30T07:41:54.631Z","comments":true,"path":"2020/05/25/Java/概念/Java集合核心概念/","link":"","permalink":"http://hofe.work/2020/05/25/Java/%E6%A6%82%E5%BF%B5/Java%E9%9B%86%E5%90%88%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"","text":"没有进行扩展，方便复习。需要更详细的，可以访问我的博客查看其它文章 一、List存放是有序的，允许存放重复元素 1.1 ArrayListArrayList 是常用的 List 实现类，内部是通过数组实现的，它允许对元素进行快速随机访问。数 组的缺点是每个元素之间不能有间隔，当数组大小不满足时需要增加存储能力，就要将已经有数 组的数据复制到新的存储空间中。当从 ArrayList 的中间位置插入或者删除元素时，需要对数组进 行复制、移动、代价比较高。因此，它适合随机查找和遍历，不适合插入和删除。 1.2 LinkedListLinkedList是用双向链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较 慢。另外，他实现了Deque接口，可以用于操作表头和表尾元素，可以当作堆 栈、队列和双向队列使用。 1.3 VectorVector与ArrayList一样，也是通过数组实现的，不同的是它支持线程的同步，即某一时刻只有一 个线程能够写 Vector，避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此， 访问它比访问ArrayList慢 二、SetSet是没有顺序，且不可重复的。 2.1 HashSet（哈希表）Set不允许重复，底层是HashMap，无容量限制，是非线程安全的。虽然底层是HashMap但不存储键值对，值存储对象。 HashSet根据元素的哈希值来决定存储在哪个位置，哈希值相同的使用equals判断是否是同一元素，是的话插入操作失败，不是的话就在相同的哈希值下顺延 HashSet存储元素的顺序并不是按照存入时的顺序（和List显然不 同） 而是按照哈希值来存的所以取数据也是按照哈希值取得。 元素的哈希值是通过元素的 hashcode方法来获取的, HashSet首先判断两个元素的哈希值，如果哈希值一样，接着会比较 equals方法 如果 equls结果为true ，HashSet就视为同一个元素。如果equals 为false就不是 同一个元素。 哈希值相同equals为false的元素在同样的哈希值下顺延. 2.2 TreeSet基于TreeMap实现，支持排序，是非线程安全的 TreeSet()是使用二叉树的原理对新add()的对象按照指定的顺序排序（升序、降序），每增 加一个对象都会进行排序，将对象插入的二叉树指定的位置。Integer和String对象都可以进行默认的TreeSet排序，而自定义类的对象是不可以的，自 己定义的类必须实现Comparable接口，并且覆写相应的compareTo()函数，才可以正常使 用。在覆写compare()函数时，要返回相应的值才能使TreeSet按照一定的规则来排序比较此对象与指定对象的顺序。如果该对象小于、等于或大于指定对象，则分别返回负整 数、零或正整数 2.3 LinkHashSetLinkHashSet(HashSet+LinkHashMap)，继承自HashSet，底层是LinkedHashMap。这样做的好处是LinkHashSet中的元素顺序是可以保证的，也就是遍历序和插入序是一样的。对于 LinkedHashSet 而言，它继承于 HashSet、又基于 LinkedHashMap 来实现的。 LinkedHashSet 底层使用 LinkedHashMap（有序） 来保存所有元素，它继承与 HashSet，其所有的方法 操作上又与HashSet相同，因此LinkedHashSet 的实现上非常简单，只提供了四个构造方法，并 通过传递一个标识参数，调用父类的构造器，底层构造一个 LinkedHashMap 来实现，在相关操 作上与父类HashSet的操作相同，直接调用父类HashSet的方法即可 三、Map 3.1 HashMap1.底层是数组+链表/红黑树。 2.无序键值对集合，根据键的hashCode值存储数据，大多数情况下可以直接定位到它的值，因而具有很快 的访问速度。 3.HashMap多只允许一条记录的键为null，允许多条记 录的值为 null。 4.HashMap 非线程安全，如果需要满足线程安全，可以用 Collections 的 synchronizedMap 方法使 HashMap 具有线程安全的能力，或者使用 ConcurrentHashMap。 3.2 HashtableHashtable 是遗留类，很多映射的常用功能与 HashMap 类似，不同的是它承自 Dictionary 类，实现Map接口 并且是线程安全的，所有的读写等操作都进行了锁（synchronized）保护，任一时间只有一个线程能写 Hashtable，并发性不如 ConcurrentHashMap， 因为 ConcurrentHashMap 引入了分段锁。Hashtable 不建议在新代码中使用，不需要线程安全 的场合可以用HashMap替换，需要线程安全的场合可以用ConcurrentHashMap替换。 3.3 TreeMapTreeMap 实现 SortedMap 接口，能够把它保存的记录根据键排序，默认是按键值的升序排序， 也可以指定排序的比较器，当用Iterator遍历TreeMap时，得到的记录是排过序的。如果使用排序的映射，建议使用TreeMap。在使用 TreeMap 时，key 必须实现 Comparable 接口或者在构造 TreeMap 传入自定义的 Comparator，否则会在运行时抛出java.lang.ClassCastException类型的异常。 3.4 LinkedHashMapLinkedHashMap 是 HashMap 的一个子类，底层结构和HashMap一样，但通过双向链表保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap时，先得到的记录肯定是先插入的，也可以在构造时带参数，按照访问次序排序。 注：TreeMap是按照hashcode进行排序的，LinkedHashMap是根据插入顺序进行排序 四、面试题4.1 list实现类之间的区别对于需要快速插入、删除元素的场景，应该使用LinkedList 对于需要快速随机访问元素，应该使用ArrayList 对于单线程环境，应该使用非同步的类如ArrayList 对于多线程环境，应该使用同步的类如Vector 4.2 ArrayList和LinkedList的区别？①ArrayList和LinkedList，一个基于数组实现，一个基于链表实现前者是数组队列，相当于动态数组；后者为双向链表结构，也可当作堆栈、队列、双端队列②当随机访问List时（get和set操作），ArrayList比LinkedList的效率更高，因为LinkedList是线性的数据存储方式，所以需要移动指针从前往后依次查找。③当对数据进行增加和删除的操作时(add和remove操作)，LinkedList比ArrayList的效率更高，因为ArrayList是数组，所以在其中进行增删操作时，会对操作点之后所有数据的下标索引造成影响，需要进行数据的移动。④内存空间占用来看，ArrayList的空间浪费体现在会预留一定的容量空间，而LinkedList中存储的每个元素都需要存放前驱和后继的指针，空间占用会比ArrayList中每个元素的多。⑤ArrayList和LinkedList都是线程不安全的 场景：链表，插入删除快，查找修改慢。 适用于频繁增删的场景。数组，查找快，插入删除慢。 适用于频繁查找和修改的场景。 4.3 ArrayList和Vector的区别？1.ArrayList是线程不安全的，Vector是线程安全的 2.在不需要保证线程安全时应使用ArrayList，因为Vector在同步操作上会消耗大量时间 4.4 HashMap与HashTable区别1.HashMap没有对读写等操作进行锁保护，所以是线程不安全的，在多线程场景下会出现数据不一致的问题。而HashTable是同步的，所有的读写等操作都进行了锁（synchronized）保护，在多线程环境下没有安全问题。但是锁保护也是有代价的，会对读写的效率产生较大影响。 2.HashMap允许键和值是null，而Hashtable不允许键或者值是null。 3.HashMap的迭代器（Iterator）是fail-fast迭代器，但是Hashtable的迭代器（enumerator）不是fail-fast的。如果有其它线程对HashMap进行的添加/删除元素，将会抛出ConcurrentModificationException，但迭代器本身的remove方法移除元素则不会抛出异常。 4.5 Collection 和 Collections的区别?Collection是集合类的上级接口，继承于他的接口主要有Set 和List.Collections是针对集合类的一个辅助类，他提供一系列静态方法实现对各种集合的搜索、排序、线程安全化等操作 4.6 Collection集合接口和Map接口有什么关系？没有直接关系，但是一些子类会有依赖，Collection是最基本的集合接口，声明了适用于JAVA集合（只包括Set和List）的通用方法。Map接口并不是Collection接口的子接口，但是它仍然被看作是Collection框架的一部分。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"基础","slug":"Java/基础","permalink":"http://hofe.work/categories/Java/%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://hofe.work/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"进程与线程核心概念","slug":"Java/概念/进程与线程","date":"2020-05-23T08:25:27.000Z","updated":"2020-05-29T15:52:06.171Z","comments":true,"path":"2020/05/23/Java/概念/进程与线程/","link":"","permalink":"http://hofe.work/2020/05/23/Java/%E6%A6%82%E5%BF%B5/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"1.进程与线程概念进程 进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程 线程 线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 协程 协程，英文Coroutines，是一种比线程更加轻量级的存在。正如一个进程可以拥有多个线程一样，一个线程也可以拥有多个协程。协程不是被操作系统内核所管理，而完全是由程序所控制（也就是在用户态执行）。这样带来的好处就是性能得到了很大的提升，不会像线程切换那样消耗资源。 2.进程和线程的区别 进程是资源分配的最小单位，线程是程序执行的最小单位（资源调度的最小单位） 进程有自己的独立地址空间，每启动一个进程，系统就会为它分配地址空间，建立数据表来维护代码段、堆栈段和数据段，这种操作非常昂贵。线程是共享进程中的数据的，使用相同的地址空间，因此CPU切换一个线程的花费远比进程要小很多，同时创建一个线程的开销也比进程要小很多。 线程之间的通信更方便，同一进程下的线程共享全局变量、静态变量等数据，而进程之间的通信需要以通信的方式（IPC)进行。 但是多进程程序更健壮，多线程程序只要有一个线程死掉，整个进程也死掉了，而一个进程死掉并不会对另外一个进程造成影响，因为进程有自己独立的地址空间 总结： 线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。 线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。 线程执行开销小，但不利于资源的管理和保护；而进程正相反。 3.进程与线程状态转换 关于状态这块，网上的说法太多了，有5种、6种的，《Java并发编程艺术》书里也没讲进程。下面是我找到的比较流行的说法，如果大家有推荐的书籍，欢迎留言告知。这里的线程指Java中的线程 进程有5种状态 创建状态(new) ：进程正在被创建，尚未到就绪状态。 就绪状态(ready) ：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。 运行状态(running) ：进程正在处理器上上运行(单核CPU下任意时刻只有一个进程处于运行状态)。 阻塞状态(waiting) ：又称为等待状态，进程正在等待某一事件而暂停运行如等待某资源为可用或等待 IO 操作完成。即使处理器空闲，该进程也不能运行。 结束状态(terminated) ：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行 线程有6种状态线程状态这一块有点复杂，会专门出一篇研究 初始(NEW)：新创建了一个线程对象，但还没有调用start()方法。 运行(RUNNABLE)：Java线程中将就绪（ready）和运行中（running）两种状态笼统的称为“运行”。线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取CPU的使用权，此时处于就绪状态（ready）。就绪状态的线程在获得CPU时间片后变为运行中状态（running）。 阻塞(BLOCKED)：表示线程阻塞于锁。注意和进程的区别，进程是IO阻塞 等待(WAITING)：进入该状态的线程需要等待其他线程做出一些特定动作（通知或中断）。 超时等待(TIMED_WAITING)：该状态不同于WAITING，它可以在指定的时间后自行返回。 终止(TERMINATED)：表示该线程已经执行完毕。 4.进程与线程的通信方式进程间的通信方式大概有 7 种常见的进程间的通信方式。 管道/匿名管道(Pipes) ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。只存在于内存中的文件 有名管道(Names Pipes) : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循先进先出(first in first out)。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 消息队列(Message Queuing) ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。与管道（无名管道：只存在于内存中的文件；命名管道：存在于实际的磁盘介质或者文件系统）不同的是消息队列存放在内核中，只有在内核重启(即，操作系统重启)或者显示地删除一个消息队列时，该消息队列才会被真正的删除。消息队列可以实现消息的随机查询,消息不一定要以先进先出的次序读取,也可以按消息的类型读取.比FIFO更有优势。消息队列克服了信号承载信息量少，管道只能承载无格式字 节流以及缓冲区大小受限等缺。 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。套接字是支持TCP/IP的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点，简单的说就是通信的两方的一种约定，用套接字中的相关函数来完成通信过程。 线程间的通信方式线程间的通信有JMM内存模型控制，共享变量存储在主内存中，每个线程可以将其拷贝到自己的本地内存中，在本地内存中更新过的共享变量刷新会刷新到主内存，另一线程在从主内存中读取更新后的共享变量。 volatile与synchronize关键字 关键字volatile可以用来修饰字段（成员变量），就是告知程序任何对该变量的访问均需要从共享内存中获取，而对它的改变必须同步刷新回共享内存，它能保证所有线程对变量访问的可见性。 关键字synchronized可以修饰方法或者以同步块的形式来进行使用，它主要确保多个线程在同一个时刻，只能有一个线程处于方法或者同步块中，它保证了线程对变量访问的可见性和排他性。 等待/通知机制：是指一个线程A调用了对象O的wait()方法进入等待状态，而另一个线程B 调用了对象O的notify()或者notifyAll()方法，线程A收到通知后从对象O的wait()方法返回，进而 执行后续操作。上述两个线程通过对象O来完成交互，而对象上的wait()和notify/notifyAll()的关系就如同开关信号一样，用来完成等待方和通知方之间的交互工作。 补充： 1）使用wait()、notify()和notifyAll()时需要先对调用对象加锁（在synchronized中调用）。2）调用wait()方法后，线程状态由RUNNING变为WAITING，并将当前线程放置到对象的等待队列。3）notify()或notifyAll()方法调用后，等待线程依旧不会从wait()返回，需要调用notify()或 notifAll()的线程释放锁之后，等待线程才有机会从wait()返回。4）notify()方法将等待队列中的一个等待线程从等待队列中移到同步队列中，而notifyAll() 方法则是将等待队列中所有的线程全部移到同步队列，被移动的线程状态由WAITING变为 BLOCKED。5）从wait()方法返回的前提是获得了调用对象的锁。从上述细节中可以看到，等待/通知机制依托于同步机制，其目的就是确保等待线程从 wait()方法返回时能够感知到通知线程对变量做出的修改。 管道输入/输出流：和普通的文件输入/输出流或者网络输入/输出流不同之处在于，它主要用于线程之间的数据传输，而传输的媒介为内存。 Thread.join()：如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后才 从thread.join()返回。 ThreadLocal，即线程变量，是一个以ThreadLocal对象为键、任意对象为值的存储结构。这 个结构被附带在线程上，也就是说一个线程可以根据一个ThreadLocal对象查询到绑定在这个线程上的一个值。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"基础","slug":"Java/基础","permalink":"http://hofe.work/categories/Java/%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"线程","slug":"线程","permalink":"http://hofe.work/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"进程","slug":"进程","permalink":"http://hofe.work/tags/%E8%BF%9B%E7%A8%8B/"}]},{"title":"学生成绩管理系统（C语言）","slug":"项目/学生管理系统(C版本)","date":"2020-05-23T06:53:45.000Z","updated":"2020-06-06T15:51:15.792Z","comments":true,"path":"2020/05/23/项目/学生管理系统(C版本)/","link":"","permalink":"http://hofe.work/2020/05/23/%E9%A1%B9%E7%9B%AE/%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F(C%E7%89%88%E6%9C%AC)/","excerpt":"","text":"一、系统背景系统名称：学生成绩管理系统 该系统采用C语言实现，完成学生成绩记录的查看、录入、查询、修改、删除、排序、统计等功能，在记录的存储上采用了哈希表的数据结构，即数组+链表的形式进行存储，可以加快查询的速度，也使用了堆排序算法来完成排序功能。 不足：没有持久化存储机制，可以采用文件的方式进行存储。 二、需求分析菜单 基本功能以菜单形式提供，可反复使用各项功能 学生信息及成绩的录入 要求包括的学生信息有：学号、姓名、性别、出生日期及某科目成绩； 录入的学生按学号散列存储, 采用链表法解决冲突。 学生成绩的查询 根据提供的学号完成学生成绩的查询。 学生成绩的分段统计和排序输出 可以统计出各分数段学生人数；采用堆排序, 将学生成绩从高到低排序输出。 学生成绩记录的删除 可以删除学生成绩记录 三、模块设计 功能 函数原型 说明 1、菜单 Menu() 程序的入口，拥有学生成绩查看、录入、查找、排序统计、修改、删除等操作。 2、学生成绩查看 doViewall() 将文件中学生的成绩按学号顺序以列表陈列。 3、学生成绩录入 doInsert() 将学生成绩按学号散列存储，并采用链表法解决冲突。 4、学生成绩查找 doSearch() 输入学号，查找出学生信息，以及询问是否修改删除或继续查找。 5、学生成绩排序 doSort() 采用堆排序将学生成绩从高到低排序。 6、学生成绩统计 Statistics() 统计各分数段学生人数。 7、学生成绩修改 doModify() 对学生的信息进行修改。输入学号，程序会跳出学生的详细信息，确认是否修改。 8、学生成绩删除 doClear() 对学生的信息进行删除操作。 四、数据流程图流程图这块就不放上来了，系统业务本身并不复杂，代码是很容易懂的。 五、模块实现5.1 查看学生信息将全部学生的数据条按照存储数据排列输出。由于hash表的结构是数组+链表，故遍历数组的时候，如果数组中元素后跟着链表则遍历链表输出。 void doViewall(Hash *H)&#123; stu t; int i = 0; printf(\"=================================================================\\n\"); printf(\"学号\\t姓名\\t性别\\t生日\\t\\t民族\\t科目成绩\\n\"); for (int i = 0; i&lt;5; i++) &#123; for (t = H[i]; t != NULL; t = t-&gt;next) &#123; doDisplay(H, t); printf(\"\\n\"); &#125; &#125; printf(\"\\n=================================================================\\n\");&#125; 5.2 录入学生信息使用哈希表（散列表）的存储形式。散列表是在存储元素时通过计算关键码的散列函数（关键码和存储位置之间的对应函数是散列函数）值确定存储地址。通过选择适当的正整数p，按计算公式H(K)=Kmodp来计算关键码K的散列地址。若关键码个数为n，散列表表长为m（一般m&gt;=n），通常选p为小于或等于表长m的最大素数或不包含小于20的质因子的合数，一般也要求p&gt;=n。在此题要求p = 5。散列表特点就是在当散列地址相同时会出现冲突。如：取余的余数为5，当要输入学号1时，就存储在H[1]中，但若要在输入一个学号为6的学生，那么6%5=1就会和存储在H[1]的学号1的学生产生冲突。 解决冲突使用拉链法。将所有散列地址相同的记录存储在同一个单链表中。该单链表头指针存储在散列表中。散列表就是个指针数组，下标就是由关键码用散列函数计算出的散列地址。初始，指针数组每个元素为空指针，相当于所有单链表头指针为空，以后每扫描到一条记录，按其关键码的散列地址，在相应的单链表中加入含该记录的节点。 采用数组和链表相结合的办法，将Hash地址相同的记录存储在一张线性表中，而每张表的表头的序号即为计算得到的Hash地址。比如学号为1的就存在H[1]当作表头，H[n]数组包括学号num,姓名name等信息，还包括类似链表的指向下一个结点的next指针。 Status Build_Hash(Hash *H,stu s)//输入一组关键字,建立Hash表,表长为m,用链地址法处理冲突.&#123; stu p; int n; n=s-&gt;num%5; if(H[n]==NULL) H[n]=s; //作为链表的第一个结点 else &#123; for(p=H[n];p-&gt;next;p=p-&gt;next); &#123; if(p-&gt;num==s-&gt;num) &#123; printf(\" 已存在该学号的学生，添加学生信息出错!\\n\"); return ERROR; &#125; p-&gt;next=s; //插入链表尾部.本算法不考虑排序问题. &#125; &#125; return OK;&#125;//Build_Hash 5.3 查找学生信息采取散列查找。散列查找也如同散列存储一样，会出现冲突的问题，解决方法也是采用拉链法的形式，具体操作如上。根据关键值计算出散列地址。散列地址即散列表的下标，在其相应单链表找出该结点。 //成绩查询Status Search(Hash *H,int key)&#123; int n; stu t; n=key%5; if(H[n]) &#123; t = H[n]; if(EQ(H[n]-&gt;num,key))//在表头 &#123; printf(\"=============================================================\\n\"); printf(\"学号\\t姓名\\t性别\\t生日\\t\\t民族\\t数构成绩\\n\"); doDisplay(H, t); printf(\"\\n=============================================================\\n\"); return OK; &#125; else//探测下一个地址，在链表中是查找 &#123; t=H[n]-&gt;next; while(t) &#123; if(EQ(t-&gt;num,key)) &#123; printf(\"=============================================================\\n\"); printf(\"学号\\t姓名\\t性别\\t生日\\t\\t民族\\t数构成绩\\n\"); doDisplay(H, t); printf(\"\\n=============================================================\\n\"); return OK; &#125; t=t-&gt;next; &#125; &#125; &#125; return ERROR;&#125; 5.4 学生信息排序核心算法是堆排序。堆是一棵顺序存储的完全二叉树。其中每个结点的关键字都不大于其孩子结点的关键字，这样的堆称为小根堆。其中每个结点的关键字都不小于其孩子结点的关键字，这样的堆称为大根堆。这里我们采用的是大根堆。其算法思想即是 （1）根据初始数组去构造初始堆（构建一个完全二叉树，保证所有的父结点都比它的孩子结点数值大）。 （2）每次交换第一个和最后一个元素，输出最后一个元素（最大值），然后把剩下元素重新调整为大根堆。 如此反复，这个数组已经是顺序排列了。 为方便理解，引用网上图片 构建初始堆的过程如下图： ​ ——来源技术博客 ​ 堆排序过程如下图 ​ ——来源于网上博客 // 堆排序void HeapAdjust(HeapType &amp;H, int i, int m)&#123; // 已知 R[s..m]中记录的关键字除 R[i] 之外均 // 满足堆的特征，本函数自上而下调整 R[i] 的 // 关键字，使 R[i..m] 也成为一个大顶堆。 int temp; int j; temp = H.r[i]; // 暂存 R[s] for (j = 2 * i; j &lt;= m; j *= 2) &#123; // j 初值指向左孩子 //自上而下的筛选过程; if (j&lt;m &amp;&amp; LT(H.r[j], H.r[j + 1])) ++j; // 左/右“子树根”之间先进行相互比较 // 令 j 指示关键字较大记录的位置 if (!LT(temp, H.r[j])) break; // 再作“根”和“子树根”之间的比较， // 若“&gt;=”成立，则说明已找到 temp 的插 // 入位置 i ，不需要继续往下调整 H.r[i] = H.r[j]; i = j; // 否则记录上移，尚需继续往下调整 &#125; H.r[i] = temp; // 将调整前的堆顶记录插入到 i 位置&#125; // HeapAdjustvoid HeapSort(HeapType &amp;H) &#123; // 对顺序表 H 进行堆排序。 int temp; int i; for (i = H.length / 2; i&gt;0; --i) HeapAdjust(H, i, H.length); // 建大顶堆 for (i = H.length; i&gt;1; --i) &#123; //H.r[1]←→H.r[i]; temp = H.r[1]; H.r[1] = H.r[i]; H.r[i] = temp; // 将堆顶记录当前未经排序子序列 // H.r[1..i]中最后一个记录相互交换 HeapAdjust(H, 1, i - 1); // 对 H.r[1] 进行筛选 &#125;&#125; // HeapSort 5.5 学生成绩统计遍历学生成绩，分段统计 //统计各个分数段的人数void Calculate(HeapType h)&#123; int a, b, c, d, e; int i; a = b = c = d = e = 0; for (i = 1; i &lt;= h.length; i++) &#123; if (h.r[i] &gt;= 90) a++; else if (h.r[i] &gt;= 80) b++; else if (h.r[i]&gt;70) c++; else if (h.r[i]&gt;60) d++; else e++; &#125; printf(\"\\t科目成绩\\n\"); printf(\"91-100分：%d人\\n\", a); printf(\"81-90分： %d人\\n\", b); printf(\"71-80分： %d人\\n\", c); printf(\"61-70分： %d人\\n\", d); printf(\"60分以下：%d人\\n\", e);&#125; 5.6 修改学生信息就是需要找到要修改的数据存储地址，在这个存储地址上通过输入数据覆盖原先的数据。在修改功能里，加入判断，可以实现只修改学生的个别信息，而不是重新输入学生全部信息。 while(t-&gt;next!=NULL) &#123; if(EQ(t-&gt;next-&gt;num,key)) &#123; printf(\"\\n&gt;&gt;是否修改姓名?(Y/N):\"); fflush(stdin);//把scanf的输入流自动清除 scanf(\"%c\", &amp;cmd); if (cmd == 'y' || cmd == 'Y') &#123; printf(\"&gt;&gt;请输入要修改的姓名\\n\"); fflush(stdin);//把scanf的输入流自动清除 gets(t-&gt;name); printf(\"修改成功！\\n\"); &#125; printf(\"\\n&gt;&gt;是否修改性别?(Y/N):\"); fflush(stdin);//把scanf的输入流自动清除 scanf(\"%c\", &amp;cmd); if (cmd == 'y' || cmd == 'Y') &#123; printf(\"&gt;&gt;请输入修改的性别[F/M]\\n\"); fflush(stdin);//把scanf的输入流自动清除 scanf(\"%c\", &amp;t-&gt;sex); printf(\"修改成功！\\n\"); &#125; printf(\"\\n&gt;&gt;是否修改出生年月?(Y/N):\"); fflush(stdin);//把scanf的输入流自动清除 scanf(\"%c\", &amp;cmd); if (cmd == 'y' || cmd == 'Y') &#123; printf(\"&gt;&gt;请输入修改的出生年月\\n\"); fflush(stdin);//把scanf的输入流自动清除 gets(t-&gt;birthday); printf(\"修改成功！\\n\"); &#125; printf(\"\\n&gt;&gt;是否修改民族?(Y/N):\"); fflush(stdin);//把scanf的输入流自动清除 scanf(\"%c\", &amp;cmd); if (cmd == 'y' || cmd == 'Y') &#123; printf(\"&gt;&gt;请输入修改的民族\\n\"); fflush(stdin);//把scanf的输入流自动清除 gets(t-&gt;folk); printf(\"修改成功！\\n\"); &#125; printf(\"\\n&gt;&gt;是否修改数据结构的成绩?(Y/N):\"); fflush(stdin);//把scanf的输入流自动清除 scanf(\"%c\", &amp;cmd); if (cmd == 'y' || cmd == 'Y') &#123; printf(\"&gt;&gt;请输入修改的数据结构成绩\\n\"); fflush(stdin);//把scanf的输入流自动清除 scanf(\"%d\",&amp;k); H[n]-&gt;s.DataStructure=k; printf(\"修改成功！\\n\"); &#125; printf(\"&gt;&gt;请按回车键返回主菜单...\"); printf(\"\\n========================================================\\n\"); return OK; &#125; t=t-&gt;next; &#125; 5.7 删除学生信息与修改功能相似。找到目的数据的存储地址，将前一个元素的next指针指向当前元素的下一个。同时需要释放删除元素的空间。 Status doClear(Hash *H)&#123; int key=0; char cmd; int n; stu t; printf(\"&gt;&gt;请输入要删除学生的学号:\"); scanf(\"%d\",&amp;key); n=key%5; if(!Search(H,key)) printf(\" 不存在该学生的信息...\"); else if(H[n]) &#123; t=H[n]; if(EQ(H[n]-&gt;num,key)) &#123; H[n]=H[n]-&gt;next; printf(\"删除成功！\\n\"); printf(\"\\n========================================================\\n\"); return OK; &#125; else &#123; t = H[n]; while (t-&gt;next != NULL) &#123; if (EQ(t-&gt;next-&gt;num, key)) &#123; t-&gt;next = t-&gt;next-&gt;next; printf(\"删除成功！\\n\"); printf(\"\\n========================================================\\n\"); return OK; &#125; t = t-&gt;next; &#125; &#125; &#125; fflush(stdin);&#125; 六、数据介绍系统运行用到的数据在启动的时候通过函数进行载入内存。预先读取代码存储的数据，将数据存入Hash表中，之后的添加删除操作也都在Hash表上操作。数据字段包含学号、姓名、性别、出生日期及某科目成绩。 七、运行结果截图菜单 启动程序，即弹出菜单，一个操作完成也会弹出菜单方便下一次选择 【1】学生成绩查看 列出所有的学生记录 【2】学生成绩录入 分别录入字段，并存入hash表中 【3】学生成绩查询 输入学生学号即可查询该生成绩 【4】学生成绩排序 将学生记录按照成绩高低进行排序 【5】学生成绩统计 将学生按照成绩进行分段统计 【6】学生成绩修改 输入学生学号，进行修改该生信息，修改完成后可以选择是否继续修改 【7】学生成绩删除 输入学生学号，进行删除学生记录 删除学号为4的学生后，记录中就不再有该生","categories":[{"name":"项目","slug":"项目","permalink":"http://hofe.work/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"C","slug":"C","permalink":"http://hofe.work/tags/C/"}]},{"title":"RabbitMQ六种工作模式","slug":"微服务分布式集群/RabbitMQ面试题","date":"2020-05-23T03:17:27.000Z","updated":"2020-05-23T04:26:09.151Z","comments":true,"path":"2020/05/23/微服务分布式集群/RabbitMQ面试题/","link":"","permalink":"http://hofe.work/2020/05/23/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/RabbitMQ%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"","text":"一、RabbitMQ六种工作模式1.1 simple简单模式 消息产生者将消息放入队列 消息的消费者(consumer) 监听(while) 消息队列,如果队列中有消息,就消费掉,消息被拿走后,自动从队列中删除(隐患 消息可能没有被消费者正确处理,已经从队列中消失了,造成消息的丢失) 应用场景:聊天(中间有一个过度的服务器;p端,c端) 生产者 public class Sender &#123; private final static String QUEUE_NAME = \"simple_queue\"; public static void main(String[] args) throws IOException &#123; Connection connection = ConnectionUtil.getConnection();//创建连接 Channel channel = connection.createChannel();//创建通道 // 队列名\\是否持久化\\是否排外\\是否自动删除\\消费完删除 channel.queueDeclare(QUEUE_NAME, false, false, false, null); String message = \"错的不是我，是这个世界~\"; //消息内容 // 交换机\\队列名\\其他属性(路由)\\消息body channel.basicPublish(\"\", QUEUE_NAME,null,message.getBytes()); channel.close(); connection.close(); &#125;&#125; 消费者 public class Receiver &#123; private final static String QUEUE_NAME = \"simple_queue\"; public static void main(String[] args) throws IOException, InterruptedException &#123; Connection connection = ConnectionUtil.getConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); QueueingConsumer consumer = new QueueingConsumer(channel); channel.basicConsume(QUEUE_NAME, true, consumer); while(true)&#123; //该方法会阻塞 QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); &#125; &#125;&#125; 1.2 work工作模式(资源的竞争) 消息产生者将消息放入队列 消费者可以有多个,消费者1,消费者2,同时监听同一个队列；C1 C2共同争抢当前的消息队列内容,谁先拿到谁负责消费消息(隐患,高并发情况下,默认会产生某一个消息被多个消费者共同使用,可以设置一个开关(syncronize,与同步锁的性能不一样) 保证一条消息只能被一个消费者使用) 应用场景:红包;大项目中的资源调度(任务分配系统不需知道哪一个任务执行系统在空闲,直接将任务扔到消息队列中,空闲的系统自动争抢) 1.3 publish/subscribe发布订阅(共享资源) 消息产生者将消息放入交换机X 交换机发布订阅把消息发送到所有消息队列中 对应消息队列的消费者拿到消息进行消费 相关场景:邮件群发,群聊天,广播(广告) 1.4 routing路由模式 消息生产者将消息发送给交换机，当前产生的消息携带路由字符(对象的方法) 交换机按照路由的key判断，将消息按照key放到对应的队列 消费者从队列中消费消息 业务场景:可将不同类型的消息放到不同队列，并有对应的消费者处理 1.5 topic 主题模式(路由模式的一种) 星号井号代表通配符 星号代表多个单词,井号代表一个单词 路由功能添加模糊匹配 消息产生者产生消息,把消息交给交换机 交换机根据key的规则模糊匹配到对应的队列,由队列的监听消费者接收消息消费 1.6 RPC模式 客户端通过RPC向服务端发出请求，携带着correlation_id：这是我的请求标识，erply_to：你处理完过后把结果返回到这个队列中 服务端拿到了请求，开始处理并返回 客户端用自己的correlation_id与服务端返回的id进行对比；是我的，就接收。从发送到接收客户端是阻塞的。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"MQ","slug":"分布式/MQ","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/MQ/"}],"tags":[{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"MQ","slug":"MQ","permalink":"http://hofe.work/tags/MQ/"}]},{"title":"一文详解Lock","slug":"Java/一文详解Lock","date":"2020-05-22T07:35:27.000Z","updated":"2020-05-23T05:44:22.993Z","comments":true,"path":"2020/05/22/Java/一文详解Lock/","link":"","permalink":"http://hofe.work/2020/05/22/Java/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3Lock/","excerpt":"","text":"摘要Lock、AQS、锁之间的关系Java 5之后新增了Lock接口，自定义类可实现Lock接口，并通过内部静态类继承AQS抽象类的方式实现独占锁、共享锁。锁是面向使用者的，它定义了使用者与锁交互的接口，隐藏了实现细节；同步器面向的是锁的实现者，它简化了锁的实现方式，屏蔽了同步状态管理、线程的排队、等待与唤醒等底层操作。用户调用子类实现的Lock接口中提供的方法，而这些方法又调用同步器的方法来实现具体的功能。 AQS实现原理AQS是一个Java提供的底层同步工具类，用一个int类型的变量state表示同步状态，并使用CAS操作来管理这个同步状态。同时，它还实现了一个FIFO的队列，底层采用双向链表实现，并有head和tial指针指向头和尾。 当一个线程获取到锁之后，通过CAS将state设置为1代表线程获取到锁；如果这时候有其它的线程在竞争锁，那么在失败后其它将会被加入队列尾部，并且自旋判断其前驱节点为头节点&amp;是否成功获取同步状态，两个条件都成立，则将当前线程设置为头节点，如果不是，则用LockSupport.park(this)将当前线程挂起 ,等待前驱节点释放unpark唤醒自己。 ReentrantLock实现原理ReentrantLock是可重入锁，通过判断上次获取锁的线程是否为当前线程(current == getExclusiveOwnerThread())，如果是则可再次进入临界区并且增加同步状态值返回最后true，如果不是，则返回false。当释放锁时也要减小同步状态值。 ReentrantLock可实现公平锁，通过构造传参的方式。在非公平锁的基础上加入了对同步队列中当前节点是否有前驱节点的判断，如果该 方法返回true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。 一、Lock锁是用来控制多个线程访问共享资源的方式。在Lock接 口出现之前，Java程序是靠synchronized关键字实现锁功能的，而Java SE 5之后，并发包中新增 了Lock接口（以及相关实现类）用来实现锁功能，它提供了synchronized关键字类似的同步功 能，只是在使用时需要显式地获取和释放锁。 来看看源码以及方法，这些方法都是由子类具体实现的 public interface Lock &#123; void lock(); void lockInterruptibly() throws InterruptedException; boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; void unlock(); Condition newCondition();&#125; 与Synchronized锁区别 二、什么是AQS2.1 概述AQS是AbustactQueuedSynchronizer（队列同步器）的简称，它是一个Java提供的底层同步工具类，用一个int类型的变量表示同步状态，并提供了一系列的CAS操作来管理这个同步状态。 AQS的主要作用是为Java中的并发同步组件提供统一的底层支持，例如ReentrantLock，CountdowLatch就是基于AQS实现的，用法是通过继承AQS实现其模版方法，然后将子类作为同步组件的内部类。 AQS中可重写的方法分为独占式与共享式的 可以直接调用的模板方法有 同步器提供的如下3个方法来访问或修改同步状态。·getState()：获取当前同步状态。·setState(int newState)：设置当前同步状态。·compareAndSetState(int expect,int update)：使用CAS设置当前状态，该方法能够保证状态设置的原子性。 2.2 实现2.2.1 同步队列同步队列是AQS很重要的组成部分，它是一个双端队列，遵循FIFO原则，主要作用是用来存放在锁上阻塞的线程，当一个线程尝试获取锁时，如果已经被占用，获取锁失败那么当前线程就会被构造成一个Node节点加入到同步队列的尾部，队列的头节点是成功获取锁的节点，当头节点线程释放锁时，会唤醒后面的节点并释放当前头节点的引用 同步队列中的节点（Node）用来保存获取同步状态失败的线程引用、等待状态以及前驱和 后继节点 使用CAS将节点插入到尾部，并用tail指向该结点 ####2.2.2 独占锁的获取和释放流程获取 调用入口方法acquire(arg) 调用模版方法tryAcquire(arg)尝试获取锁，若成功则返回，若失败则走下一步 将当前线程构造成一个Node节点，并利用addWaiter(Node node) 将其加入到同步队列尾部 调用acquireQueued(Node node,int arg)方法，使得该 节点以“死循环”的方式获取同步状态 自旋时，首先判断其前驱节点为头节点且释放&amp;是否成功获取同步状态，两个条件都成立，则将当前线程的节设置为头节点，如果不是，则利用LockSupport.park(this)将当前线程挂起 ,等待前驱节点释放唤醒自己，之后继续判断。 释放 调用入口方法release(arg) 调用模版方法tryRelease(arg)释放同步状态 利用LockSupport.unpark(currentNode.next.thread)唤醒后继节点（接获取的第五步） ####2.2.3 共享锁的获取和释放流程 共享式获取与独占式获取最主要的区别在于同一时刻能否有多个线程同时获取到同步状态 获取锁 在acquireShared(int arg)方法中，同步器调用tryAcquireShared(int arg)方法尝试获取同步状态 tryAcquireShared(int arg)方法返回值为int类型，当返回值大于等于0时，表示能够获取到同步状态。因此，在共享式获取的自旋过程中，成功获取到同步状态并退出自旋的条件就是 tryAcquireShared(int arg)方法返回值大于等于0。 可以看到，在doAcquireShared(int arg)方法的自 旋过程中，如果当前节点的前驱为头节点时，尝试获取同步状态，如果返回值大于等于0，表示该次获取同步状态成功并从自旋过程中退出。 释放锁 调用releaseShared(arg)模版方法释放同步状态 调用模版方法tryReleaseShard(arg)释放同步状态 如果释放成功，则遍历整个队列，利用LockSupport.unpark(nextNode.thread)唤醒所有后继节点 与独占式区别在于线程安全释放，通过循环和CAS保证，因为释放同步状态的操作会同时来自多个线程 2.2.4 独占锁和共享锁在实现上的区别 独占锁的同步状态值为1，即同一时刻只能有一个线程成功获取同步状态 共享锁的同步状态&gt;1，取值由上层同步组件确定 独占锁队列中头节点运行完成后释放它的直接后继节点 共享锁队列中头节点运行完成后释放它后面的所有节点 共享锁中会出现多个线程（即同步队列中的节点）同时成功获取同步状态的情况 2.2.5 重入锁重入锁指的是当前线程成功获取锁后，如果再次访问该临界区，则不会对自己产生互斥行为。Java中ReentrantLock和synchronized都是可重入锁，synchronized由JVM偏向锁实现可重入锁，ReentrantLock可重入性基于AQS实现。 重入锁的基本原理是判断上次获取锁的线程是否为当前线程(current == getExclusiveOwnerThread())，如果是则可再次进入临界区，如果不是，则阻塞。 final boolean nonfairTryAcquire(int acquires) &#123; //获取当前线程 final Thread current = Thread.currentThread(); //通过AQS获取同步状态 int c = getState(); //同步状态为0，说明临界区处于无锁状态， if (c == 0) &#123; //修改同步状态，即加锁 if (compareAndSetState(0, acquires)) &#123; //将当前线程设置为锁的owner setExclusiveOwnerThread(current); return true; &#125; &#125; //如果临界区处于锁定状态，且上次获取锁的线程为当前线程 else if (current == getExclusiveOwnerThread()) &#123; //则递增同步状态 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 如果是获取锁的线程再次请求，则将同步状态值进行增加并返回 true，表示获取同步状态成功。 成功获取锁的线程再次获取锁，只是增加了同步状态值，这也就要求ReentrantLock在释放 同步状态时减少同步状态值 2.2.6 公平锁和非公平锁对于非公平锁，其它线程可以和同步队列中的锁进行竞争，只要CAS设置 同步状态成功，则表示当前线程获取了锁，而公平锁则不同 protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; //此处为公平锁的核心，即判断同步队列中当前节点是否有前驱节点 if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; &#125; return false;&#125; 该方法与nonfairTryAcquire(int acquires)比较，唯一不同的位置为判断条件多了 hasQueuedPredecessors()方法，即加入了同步队列中当前节点是否有前驱节点的判断，如果该 方法返回true，则表示有线程比当前线程更早地请求获取锁，因此需要等待前驱线程获取并释放锁之后才能继续获取锁。 2.2.7 读写锁Java提供了一个基于AQS到读写锁实现ReentrantReadWriteLock，该读写锁到实现原理是：将同步变量state按照高16位和低16位进行拆分，高16位表示读锁，低16位表示写锁。 写锁的获取与释放写锁是一个独占锁，所以我们看一下ReentrantReadWriteLock中tryAcquire(arg)的实现： protected final boolean tryAcquire(int acquires) &#123; Thread current = Thread.currentThread(); int c = getState(); int w = exclusiveCount(c); if (c != 0) &#123; if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(\"Maximum lock count exceeded\"); // Reentrant acquire setState(c + acquires); return true; &#125; if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true; &#125; 上述代码的处理流程已经非常清晰： 获取同步状态，并从中分离出低16位的写锁状态 如果同步状态不为0，说明存在读锁或写锁 如果存在读锁（c ！=0 &amp;&amp; w == 0），则不能获取写锁（保证写对读的可见性） 如果当前线程不是上次获取写锁的线程，则不能获取写锁（写锁为独占锁） 如果以上判断均通过，则在低16为写锁同步状态上利用CAS进行修改（增加写锁同步状态，实现可重入）将当前线程设置为写锁的获取线程 写锁的释放过程与独占锁基本相同： protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); int nextc = getState() - releases; boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); setState(nextc); return free; &#125; 在释放的过程中，不断减少读锁同步状态，只为同步状态为0时，写锁完全释放。 读锁的获取与释放 读锁是一个共享锁，获取读锁的步骤如下： 获取当前同步状态 计算高16为读锁状态+1后的值 如果大于能够获取到的读锁的最大值，则抛出异常 如果存在写锁并且当前线程不是写锁的获取者，则获取读锁失败 如果上述判断都通过，则利用CAS重新设置读锁的同步状态 读锁的释放步骤与写锁类似，即不断的释放写锁状态，直到为0时，表示没有线程获取读锁。 三、使用AQS与Lock自定义一个锁class Mutex implements Lock &#123; // 静态内部类，自定义同步器 private static class Sync extends AbstractQueuedSynchronizer &#123; // 是否处于占用状态 protected boolean isHeldExclusively() &#123; return getState() == 1; &#125; // 当状态为0的时候获取锁 public boolean tryAcquire(int acquires) &#123; if (compareAndSetState(0, 1)) &#123; setExclusiveOwnerThread(Thread.currentThread()); return true; &#125; return false; &#125; // 释放锁，将状态设置为0 protected boolean tryRelease(int releases) &#123; if (getState() == 0) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(0); return true; &#125; // 返回一个Condition，每个condition都包含了一个condition队列 Condition newCondition() &#123; return new ConditionObject(); &#125; &#125; // 仅需要将操作代理到Sync上即可 private final Sync sync = new Sync(); public void lock() &#123; sync.acquire(1); &#125; public boolean tryLock() &#123; return sync.tryAcquire(1); &#125; public void unlock() &#123; sync.release(1); &#125; public Condition newCondition() &#123; return sync.newCondition(); &#125; public boolean isLocked() &#123; return sync.isHeldExclusively(); &#125; public boolean hasQueuedThreads() &#123; return sync.hasQueuedThreads(); &#125; public void lockInterruptibly() throws InterruptedException &#123; sync.acquireInterruptibly(1); &#125; public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException&#123; return sync.tryAcquireNanos(1, unit.toNanos(timeout)); &#125; &#125; 流程： 这个自定义类Mutex首先实现了Lock接口， 内部静态类Sync继承了AQS抽象类，并重写了独占式的tryAcquire和tryRelease方法， 接着Mutex实例化Sync内部类， Mutex类重写Lock接口的方法，如lock、tryLock、unlock等方法，具体实现是通过调用Sync类中的重写的方法（tryAcquire）以及模板方法（acquire）等 用户使用Mutex时调用Mutex提供的方法，在Mutex的实现中，调用同步器的模板方法acquire(int args)","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"锁","slug":"Java/锁","permalink":"http://hofe.work/categories/Java/%E9%94%81/"}],"tags":[{"name":"Lock","slug":"Lock","permalink":"http://hofe.work/tags/Lock/"},{"name":"AQS","slug":"AQS","permalink":"http://hofe.work/tags/AQS/"}]},{"title":"SpringBoot核心概念","slug":"Java/SpringBoot面试题锦集","date":"2020-05-22T04:35:00.000Z","updated":"2020-05-29T13:39:34.790Z","comments":true,"path":"2020/05/22/Java/SpringBoot面试题锦集/","link":"","permalink":"http://hofe.work/2020/05/22/Java/SpringBoot%E9%9D%A2%E8%AF%95%E9%A2%98%E9%94%A6%E9%9B%86/","excerpt":"","text":"1、什么是 Spring Boot？Spring Boot 是 Spring 开源组织下的子项目，是 Spring 组件一站式解决方案，主要是简化了使用 Spring 的难度，简省了繁重的配置，提供了各种启动器，开发者能快速上手 2、自动装配原理在SpringBoot启动类中会加注解@SpringBootApplication 点入注解之后会发现封装了@ComponentScan、@SpringBootConfiguration、@EnableAutoConfiguration注解 @ComponentScan：自动扫描并加载符合条件的Bean到容器中 @SpringBootConfiguration：对Configuration注解的一个封装，指明是配置类 @EnableAutoConfiguration：是主要的，利用@Import注解，将所有符合自动装配条件的bean注入到IOC容器中 总结： @EnableAutoConfiguration作用就是从classpath中搜寻所有的META-INF/spring.factories配置文件，并将其中org.springframework.boot.autoconfigure.EnableutoConfiguration对应的配置项通过反射（Java Refletion）实例化为对应的标注了@Configuration的JavaConfig形式的IoC容器配置类，然后汇总为一个并加载到IoC容器。这些功能配置类要生效的话，会去classpath中找是否有该类的依赖类（也就是pom.xml必须有对应功能的jar包才行）并且配置类里面注入了默认属性值类，功能类可以引用并赋默认值。生成功能类的原则是自定义优先，没有自定义时才会使用自动装配类。 @Import(&#123;AutoConfigurationImportSelector.class&#125;)public @interface EnableAutoConfiguration &#123; String ENABLED_OVERRIDE_PROPERTY = \"spring.boot.enableautoconfiguration\"; Class&lt;?&gt;[] exclude() default &#123;&#125;; String[] excludeName() default &#123;&#125;;&#125; AutoConfigurationImportSelector.class类中有个方法selectImports，首先判断是否禁用了自动配置，没有的话就执行以下步骤： 加载META-INF/spring-autoconfigure-metadata.properties文件 获取注解的属性及其值（PS：注解指的是@EnableAutoConfiguration注解） 在classpath下所有的META-INF/spring.factories文件中查找org.springframework.boot.autoconfigure.EnableAutoConfiguration的值，并将其封装到一个List中返回 对上一步返回的List中的元素去重、排序 依据第2步中获取的属性值排除一些特定的类 对上一步中所得到的List进行过滤，过滤的依据是条件匹配。 public String[] selectImports(AnnotationMetadata annotationMetadata) &#123; if (!this.isEnabled(annotationMetadata)) &#123; return NO_IMPORTS; &#125; else &#123; AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader); AnnotationAttributes attributes = this.getAttributes(annotationMetadata); List&lt;String&gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); configurations = this.removeDuplicates(configurations); Set&lt;String&gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.filter(configurations, autoConfigurationMetadata); this.fireAutoConfigurationImportEvents(configurations, exclusions); return StringUtils.toStringArray(configurations); &#125; &#125;","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"SpringBoot","slug":"Java/SpringBoot","permalink":"http://hofe.work/categories/Java/SpringBoot/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://hofe.work/tags/SpringBoot/"}]},{"title":"Spring核心概念","slug":"Java/Spring面试题锦集","date":"2020-05-22T04:35:00.000Z","updated":"2020-05-26T06:28:39.272Z","comments":true,"path":"2020/05/22/Java/Spring面试题锦集/","link":"","permalink":"http://hofe.work/2020/05/22/Java/Spring%E9%9D%A2%E8%AF%95%E9%A2%98%E9%94%A6%E9%9B%86/","excerpt":"","text":"面试题锦集系列专注于面试题，为的只是回答面试题的时候能组织好语言，所以说明的不会很详细，不过建议需要了解原理，只背面试题可不行。如果需要更详细的内容，可以看我的其他博客 说说Spring的三大核心思想IOC ：控制反转，将对象的创建权交给Spring管理 DI ： 依赖注入，将对象的属性赋值，对象依赖关系维护交给Spring AOP ：面向切面编程，将重复代码与业务逻辑分离 IOC与DISpring的核心思想之一：Inversion of Control , 控制反转 IOC 对象的创建交给外部容器完成，这个就做控制反转，可以解决手动创建对象以及对象依赖问题。 对象间的依赖关系采用依赖注入：dependency injection.来解决 好处： 第一，资源集中管理，实现资源的可配置和易管理。第二，降低了耦合度。 AOPAop： aspect object programming 面向切面编程，将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。 简单来讲就是将重复代码与业务逻辑分离 面向切面编程：对很多功能都有的重复的代码抽取，再在运行的时候往业务方法上动态植入“切面类代码” Spring AOP 和 AspectJ AOP 有什么区别？Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。 Spring AOP 已经集成了 AspectJ 。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单， 如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比Spring AOP 快很多。 Spring 中的 bean 的作用域有哪些? singleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次HTTP请求都会产生一个新的bean，该bean仅在当前HTTP request内有效。 session : 每一次HTTP请求都会产生一个新的 bean，该bean仅在当前 HTTP session 内有效。 global-session：全局session作用域，仅仅在基于portlet的web应用中才有意义，Spring5已经没有了。 Spring 中的单例 bean 的线程安全问题了解吗？单例 bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候，对这个对象的非静态成员变量的写操作会存在线程安全问题。 常见的有两种解决办法： 在Bean对象中尽量避免定义可变的成员变量（不太现实）。 在类中定义一个ThreadLocal成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 SpringMVC流程 纠正： DispatcherServlet 的作用是接收请求，响应结果。 流程说明（重要）： 客户端（浏览器）发送请求，直接请求到 DispatcherServlet。 DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。 解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由 HandlerAdapter 适配器处理。 HandlerAdapter 会根据 Handler来调用真正的处理器开处理请求，并处理相应的业务逻辑。 处理器处理完业务后，会返回一个 ModelAndView 对象，Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器） Spring 框架中用到了哪些设计模式？ 工厂设计模式 : Spring使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 Spring 管理事务的方式有几种？ 编程式事务，在代码中硬编码。(不推荐使用) 声明式事务，在配置文件中配置（推荐使用） 基于XML的声明式事务 基于注解的声明式事务 Spring如何控制事务循环依赖、三级缓存","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://hofe.work/categories/Java/Spring/"}],"tags":[{"name":"Spring","slug":"Spring","permalink":"http://hofe.work/tags/Spring/"}]},{"title":"Java基础核心概念","slug":"Java/概念/Java基础核心概念","date":"2020-05-22T02:30:00.000Z","updated":"2020-06-04T15:25:23.628Z","comments":true,"path":"2020/05/22/Java/概念/Java基础核心概念/","link":"","permalink":"http://hofe.work/2020/05/22/Java/%E6%A6%82%E5%BF%B5/Java%E5%9F%BA%E7%A1%80%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5/","excerpt":"","text":"这篇文章专注于Java基础知识，不涉及List、Map、多线程、锁相关的内容，需要的可以查看我的其他博客 JDK&amp;JRE&amp;JVMJDK（Java Development Kit）是针对 Java 开发员的产品，是整个 Java 的核心，包括了 Java 运行环境 JRE、Java 工具（编译、开发工具）和 Java 核心类库。 Java Runtime Environment（JRE）是运行 JAVA 程序所必须的环境的集合，包含 JVM 标准实 现及 Java 核心类库。 JVM 是 Java Virtual Machine（Java 虚拟机）的缩写，是整个 java 实现跨平台的最核心的部 分，能够运行以 Java 语言写作的软件程序 跨平台字节码是在虚拟机上运行的，而不是编译器。换而言之，是因为 JVM 能跨平台安装，所以 相应JAVA字节码便可以跟着在任何平台上运行。只要JVM自身的代码能在相应平台上运行， 即 JVM 可行，则 JAVA 的程序员就可以不用考虑所写的程序要在哪里运行，反正都是在虚拟 机上运行，然后变成相应平台的机器语言，而这个转变并不是程序员应该关心的 一、基本数据类型0.1*3精度问题(6.6f+1.3f)可参考这篇：https://mp.weixin.qq.com/s?__biz=MzIwNTk5NjEzNw==&amp;mid=2247490447&amp;idx=2&amp;sn=ef68a5adbad88fe4012b78356a25bdf5&amp;chksm=97293289a05ebb9f29e9d6c96d9f86d4f4a7bd69c47d6fe92a3591d3c3ab76701b40f544d4e8&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1590383496510&amp;sharer_shareid=d476d18cbe4a83b141ea1ff413565f8c#rd 二、包装类2.1 为什么需要包装类由于基本数据类型不是对象，所以 java 并不是纯面向对象的语言，好处是效率较高（全部 包装为对象效率较低）。 Java 是一个面向对象的编程语言，基本类型并不具有对象的性质，为了让基本类型也具有 对象的特征，就出现了包装类型（如我们在使用集合类型 Collection 时就一定要使用包装类 型而非基本类型），使得它具有了对象的性质，并且为其 添加了属性和方法，丰富了基本类型的操作。 2.2 自动装箱、自动拆箱（编译器行为）自动装箱：可以将基础数据类型包装成对应的包装类 自动拆箱：可以将包装类转为对应的基础数据类型 Integer i = 10000; // 编译器会改为 new Integer(10000) int i = new Integer(1000);//编译器会修改为 int i = new Integer(1000).intValue(); 自动拆箱时如果包装类是 null，那么会抛出 NPE 2.3 Integer数组范围（-128~127）java中如果Integer不是new出Integer对象,而是Integer.valueOf或者直接赋值如： ​ Integer b1 = 12; ​ Integer b2 = 12; 这种情况是在常量池中开辟出同一个空间来存储12，所以b1和b2都指向12 接下来说说，Integer的缓冲范围，因为不是在堆区new一个对象，那么在常量池中就必须对其的大小范围做出一个规定，就是数值多少的可以存放在缓存内 如果超出了范围，会从堆区new一个Integer对象来存放值 源码中static final int low = -128;规定了下限为-128，但是最大范围没有确定下来，上限可以通过设置JDK的AutoBoxCacheMax参数调整。 所以在比较-128~127内的两个Integer数据时因为都是常量池的对象，所以==或equals都是true；超过这个范围会在堆中new 对象，==比较的是内存地址，返回 false。 2.4 == 与 equals的区别如果两个引用类型变量使用==运算符，那么比较的是地址，它们分别指向的是否是同一地 址的对象。要求是两个对象都不是空值，与空值比较返回 false。 ==不能实现比较对象的值是否相同(无法重写)。 所有对象都有 equals 方法，默认是 Object 类的 equals，其结果与==一样。 如果希望比较对象的值相同，必须重写 equals 方法。 public boolean equals(Object obj) &#123; return (this == obj); &#125; 2.5 hashCode 与 equals的区别Object 中的 equals: public boolean equals(Object obj) &#123; return (this == obj); &#125; equals 方法要求满足： 自反性 a.equals(a) 对称性 x.equals(y) ==y.equals(x) 一致性 x.equals(y) 多次调用结果一致 对于任意非空引用 x，x.equals(null) 应该返回 false Object 中的 hashCode: public native int hashCode(); 它是一个本地方法，它的实现与本地机器有关，这里我们暂且认为他返回的是对象存储的物理位置。 当 equals 方法被重写时，通常有必要重写 hashCode 方法，以维护 hashCode 方法的常规约定：值相同的对象必须有相同的 hashCode。 object1.equals(object2)为 true，hashCode 也相同； hashCode 不同时，object1.equals(object2)为 false； hashCode 相同时，object1.equals(object2)不一定为 true； // 多个key的hash值相同 2.6 hashCode 与 equals重写问题向一个 Hash 结构的集合中添加某个元素时，先调用 hashCode，唯一则存储，不唯一则再调用 equals，结果相同则不再存储，结果不同则散列到其他位置。 2.6.1 为什么要重写equals()方法？因为object中的equals()方法比较的是对象的引用地址是否相等，如果你需要判断对象里的内容是否相等，则需要重写equals()方法。 2.6.2 为什么改写了equals()，也需要改写hashcode()如果你重写了equals，比如说是基于对象的内容实现的，而保留hashCode的实现（基于内存地址的hash值）不变，那么在添加进map中时需要比对hashcode，很可能某两个对象明明是“相等”，而hashCode却不一样。 2.6.3 为什么改写了hashcode()，也需要改写equals()改写hashcode()方法是为了让两个值相同的对象hashcode也一样，而不再是基于内存地址，在map中表现为可能是存储在同一位置的一个对象。而如果不改写equals，还是基于内存地址进行比较，这样的话，两个值相同的对象就不被映射到同一位置。 Hashmap的key可以是任何类型的对象，例如User这种对象，为了保证两个具有相同属性的user的hashcode相同，我们就需要改写hashcode方法，比方把hashcode值的计算与User对象的id关联起来，那么只要user对象拥有相同id，那么他们的hashcode也能保持一致了，这样就可以找到在hashmap数组中的位置了。如果这个位置上有多个元素，还需要用key的equals方法在对应位置的链表中找到需要的元素，所以只改写了hashcode方法是不够的，equals方法也是需要改写。 2.7 StringString 是 final 类，不可被继承，也不可重写一个 java.lang.String（类加载机制）。 一般是使用 StringUtils 来增强 String 的功能。 字符串修改的时候会创建一个新的字符串，编译时会将+转为 StringBuilder 的 append 方法。 注意新的字符串是在运行时在堆里创建的。 String#intern（JDK1.7 之后）JDK1.7 之后 JVM 里字符串常量池放入了堆中，之前是放在方法区。intern()方法设计的初衷，就是重用 String 对象，以节省内存消耗。 一定是 new 得到的字符串才会调用 intern，字符串常量没有必要去 intern。 当调用 intern 方法时，如果池已经包含一个等于此 String 对象的字符串（该对象由equals(Object) 方法确定），则返回池中的字符串。否则，常量池中直接存储堆中该字符串 的引用（1.7 之前是常量池中再保存一份该字符串）。 2.8 StringBuffer与StringBuilderStringBuffer 是线程安全的，StringBuilder 不是线程安全的，但它们两个中的所有方法都是 相同的。StringBuffer 在 StringBuilder 的方法之上添加了 synchronized，保证线程安全。 StringBuilder 比 StringBuffer 性能好10%-15%。 三、关键字final关键字可以修饰类，函数，变量。 被final修饰的类不可以被继承，final类中的方法默认是final的 被final修饰的方法不能被重写 被final修饰的变量是一个常量只能赋值一次 Java中final、finally、finalize的区别与用法final：java中的关键字，修饰符。A).如果一个类被声明为final，就意味着它不能再派生出新的子类，不能作为父类被继承。因此，一个类不能同时被声明为abstract抽象类的和final的类。B).如果将变量或者方法声明为final，可以保证它们在使用中不被改变. 1)被声明为final的变量必须在声明时给定初值，而在以后的引用中只能读取，不可修改。 2)被声明final的方法只能使用，不能重载。finally：java的一种异常处理机制。finally是对Java异常处理模型的最佳补充。finally结构使代码总会执行，而不管无异常发生。使用finally可以维护对象的内部状态，并可以清理非内存资源。特别是在关闭数据库连接这方面，如果程序员把数据库连接的close()方法放到finally中，就会大大降低程序出错的几率。finalize：Java中的一个方法名。Java技术使用finalize()方法在垃圾收集器将对象从内存中清除出去前，做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没被引用时对这个对象调用的。它是在Object类中定义的，因此所的类都继承了它。子类覆盖finalize()方法以整理系统资源或者执行其他清理工作。finalize()方法是在垃圾收集器删除对象之前对这个对象调用的。 static关键字一句话来概括：方便在没有创建对象的情况下来进行调用。 修饰内部类、成员变量、成员方法、代码块 1、static关键字修饰内部类 java里面static一般用来修饰成员变量或函数。但有一种特殊用法是用static修饰内部类，普通类是不允许声明为静态的，只有内部类才可以。下面看看如何使用。 public class StaticTest &#123;//static关键字修饰内部类public static class InnerClass&#123; InnerClass()&#123; System.out.println(\"====== 静态内部类======\"); &#125; public void InnerMethod() &#123; System.out.println(\"===== 静态内部方法=====\"); &#125; &#125; public static void main(String[] args) &#123; //直接通过StaticTest类名访问静态内部类InnerClass InnerClass inner=new StaticTest.InnerClass(); //静态内部类可以和普通类一样使用 inner.InnerMethod(); &#125;&#125;/*输出是* ============= 静态内部类=============* ============= 静态内部方法=============*/ 如果没有用static修饰InterClass，则只能new 一个外部类实例。再通过外部实例创建内部类。 成员内部类和静态内部类的区别： 1）前者只能拥有非静态成员；后者既可拥有静态成员，又可拥有非静态成员 2）前者持有外部类的引用，可以访问外部类的静态成员和非静态成员；后者不持有外部 类的引用，只能访问外部类的静态成员 3）前者不能脱离外部类而存在；后者可以 2、static关键字修饰方法 修饰方法的时候，其实跟类一样，可以直接通过类名来进行调用： public class StaticMethod &#123; public static void test() &#123; System.out.println(\"======= 静态方法====\"); &#125;; public static void main(String[] args) &#123; //方式一：直接通过类名 StaticMethod.test(); //方式二： StaticMethod fdd=new StaticMethod(); fdd.test(); &#125;&#125; 在静态方法中不能访问类的非静态成员变量和非静态成员方法； 在非静态成员方法中是可以访问静态成员方法/变量的； 即使没有显式地声明为 static，类的构造器实际上也是静态方法 3、static关键字修饰变量 被static修饰的成员变量叫做静态变量，也叫做类变量，说明这个变量是属于这个类的，而不是属于是对象，没有被static修饰的成员变量叫做实例变量，说明这个变量是属于某个具体的对象的。 我们同样可以使用上面的方式进行调用变量： public class StaticVar &#123; private static String name=\"java的架构师技术栈\"； public static void main(String[] args) &#123; //直接通过类名 StaticVar.name; &#125;&#125; 静态变量和非静态变量的区别是：静态变量被所有的对象所共享，在内存中只有一个副本， 它当且仅当在类初次加载时会被初始化。而非静态变量是对象所拥有的，在创建对象的时候 被初始化，存在多个副本，各个对象拥有的副本互不影响。静态成员变量并发下不是线程安全的，并且对象是单例的情况下，非静态成员变量也不是线 程安全的。怎么保证变量的线程安全? 只有一个线程写，其他线程都是读的时候，加 volatile；线程既读又写，可以考虑 Atomic 原 子类和线程安全的集合类；或者考虑 ThreadLocal 4、static关键字修饰代码块 用来构造静态代码块以优化程序性能。static 块可以置于类中的任何地方，类中可以有多个 static 块。在类初次被加载的时候，会按照 static 块的顺序来执行每个 static 块，并且只会执 行一次。 静态代码块在类第一次被载入时执行，在这里主要是想验证一下，类初始化的顺序。 父类静态变量、父类静态代码块 子类静态变量、子类静态代码块 父类普通变量、父类普通代码块、父类构造函数 子类普通变量、子类普通代码块、子类构造函数 四、面向对象4.1 面向对象与面向过程的本质的区别在于考虑问题的出发点不同， 面向过程是以事件流程为考虑问题的出发点， 而面向对象则是以参与事件的角色（对象）为考虑问题的出发点 4.2 抽象类与接口区别： 1)抽象类中方法可以不是抽象的；接口中的方法必须是抽象方法； 2)抽象类中可以有普通的成员变量；接口中的变量必须是 static final 类型的，必须被初始 化 , 接口中只有常量，没有变量。 3)抽象类只能单继承，接口可以继承多个父接口； 4)Java8 中接口中会有 default 方法，即方法可以被实现。 使用场景： 如果要创建不带任何方法定义和成员变量的基类，那么就应该选择接口而不是抽象类。 如果知道某个类应该是基类，那么第一个选择的应该是让它成为一个接口，只有在必须要有 方法定义和成员变量的时候，才应该选择抽象类。 因为抽象类中允许存在一个或多个被具体 实现的方法，只要方法没有被全部实现该类就仍是抽象类 4.3 对象三大特性面向对象的三个特性：封装；继承；多态 封装：将数据与操作数据的方法绑定起来，隐藏实现细节，对外提供接口。 继承：代码重用；可扩展性 多态：允许不同子类对象对同一消息做出不同响应多态的三个必要条件：继承、方法的重写、父类引用指向子类对象 封装封装是指将某事物的属性和行为包装到对象中，这个对象只保留一些对外接口使之与外部发生联系。用户无需知道对象内部的细节，但可以通过对象对外提供的接口来访问该对象。 继承子类继承父类的特征和行为，子类可以有父类的方法和属性，子类也可以对父类进行扩展，也可以提供重写的方法；继承的方式有两种：实现继承和接口继承 多态多态就是指多种状态，就是说当一个操作在不同的对象时，会产生不同的结果。多态分为编译时多态和运行时多态，编译时多态主要指方法的重载，运行时多态指程序中定义的对象引用所指向的具体类型在运行期间才确定，运行时多态主要通过重写来实现。多态的作用：消除类型之间的耦合关系。 那么JAVA的多态是怎么实现的？接口实现、抽象类、继承父类进行方法重写、同一个类中进行方法重载。 4.4 JAVA中重载与重写的概念？（Overload）重载：发生在同一个类之中，方法名相同、参数列表不同，与返回值无关、与final无关、与修饰符无关、与异常无关。（Override）重写：发生在子类和父类之间，方法名相同、参数列表相同、返回值相同、不能是final的方法、重写的方法不能有比父类方法更为严格的修饰符权限、重写的方法所抛出的异常不能比父类的更大。 五、引用强引用StringReference GC 时不回收 当内存空间不足，Java 虚拟机宁愿抛出 OutOfMemoryError 错误，使程序异常终止，也不会 靠随意回收具有强引用的对象来解决内存不足问题。 软引用SoftReference GC 时如果 JVM 内存不足时会回收 软引用可用来实现内存敏感的高速缓存。 软引用可以和一个引用队列（ReferenceQueue） 联合使用，如果软引用所引用的对象被垃圾回收，Java 虚拟机就会把这个软引用加入到与 之关联的引用队列中。 弱引用WeakReference GC 时立即回收 弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃 圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。 虚引用PhantomReference 如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收。 虚引用主要用来跟踪对象被垃圾回收的活动。虚引用与软引用和弱引用的一个区别在于：虚 引用必须和引用队列（ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时， 如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用 队列中。程序可以通过判断引用队列中是否已经加入了虚引用，来了解被引用的对象是否将 要被垃圾回收。程序如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对 象的内存被回收之前采取必要的行动。 六、ThreadLocal在线程之间共享变量是存在风险的，有时可能要避免共享变量，使用 ThreadLocal 辅助类为 各个线程提供各自的实例。 每个线程内部都会维护一个类似 HashMap 的对象，称为 ThreadLocalMap，里边会包含 若干了 Entry（K-V 键值对），相应的线程被称为这些 Entry 的属主线程； Entry 的 Key 是一个 ThreadLocal 实例，Value 是一个线程特有对象。Entry 的作用即是： 为其属主线程建立起一个 ThreadLocal 实例与一个线程特有对象之间的对应关系； Entry 对 Key 的引用是弱引用；Entry 对 Value 的引用是强引用。 ThreadLocalMap 的 Key 是弱引用，如果是强引用，ThreadLocal 将无法被释放内存。 因为如果这里使用普通的 key-value 形式来定义存储结构，实质上就会造成节点的生命周期 与线程强绑定，只要线程没有销毁，那么节点在 GC 分析中一直处于可达状态，没办法被回 收，而程序本身也无法判断是否可以清理节点。弱引用是 Java 中四档引用的第三档，比软 引用更加弱一些，如果一个对象没有强引用链可达，那么一般活不过下一次 GC。当某个 ThreadLocal 已经没有强引用可达，则随着它被垃圾回收，在 ThreadLocalMap 里对应的 Entry 的键值会失效，这为 ThreadLocalMap 本身的垃圾清理提供了便利 七、异常try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？会执行 1、不管有没有异常，finally中的代码都会执行2、当try、catch中有return时，finally中的代码依然会继续执行3、finally是在return后面的表达式运算之后执行的，此时并没有return运算之后的值，而是把值保存起来，不管finally对该值做任何的改变，返回的值都不会改变，依然返回保存起来的值。也就是说方法的返回值是在finally运算之前就确定了的。4、如果return的数据是引用数据类型，而在finally中对该引用数据类型的属性值的改变起作用，try中的return语句返回的就是在finally中改变后的该属性的值。5、finally代码中最好不要包含return，程序会提前退出，也就是说返回的值不是try或catch中的值 八、反射概念Java 中的反射机制是指在运行状态中，对于任意一个类都能够知道这个类所有的属性和方法； 并且对于任意一个对象，都能够调用它的任意一个方法；这种动态获取信息以及动态调用对象方 法的功能称为Java 语言的反射机制 Java反射机制的作用？应用场合：在Java程序中许多对象在运行是都会出现两种类型：编译时类型和运行时类型。 编译时的类型由 声明对象时用的类型来决定，运行时的类型由实际赋值给对象的类型决定 。如Person p=new Student();其中编译时类型为Person，运行时类型为Student 程序在运行时还可能接收到外部传入的对象，该对象的编译时类型为 Object,但是程序有需要调用 该对象的运行时类型的方法。为了解决这些问题，程序需要在运行时发现对象和类的真实信息。 此时就必须使用到反射了 。 总结 在运行时能够判断任意一个对象所属的类、创建新类对象 在运行时构造任意一个类的对象、判断任意一个类所具有的成员变量和方法 在运行时调用任一对象的方法 应用场景JDBC中，利用反射动态加载了数据库驱动程序。 很多框架都用到反射机制，注入属性，调用方法，如Spring。 Web服务器中利用反射调用了Sevlet的服务方法。 读取配置文件 如何使用Java的反射?获得Class对象的三种方法 创建对象的两种方法 反射机制的优缺点？优点：可以动态执行，在运行期间根据业务功能动态执行方法、访问属性，最大限度发挥了java的灵活性。缺点：对性能有影响，这类操作总是慢于直接执行java代码。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"基础","slug":"Java/基础","permalink":"http://hofe.work/categories/Java/%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"基础","slug":"基础","permalink":"http://hofe.work/tags/%E5%9F%BA%E7%A1%80/"}]},{"title":"数组题目合集","slug":"数据结构与算法/数组题目合集","date":"2020-05-21T15:25:11.000Z","updated":"2020-06-06T15:47:09.323Z","comments":true,"path":"2020/05/21/数据结构与算法/数组题目合集/","link":"","permalink":"http://hofe.work/2020/05/21/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%95%B0%E7%BB%84%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86/","excerpt":"","text":"使奇数在前，偶数在后输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 解法：遍历一遍获取奇数个数oddCount；新建一个数组，第二次遍历遇到偶数则从数组oddCount位置开始存偶数；最后赋值回原数组。 public class Solution &#123; public void reOrderArray(int [] array) &#123; if(array.length==0||array.length==1)&#123; return; &#125; int oddCount=0,oddBegin=0; int[] newArray = new int[array.length]; for(int i = 0;i &lt; array.length; i++)&#123; if(array[i]%2==1)&#123; oddCount++; &#125; &#125; for(int i=0;i&lt;array.length;i++)&#123; if((array[i]&amp;1)==1)&#123; newArray[oddBegin++]=array[i]; &#125; else&#123; newArray[oddCount++]=array[i]; &#125; &#125; for(int i=0;i&lt;array.length;i++)&#123; array[i]=newArray[i]; &#125; &#125;&#125; 旋转数组的最小元素 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 要注意的是12行的判断条件，array[mid] &lt;= array[right] import java.util.ArrayList;public class Solution &#123; public int minNumberInRotateArray(int [] array) &#123; int len = array.length; if(len == 0)&#123; return 0; &#125; int left = 0; int right = len - 1; while(left &lt; right)&#123; int mid = left + (right-left)/2; if(array[mid] &lt;= array[right])&#123; right = mid; &#125;else&#123; left = mid+1; &#125; &#125; return array[left]; &#125;&#125; 构建乘积数组 给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素B[i]=A[0]A[1]…A[i-1]*A[i+1]…*A[n-1]。不能使用除法。（注意：规定B[0] = A[1] * A[2] * … * A[n-1]，B[n-1] = A[0] * A[1] * … * A[n-2];） 剑指的思路： B[i]的值可以看作下图的矩阵中每行的乘积。 下三角用连乘可以很容求得，上三角，从下向上也是连乘。 因此我们的思路就很清晰了，先算下三角中的连乘，即我们先算出B[i]中的一部分，然后倒过来按上三角中的分布规律，把另一部分也乘进去。 import java.util.ArrayList;public class Solution &#123; public int[] multiply(int[] A) &#123; int len = A.length; int[] B = new int[len]; B[0] = 1; for(int i = 1; i &lt; len; i++)&#123; B[i] = B[i-1] * A[i-1]; &#125; int temp = 1; for(int j = len - 2; j &gt;= 0; j--)&#123; temp = temp * A[j+1]; B[j] = B[j] * temp; &#125; return B; &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数组","slug":"数组","permalink":"http://hofe.work/tags/%E6%95%B0%E7%BB%84/"}]},{"title":"链表题目总结","slug":"数据结构与算法/链表题目总结","date":"2020-05-21T15:25:11.000Z","updated":"2020-06-06T15:46:37.131Z","comments":true,"path":"2020/05/21/数据结构与算法/链表题目总结/","link":"","permalink":"http://hofe.work/2020/05/21/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E9%93%BE%E8%A1%A8%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/","excerpt":"","text":"链表倒数第k个结点输入一个链表，输出该链表中倒数第k个结点。 解法：声明两个指针都指向头结点，其中一个指针cur在前一个指针pre走了k个之后在走，这样当前一个指针pre到达尾巴了，当前指针cur也就到了-k个位置 /*public class ListNode &#123; int val; ListNode next = null; ListNode(int val) &#123; this.val = val; &#125;&#125;*/public class Solution &#123; public ListNode FindKthToTail(ListNode head,int k) &#123; ListNode pre = head; ListNode out = head; // 头结点存有元素 int count = 0; while(pre!=null)&#123; count++; pre = pre.next; if(count &gt; k)&#123; // 让pre先走了count步后，out才开始走 out = out.next; &#125; &#125; if(k == 0 || k &gt; count)&#123; return null; &#125; return out; &#125;&#125; 从头到尾打印链表输入一个链表，按链表从尾到头的顺序返回一个ArrayList。 递归法，需要注意的是list不能用static声明 public class Solution &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; if(listNode!=null)&#123; printListFromTailToHead(listNode.next); list.add(listNode.val); &#125; return list; &#125;&#125; 栈存放 import java.util.*;public class Solution &#123; public static ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode)&#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); ListNode p = listNode; if(p == null)&#123; return new ArrayList(); &#125; while(p!=null)&#123; stack.push(p.val); p = p.next; &#125; while(!stack.isEmpty())&#123; list.add(stack.pop()); &#125; return list; &#125;&#125; 反转链表输入一个链表，反转链表后，输出新链表的表头。 public class Solution &#123; public ListNode ReverseList(ListNode head) &#123; // 这里有个需要注意的地方，就是如果两个判断合在一起 // 即if(head == null || head.next == null) return head // head == null 需要在前！如果head已经是null了，它的next肯定也是null，只不过好像返回值不同 if(head == null)&#123; return null; &#125; if(head.next == null)&#123; return head; &#125; ListNode p = head; ListNode q = null; while(p!=null)&#123; ListNode temp = p.next; p.next = q; q = p; p = temp; &#125; return q; &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"链表","slug":"链表","permalink":"http://hofe.work/tags/%E9%93%BE%E8%A1%A8/"}]},{"title":"谈谈单点登录","slug":"微服务分布式集群/谈谈单点登录","date":"2020-05-21T06:35:45.000Z","updated":"2020-06-02T01:42:00.682Z","comments":true,"path":"2020/05/21/微服务分布式集群/谈谈单点登录/","link":"","permalink":"http://hofe.work/2020/05/21/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/%E8%B0%88%E8%B0%88%E5%8D%95%E7%82%B9%E7%99%BB%E5%BD%95/","excerpt":"","text":"转载自https://zhuanlan.zhihu.com/p/66037342 一、单系统登录HTTP是无状态的协议，这意味着服务器无法确认用户的信息。于是，W3C就提出了：给每一个用户都发一个通行证，无论谁访问的时候都需要携带通行证，这样服务器就可以从通行证上确认用户的信息。通行证就是Cookie。 如果说Cookie是检查用户身上的”通行证“来确认用户的身份，那么Session就是通过检查服务器上的”客户明细表“来确认用户的身份的。Session相当于在服务器中建立了一份“客户明细表”。 HTTP协议是无状态的，Session不能依据HTTP连接来判断是否为同一个用户。于是服务器向用户浏览器发送了一个名为JESSIONID的Cookie，它的值是Session的id值。其实Session是依据Cookie来识别是否是同一个用户。 所以，一般我们单系统实现登录会这样做： 登录：将用户信息保存在Session对象中 如果在Session对象中能查到，说明已经登录 如果在Session对象中查不到，说明没登录（或者已经退出了登录） 注销（退出登录）：从Session中删除用户的信息 记住我（关闭掉浏览器后，重新打开浏览器还能保持登录状态）：配合Cookie来用 二、单点登录单点登录要解决的问题就是一个子系统登录了，其它子系统就不必再登陆。 之前的做法是判断session里有没有用户信息了，有就说明已经登录了；没有就说明没有登录。单登陆系统因为Session是存在一台服务器上的，所以只有一个Session，但分布式系统有多个Session，需要让多个子系统共用一个Session才行，这能做到但是不合理。 于是就采用将登陆功能单独抽取出来，每个子系统登陆都把请求发送到该登陆系统。登陆系统在用户第一次登陆之后，随机生成uuid作为key，user信息作为value存入redis，设置过期时间，这样就保留了这次的登陆信息，同时把这个uuid写入客户端的Cookie。之后其它子系统在发起请求时携带着cookie，会先向登陆系统发起请求，如果登陆系统中的redis存在这条uuid对应的用户信息，说明目前已经登录了，则放行直接登陆，否则重定向到登陆页面。 2.1 Session不共享问题以往单系统登录功能主要是用Session保存用户信息来实现的，但我们清楚的是：多系统即可能有多个Tomcat，而Session是依赖当前系统的Tomcat，所以系统A的Session和系统B的Session是不共享的。那么就需要有一个机制能够做到一个系统登录后，其它系统不用再重复登录。 解决系统之间Session不共享问题有一下几种方案： Tomcat集群Session全局复制（集群内每个tomcat的session完全同步）【会影响集群的性能呢，不建议】 根据请求的IP进行Hash映射到对应的机器上（这就相当于请求的IP一直会访问同一个服务器）【如果服务器宕机了，会丢失了一大部分Session的数据，不建议】 把Session数据放在Redis中（使用Redis模拟Session）【建议】 我们可以将登录功能单独抽取出来，做成一个子系统。 这样，当登录SSO系统的时候，验证核对密码后，将随机生成的token作为key，以及用户信息作为value写入redis。这样服务器就会保留这次的登陆信息。 其它子系统登录时，将请求转发给SSO进行登陆，因为SSO系统的redis有存token以及用户信息，所以验证之后可以得到token，再把token写入到子系统的cookie中（或者在上一步骤服务器就把token写入客户端也行，这次就直接携带含token的cookie就行；在这一步写cookie的话，需要在请求中携带用户名，密码用于验证，验证通过才写入Cookie） 此后，每次请求时，Cookie都会带上token，拦截器得到token，判断是否已经登录 到这里，其实我们会发现其实就两个变化： 将登陆功能抽取为一个系统（SSO），其他系统请求SSO进行登录 本来将用户信息存到Session，现在将用户信息存到Redis 2.2 Cookie跨域的问题上面我们解决了Session不能共享的问题，但其实还有另一个问题。Cookie是不能跨域的 比如说，我们请求https://www.google.com/时，浏览器会自动把google.com的Cookie带过去给google的服务器，而不会把https://www.baidu.com/的Cookie带过去给google的服务器。 这就意味着，由于域名不同，用户向系统A登录后，系统A返回给浏览器的Cookie，用户再请求系统B的时候不会将系统A的Cookie带过去。 针对Cookie存在跨域问题，有几种解决方案： 服务端将Cookie写到客户端后，客户端对Cookie进行解析，将Token解析出来，此后请求都把这个Token带上就行了 多个域名共享Cookie，在写到客户端的时候设置Cookie的domain。 将Token保存在SessionStroage中（不依赖Cookie就没有跨域的问题了） 到这里，我们已经可以实现单点登录了。 2.3 CAS原理说到单点登录，就肯定会见到这个名词：CAS （Central Authentication Service），下面说说CAS是怎么搞的。 如果已经将登录单独抽取成系统出来，我们还能这样玩。现在我们有两个系统，分别是www.java3y.com和www.java4y.com，一个SSOwww.sso.com 首先，用户想要访问系统Awww.java3y.com受限的资源(比如说购物车功能，购物车功能需要登录后才能访问)，系统Awww.java3y.com发现用户并没有登录，于是重定向到sso认证中心，并将自己的地址作为参数。请求的地址如下： www.sso.com?service=www.java3y.com sso认证中心发现用户未登录，将用户引导至登录页面，用户进行输入用户名和密码进行登录，用户与认证中心建立全局会话（生成一份Token，写到Cookie中，保存在浏览器上） 随后，认证中心重定向回系统A，并把Token携带过去给系统A，重定向的地址如下： www.java3y.com?token=xxxxxxx 接着，系统A去sso认证中心验证这个Token是否正确，如果正确，则系统A和用户建立局部会话（创建Session）。到此，系统A和用户已经是登录状态了。 此时，用户想要访问系统Bwww.java4y.com受限的资源(比如说订单功能，订单功能需要登录后才能访问)，系统Bwww.java4y.com发现用户并没有登录，于是重定向到sso认证中心，并将自己的地址作为参数。请求的地址如下： www.sso.com?service=www.java4y.com 注意，因为之前用户与认证中心www.sso.com已经建立了全局会话（当时已经把Cookie保存到浏览器上了），所以这次系统B重定向到认证中心www.sso.com是可以带上Cookie的。 认证中心根据带过来的Cookie发现已经与用户建立了全局会话了，认证中心重定向回系统B，并把Token携带过去给系统B，重定向的地址如下： www.java4y.com?token=xxxxxxx 接着，系统B去sso认证中心验证这个Token是否正确，如果正确，则系统B和用户建立局部会话（创建Session）。到此，系统B和用户已经是登录状态了。 看到这里，其实SSO认证中心就类似一个中转站。","categories":[{"name":"分布式","slug":"分布式","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"SSO","slug":"SSO","permalink":"http://hofe.work/tags/SSO/"}]},{"title":"Java中的浅拷贝与深拷贝","slug":"Java/Java中的浅拷贝与深拷贝","date":"2020-05-21T03:44:27.000Z","updated":"2020-05-25T02:13:45.830Z","comments":true,"path":"2020/05/21/Java/Java中的浅拷贝与深拷贝/","link":"","permalink":"http://hofe.work/2020/05/21/Java/Java%E4%B8%AD%E7%9A%84%E6%B5%85%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B7%B1%E6%8B%B7%E8%B4%9D/","excerpt":"","text":"Java 中的数据类型分为基本数据类型和引用数据类型。对于这两种数据类型，在进行赋值操作、用作方法参数或返回值时，会有值传递和引用（地址）传递的差别。 一、拷贝分类 引用拷贝 （==） 对象拷贝 （.clone()） 浅拷贝（.clone()） 深拷贝（.clone().clone()） 注意：浅拷贝与深拷贝都属于对象拷贝 浅拷贝：基本数据类型拷贝值，引用类型拷贝内存地址，修改基本数据类型不会对原对象产生影响，修改引用类型会对原对象产生影响。被拷贝对象通过实现 Cloneable 并重写 clone() 方法来实现浅拷贝。 深拷贝：基本数据类型拷贝值，引用类型拷贝会新建一个对象空间（新地址）拷贝里面的内容，修改基本数据类型和修改引用类型都不会对原对象产生影响。有多层对象的，每个对象都需要实现 Cloneable 并重写 clone() 方法，进而实现了对象的串行层层拷贝。 二、引用拷贝拷贝的是内存地址，所以更改拷贝值也会更改原值 Teacher teacher = new Teacher(\"Taylor\",26);Teacher otherteacher = teacher;System.out.println(teacher);System.out.println(otherteacher);输出blog.Teacher@355da254blog.Teacher@355da254 三、对象拷贝地址是不同的，也就是说创建了新的对象， 而不是把原对象的地址赋给了一个新的引用变量,这就叫做对象拷贝（浅拷贝的默认实现）。 Teacher teacher = new Teacher(\"Swift\",26);Teacher otherteacher = (Teacher)teacher.clone();System.out.println(teacher);System.out.println(otherteacher);blog.Teacher@355da254blog.Teacher@4dc63996 3.1 浅拷贝浅拷贝是按位拷贝对象，它会创建一个新对象，这个对象有着原始对象属性值的一份精确拷贝。如果属性是基本类型，拷贝的就是基本类型的值；如果属性是内存地址（引用类型），拷贝的就是内存地址 ，因此如果其中一个对象改变了这个地址，就会影响到另一个对象。即默认拷贝构造函数只是对对象进行浅拷贝复制(逐个成员依次拷贝)，即只复制对象空间而不复制资源。 3.2 深拷贝深拷贝，在拷贝引用类型成员变量时，为引用类型的数据成员另辟了一个独立的内存空间，实现真正内容上的拷贝。基础数据类型或者引用类型，对其中一个对象修改该值，不会影响另外一个。每个对象都需要实现 Cloneable 并重写 clone() 方法，进而实现了对象的串行层层拷贝。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"基础","slug":"Java/基础","permalink":"http://hofe.work/categories/Java/%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"深浅拷贝","slug":"深浅拷贝","permalink":"http://hofe.work/tags/%E6%B7%B1%E6%B5%85%E6%8B%B7%E8%B4%9D/"}]},{"title":"理解同步、异步、阻塞和非阻塞","slug":"Java/Java IO 学习笔记","date":"2020-05-21T03:44:27.000Z","updated":"2020-06-05T04:13:11.227Z","comments":true,"path":"2020/05/21/Java/Java IO 学习笔记/","link":"","permalink":"http://hofe.work/2020/05/21/Java/Java%20IO%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"理解同步、异步、阻塞和非阻塞关于同步、异步、阻塞和非阻塞这个概念性问题，这可能是非常容易混淆的概念之一，特别是那些刚开始解除网络编程的人来说。本篇文章争取来说清楚这个问题，如果有错误之处，恳请批评指正。 写在前面首先大家心中需要有以下的清晰认知： 阻塞操作不等于同步（blocking operation does NOT equal to synchronous） 非阻塞操作不等于异步（non-blocking operation does NOT equal to asynchronous） 事实上，同步异步于阻塞和非阻塞没有什么直接的关联关系。 同步和异步同步和异步关注的是 通信机制 (communication mechanism) 同步是指在发出一个function调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到调用结果了。这个结果可能是一个正确的期望结果，也可能是因为异常原因（比如超时）导致的失败结果。换句话说，就是由调用者主动等待这个调用的结果。 Synchronous is, when we started a function call, the call will not return anything until it gets the result, the function needs to finish everything before it can give anything to us. 异步是调用在发出之后，本次调用过程就直接返回了，并没有同时没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态变化、事件通知等机制来通知调用者，或通过回调函数处理这个调用。 Asynchronous does not need to wait for the function completes its operation, once we call it, it returns immediately without any result, the function uses callback function (or other notification method) to “notify” us to get the value after it completes execution. 阻塞和非阻塞阻塞和非阻塞关注的是 程序在等待调用结果（消息、返回值）时的状态. Unlike synchronous/asynchronous, blocking/non-blocking focuses on the status of the program while waiting for the result from the function call. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回 A blocking operation hangs up current thread before it gets the result, in other words, a blocking operation will let the current thread wait for the result returns, even if the target function will use a callback function to notify client side to fetch the result, the thread on the client side will still be blocked until it gets the returned result. 非阻塞是指在不能立刻得到结果之前，该调用不会阻塞当前线程。 the non-blocking operation will not hang up the current thread if no result returned immediately. 对unix操作系统来讲： 阻塞式I/O(默认)，非阻塞式I/O(nonblock)，I/O复用(select/poll/epoll)都属于同步I/O，因为它们在操作系统将数据由内核缓冲区复制到用户空间缓冲区时都是阻塞的(不能干别的事)。 只有异步I/O模型(AIO)是符合异步I/O操作的含义的，即在数据准备完成以后，由内核空间拷贝回用户缓冲区后再通知通知用户进程，而用户进程在等待通知的这段时间里可以干别的事。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"IO","slug":"Java/IO","permalink":"http://hofe.work/categories/Java/IO/"}],"tags":[{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"IO","slug":"IO","permalink":"http://hofe.work/tags/IO/"}]},{"title":"一文详解synchronized","slug":"Java/一文详解Synchronized","date":"2020-05-21T02:00:27.000Z","updated":"2020-05-25T01:46:08.968Z","comments":true,"path":"2020/05/21/Java/一文详解Synchronized/","link":"","permalink":"http://hofe.work/2020/05/21/Java/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3Synchronized/","excerpt":"","text":"一、Synchronized锁简介Java的关键字，jvm层面上，可重入，不可中断，非公平的锁。 JDK1.6之前加重量级锁，每次加锁都通过监视器(monitor)来实现，但监视器本质又依赖于操作系统Mutex Lock实现，涉及到用户态和内核态的切换，消耗较高； JDK1.6之后加锁分为加偏向锁和加轻量级锁，当没有线程竞争的时候加偏向锁，在只有两个线程交替执行同步块的时候加轻量级锁，多个线程竞争升级到重量级锁。 实现原理 monitorenter 和 monitorexit 是上层指令，底层实现可能是偏向锁、轻量级锁、重量级锁 等 加偏向锁的过程 一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。如果测试成功，表示线程已经获得了锁。如果测试失败，则需要再测试一下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁）：如果没有设置，则使用CAS竞争锁；如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。 加轻量级锁的过程 线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程则自旋重试。重试一定次数后则膨胀为重量级锁（修改Mark Word，改为指向重量级锁的指针） 膨胀到重量级锁的过程 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。 因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。 二、三种作用范围 在静态方法上加锁； 在非静态方法上加锁； 在代码块上加锁； 下面是这三种情况： public class SynchronizedSample &#123; private final Object obj = new Object(); privatestaticint money = 0; //非静态方法 public synchronized void noStaticMethod()&#123; money++; &#125; //静态方法 public static synchronized void staticMethod()&#123; money++; &#125; public void codeBlock()&#123; //代码块 synchronized (obj)&#123; money++; &#125; &#125;&#125; 那么经常会有面试题问这三种作用范围是加在哪 我在网上看到比较普遍的一种答案是 在静态方法上加锁；——–锁类 在非静态方法上加锁；—–锁对象 在代码块上加锁；———–锁对象 这种说法不能说错的，但是不准确的！首先明确一点：锁是加在对象上面的，我们是在对象上加锁 作用范围 锁对象 非静态方法 当前对象 =&gt; this 静态方法 类对象 =&gt; SynchronizedSample.class （一切皆对象，这个是类对象） 代码块 指定对象 =&gt; obj（以上面的代码为例） 2.1 底层实现synchronized 在代码块上是通过 monitorenter 和 monitorexit指令实现，在静态方法和 方法上加锁是在方法的flags 中加入 ACC_SYNCHRONIZED 。JVM 运行方法时检查方法的flags，遇到同步标识开始启动加锁流程，在方法内部遇到monitorenter指令开始加锁。 monitorenter/monitorexit monitorenter指令时，首先要去尝试获取对象的锁。如果 这个对象没被锁定，或者当前线程已经持有了那个对象的锁，就把锁的计数器的值增加一，而在执行 monitorexit指令时会将锁计数器的值减一。一旦计数器的值为零，锁随即就被释放了。如果获取对象 锁失败，那当前线程就应当被阻塞等待，直到请求锁定的对象被持有它的线程释放为止 monitorenter 和 monitorexit 是上层指令，底层实现可能是偏向锁、轻量级锁、重量级锁 等。 那以代码块加重量级锁解释下为什么其它线程就不能访问了： 当前对象A，声明了个用来给自己作为锁的obj对象，当A有一段代码需要同步时，就对obj加锁。这段代码前后也就标识了monitorenter 和 monitorexit指令，触发了争夺锁的机制，如果成功则让与obj关联的monitor监视器对象的owner指针指向A，同时将A对象头的markword中重量级锁需要的监视器地址设置为obj关联的monitor对象地址，这样monitor知道谁拥有对象锁，线程A也知道自己有monitor也就有锁了。其它线程在进入这个方法前，争夺obj的monitor，失败就被阻塞，直到下一次参与竞争。 三、synchronized 实现原理上面说了，遇到monitorenter指令或者ACC_synchronized时会加锁，首先要去尝试获取对象的锁，这个锁就是我们现在要讲的对象监视器。 3.1 先了解下Mark Word和Monitor对象3.1.1 Mark Word首先synchronized 锁 和 对象头息息相关。Java 对象头，锁的类型和状态和对象头的Mark Word息息相关； synchronized 锁 – 对象头 – Mark Word Mard Word存储对象的hashCode、锁信息或分代年龄或GC标志等信息 这里大家注意重量级锁需要有个指向互斥量的指针，这里的互斥量指的就是下面要说的ObjectMonitor，我更喜欢把它叫做内置锁，更能表达含义。轻量级有个指向栈中锁记录的指针，偏向锁有个线程ID、Epoch以及用来区别无锁的字段（因为锁标志位都是00）。那是不是意味着满足对应条件就能实现加对应锁？ 3.1.2 Monitor每个对象都有一个与之关联的Monitor 对象，即内置锁 ObjectMonitor() &#123; _header = NULL; _count = 0; // 重入次数 _waiters = 0, // 等待线程数 _recursions = 0; _object = NULL; _owner = NULL; // 当前持有锁的线程 _WaitSet = NULL; // 调用了 wait 方法的线程被阻塞 放置在这里 _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; // 等待锁 处于block的线程 有资格成为候选资源的线程 _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; &#125; 看这个代码以及注释，先不看下面的解析，思考下这些变量的含义。以下这段是我的猜想（是有误的） _count：重入次数代表是可重入的，且重入一次，_count++，退出的时候也要退出到count==0； _owner：拥有当前锁的线程，说明获取锁了，这个需要指向线程吧； _WaitSet：阻塞队列？不懂，但从名字可以想到和wait有关，应该是调用wait方法被阻塞的线程存放的队列； _EntryList：和_WaitSet一样，是阻塞队列还是等待队列？这里看不出来 根据这些注释，我们大概有了点思路，多线程竞争锁，竞争不到，则会被放入队列中，竞争到了则会被指向，并且还是可重入的。至于上面的猜想是否正确，可以继续往下看。 对象关联的 ObjectMonitor 对象监视器有一个线程内部竞争锁的机制 3.2 JDK 6 以前 synchronized实现原理 JDK 6 以前 synchronized即通过重量级锁实现，而这些操作的实现又依赖于操作系统底层的Mutex Lock。 3.2.1 重量级锁实现原理过程： JVM每次会从队列尾部取出数据作为OnDeck竞争者，但在并发情况下Contention List会被大量进程CAS访问 为了降低尾部竞争，Owner线程在unlock时，Contention List 中那些有资格成为候选资源的线程被移动到 Entry List 中; 从EntryList取出一个线程放到OnDeck作为候选者； 任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为 OnDeck; 当前已经获取到所资源的线程被称为 Owner; Owner调用wait被阻塞，则进入WaitSet队列，等待被notify唤醒进入EntryList 处于 ContentionList、EntryList、WaitSet 中的线程都处于阻塞状态，该阻塞是由操作系统来完成的(Linux 内核下采用 pthread_mutex_lock 内核函数实现的); 举例： A线程抢先拿到了锁。拿到锁的步骤为： 将 MonitorObject 中的 _owner设置成 A线程； 将 mark word 设置为 Monitor 对象地址，锁标志位改为10(重量级锁)； 将B线程阻塞放到 ContentionList 队列； 作为Owner 的A 线程执行过程中，可能调用wait 释放锁，这个时候A线程进入 Wait Set , 等待被唤醒 其实讲了这么多，最主要的一点是还记得刚讲过的MarkWord中重量级锁需要的字段嘛？指向互斥量（monitor、对象监视器）的指针，所以当线程中某个对象获取到锁了，就把该对象的MarkWord重量级锁字段指向monitor对象 3.3 JDK 6 之后 synchronized实现原理由于 synchronized 重量级锁有以下二个问题, 因此JDK 6 之后做了改进，引入了偏向锁和轻量级锁： JDK6 以前，synchronized 那时还属于重量级锁，每次加锁都依赖操作系统Mutex Lock实现，涉及到操作系统让线程从用户态切换到内核态，切换成本很高； 到了JDK6，研究人员引入了偏向锁和轻量级锁，因为Sun 程序员发现大部分程序大多数时间都不会发生多个线程同时访问竞态资源的情况，每次线程都加锁解锁，每次这么搞都要操作系统在用户态和内核态之前来回切，太耗性能了。偏向锁通过…进行加锁，轻量级锁通过…加锁 3.3.1 偏向锁实现原理3.3.2 轻量级锁实现原理3.3.1 偏向锁与轻量级锁1.偏向锁 偏向锁的目的是在某个线程获得锁之后，消除这个线程锁重入（CAS）的开销，看起 来让这个线程得到了偏护。这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。 2.轻量级锁 “轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的，它使用 CAS 操作来避免重量级锁使用互斥量的开销。但是，首先需要强调一点的是， 轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量 级锁使用产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场 景是线程交替执行同步块的情况（整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步），如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀 为重量级锁（ CAS 失败了再改用互斥量进行同步）。 3.总结 轻 量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进 一步提高性能。 3.3.2 加锁过程当没有线程竞争的时候加偏向锁 在没有锁竞争的时候，也就是只有一个线程在获取锁，这时候加偏向锁，偏向锁使得线程在第一次CAS替换MarkWord中的是否标志锁、锁标志位和线程id后，它就获取了锁，之后再次获取不用再进行同步操作。 当有线程与之竞争的时候，它就撤销偏向锁，转为轻量级锁 锁撤销之后（偏向锁状态为0），现在无论是A线程还是B线程执行到同步代码块进行加锁，流程如下： 线程在自己的栈桢中创建锁记录 LockRecord（锁记录）。 线程A 将 Mark Word 拷贝到线程栈的 Lock Record中，这个位置叫 displayced hdr，如下图所示： 将锁记录中的Owner指针指向加锁的对象（存放对象地址）。 将锁对象的对象头的MarkWord替换为指向锁记录的指针。这二步如下图所示： 这时锁标志位变成 00 ，表示轻量级锁 轻量级锁什么时候会升级为重量级锁 当锁升级为轻量级锁之后，如果依然有新线程过来竞争锁，首先新线程会自旋尝试获取锁，尝试到一定次数（默认10次）依然没有拿到，锁就会升级成重量级锁 四、非公平 Synchronized 在线程竞争锁时，首先做的不是直接进ContentionList 队列排队，而是尝试自旋获取锁（可能ContentionList 有别的线程在等锁），如果获取不到才进入 ContentionList，这明显对于已经进入队列的线程是不公平的； 另一个不公平的是自旋获取锁的线程还可能直接抢占 OnDeck 线程的锁资源。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"锁","slug":"Java/锁","permalink":"http://hofe.work/categories/Java/%E9%94%81/"}],"tags":[{"name":"synchronized","slug":"synchronized","permalink":"http://hofe.work/tags/synchronized/"}]},{"title":"一文详解快速排序","slug":"数据结构与算法/一文详解快速排序","date":"2020-05-20T14:11:49.000Z","updated":"2020-06-05T01:12:57.056Z","comments":true,"path":"2020/05/20/数据结构与算法/一文详解快速排序/","link":"","permalink":"http://hofe.work/2020/05/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/","excerpt":"","text":"面试的时候遇上了，但是很可惜忘记了。快排不难，但是如果没做个详细的记录的话，恐怕下次又要忘了，那真的就是让机会从手上溜走了。 一、快排简述平均时间复杂度O(nlogn)，最好时间复杂度O(nlogn)，最差时间复杂度O(n^2)；空间复杂度O(nlogn)，不稳定。 给一串数组，第一遍选取一个参考点，可以随便选择，比如a[0]，接下来遍历数组，要做到左边的都比a[0]小，右边的都比a[0]大。 从a[0]开始向右遍历，a[length-1]开始向左遍历，遇到违反上述规则的就交换两个位置的数，以满足规则。直到两个指针相遇。还要记得将相遇的点与参考点a[0]交换数。 之后重复上述过程，相遇点-1作为左边部分的数组尾，相遇点+1作为右边部分的数组头，参考点就还是每部分数组的第一个元素。 详细过程（参考）下面通过一个例子介绍快速排序算法的思想，假设要对数组a[10]={6，1，2，7，9，3，4，5，10，8}进行排序，首先要在数组中选择一个数作为基准值，这个数可以随意选择，在这里，我们选择数组的第一个元素a[0]=6作为基准值，接下来，我们需要把数组中小于6的数放在左边，大于6的数放在右边，怎么实现呢？ 我们设置两个“哨兵”，记为“哨兵i”和“哨兵j”，他们分别指向数组的第一个元素和最后一个元素，即i=0，j=8。首先哨兵j开始出动，哨兵j一步一步地向左挪动（即j–-），直到找到一个小于6的数停下来。接下来哨兵i再一步一步向右挪动（即i++），直到找到一个数大于6的数停下来。 即将开始查询 最后哨兵j停在了数字5面前，哨兵i停在了数字7面前。此时就需要交换i和j指向的元素的值。 哨兵分别停在5和7上面，并且进行交换 交换之后的数组变为a[10]={6，1，2，5，9，3，4，7，10，8}： 交换完成 第一次交换至此结束。接下来，由于哨兵i和哨兵j还没有相遇，于是哨兵j继续向前，发现比6小的4之后停下；哨兵i继续向前，发现比6大的9之后停下，两者再进行交换。交换之后的数组变为a[10]={6，1，2，5，4，3，9，7，10，8}。 第二次的查询交换 第二次交换至此结束。接下来，哨兵j继续向前，发小比6小的3停下来；哨兵i继续向前，发现i==j了！！！于是，这一轮的探测就要结束了，此时交换a[i]与基准的值，数组a就以6为分界线，分成了小于6和大于6的左右两部分：a[10]={3，1，2，5，4，6，9，7，10，8}。 第一轮查询交换结束 此后分为{3,1,2,5,4} 6 {9,7,10,8}，递归的左边继续上述步骤、右边继续上述步骤 以上查询交换过程摘抄自https://blog.csdn.net/sinat_20177327/article/details/76560079 二、快排实现/** * 快速排序算法 */public void quickSort(int[] list, int left, int right) &#123; if (left &lt; right) &#123; // !!!不是while而是if int point = partition(list, left, right); quickSort(list, left, point - 1); quickSort(list, point + 1, right); &#125;&#125;private int partition(int[] list, int left, int right) &#123; // 用数组的第一个元素作为基准数 int first = list[left]; while (left &lt; right) &#123; while (left &lt; right &amp;&amp; list[right] &gt;= first) &#123; right--; &#125; list[left] = list[right]; // swap(list, left, right); while (left &lt; right &amp;&amp; list[left] &lt;= first) &#123; left++; &#125; list[right] = list[left]; &#125; list[left] = first; // 如果采用swap，则不需要这句 // 返回分割点所在的位置 return left;&#125;/** * 交换数组中两个位置的元素 */private void swap(int[] list, int left, int right) &#123; int temp; if (list != null &amp;&amp; list.length &gt; 0) &#123; temp = list[left]; list[left] = list[right]; list[right] = temp; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://hofe.work/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"快排","slug":"快排","permalink":"http://hofe.work/tags/%E5%BF%AB%E6%8E%92/"}]},{"title":"跳台阶问题","slug":"数据结构与算法/跳台阶问题","date":"2020-05-20T14:11:49.000Z","updated":"2020-05-20T15:46:04.767Z","comments":true,"path":"2020/05/20/数据结构与算法/跳台阶问题/","link":"","permalink":"http://hofe.work/2020/05/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E8%B7%B3%E5%8F%B0%E9%98%B6%E9%97%AE%E9%A2%98/","excerpt":"","text":"跳台阶作为面试的热门题目，虽然不难，但还是有必要好好研究下；网上有很多的解题方案，迭代、递归、动态规划都可以 1.简单跳台阶 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 1.1 最简单写法public class Solution &#123; public int JumpFloor(int target) &#123; if(target &lt; 1)&#123; return 0; &#125; if(target == 1)&#123; return 1; &#125; if(target == 2)&#123; return 2; &#125; int dp[] = new int[target+1]; dp[1] = 1; dp[2] = 2; for(int i = 3; i &lt;= target; i++)&#123; dp[i] = dp[i-1] + dp[i-2]; &#125; return dp[target]; &#125;&#125; 一个数组又好理解又简单是吧，我也这样想的。但在面试的时候被面试官提醒了，假设一种情况，如果不是跳一阶、两阶，而是允许跳1-n阶咋办？总不能dp[i] = dp[i-1] + dp[i-2]+...dp[i-n]这样吧。 1.2 第二种写法public class Solution &#123; public int JumpFloor(int target) &#123; if(target==0||target==1||target==2)&#123; return target; &#125; int f1 = 1; int f2 = 2; int f3 = 0; for(int i = 3; i &lt;= target; i++)&#123; f3 = f1+f2; f1 = f2; f2 = f3; &#125; return f3; &#125;&#125; 1.3 递归方法应该很容易看出来了，有点斐波那契的味道了。那么用递归来解决趴 public class Solution &#123; public int JumpFloor(int target) &#123; if(target==0||target==1||target==2)&#123; return target; &#125; return JumpFloor(target - 1) + JumpFloor(target - 2); &#125;&#125; 2.变态跳台阶 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 f(1) = 1 f(2) = f(2-1) + f(2-2) //f(2-2) 表示2阶一次跳2阶的次数。 f(3) = f(3-1) + f(3-2) + f(3-3) 有n阶台阶，就得有 f(n) = f(n-1) + f(n-2) + f(n-3) + … + f(n-(n-1)) + f(n-n) 种 2.1 递归写法public class Solution &#123; public int JumpFloorII(int target) &#123; if(target==0||target==1||target==2)&#123; return target; &#125; int sum = 1; for(int i = 1; i &lt;= target; i++)&#123; sum += JumpFloorII(target-i); &#125; return sum; &#125;&#125; 2.2 归纳法f(n-1) = f(0) + f(1)+f(2)+f(3) + … + f((n-1)-1) = f(0) + f(1) + f(2) + f(3) + … + f(n-2) f(n) = f(0) + f(1) + f(2) + f(3) + … + f(n-2) + f(n-1) = f(n-1) + f(n-1) 可以得出： ​ f(n) = 2*f(n-1) 链接：https://www.nowcoder.com/questionTerminal/22243d016f6b47f2a6928b4313c85387?f=discussion来源：牛客网public class Solution &#123; public int JumpFloorII(int target) &#123; if (target &lt;= 0) &#123; return -1; &#125; else if (target == 1) &#123; return 1; &#125; else &#123; return 2 * JumpFloorII(target - 1); &#125; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://hofe.work/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"迭代","slug":"迭代","permalink":"http://hofe.work/tags/%E8%BF%AD%E4%BB%A3/"},{"name":"递归","slug":"递归","permalink":"http://hofe.work/tags/%E9%80%92%E5%BD%92/"}]},{"title":"一文详解排序算法","slug":"数据结构与算法/动态规划总结","date":"2020-05-18T04:23:49.000Z","updated":"2020-05-18T03:59:53.759Z","comments":true,"path":"2020/05/18/数据结构与算法/动态规划总结/","link":"","permalink":"http://hofe.work/2020/05/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%80%BB%E7%BB%93/","excerpt":"","text":"转载自https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&amp;mid=2247485814&amp;idx=2&amp;sn=704cee4eaaec8fc98765899bdeec988e&amp;chksm=cea246bdf9d5cfab168380240e9a20927930a905665b0d89e15207122f29ccc5afba0df37615&amp;token=292197051&amp;lang=zh_CN&amp;scene=21#wechat_redirect 一、动态规划的三大步骤动态规划，无非就是利用历史记录，来避免我们的重复计算。而这些历史记录，我们得需要一些变量来保存，一般是用一维数组或者二维数组来保存。下面我们先来讲下做动态规划题很重要的三个步骤， 第一步骤：定义数组元素的含义，上面说了，我们会用一个数组，来保存历史数组，假设用一维数组 dp[] 吧。这个时候有一个非常非常重要的点，就是规定你这个数组元素的含义，例如你的 dp[i] 是代表什么意思？ 第二步骤：找出数组元素之间的关系式，我觉得动态规划，还是有一点类似于我们高中学习时的归纳法的，当我们要计算 dp[n] 时，是可以利用 dp[n-1]，dp[n-2]…..dp[1]，来推出 dp[n] 的，也就是可以利用历史数据来推出新的元素值，所以我们要找出数组元素之间的关系式，例如 dp[n] = dp[n-1] + dp[n-2]，这个就是他们的关系式了。而这一步，也是最难的一步，后面我会讲几种类型的题来说。 学过动态规划的可能都经常听到最优子结构，把大的问题拆分成小的问题，说时候，最开始的时候，我是对最优子结构一梦懵逼的。估计你们也听多了，所以这一次，我将换一种形式来讲，不再是各种子问题，各种最优子结构。所以大佬可别喷我再乱讲，因为我说了，这是我自己平时做题的套路。 第三步骤：找出初始值。学过数学归纳法的都知道，虽然我们知道了数组元素之间的关系式，例如 dp[n] = dp[n-1] + dp[n-2]，我们可以通过 dp[n-1] 和 dp[n-2] 来计算 dp[n]，但是，我们得知道初始值啊，例如一直推下去的话，会由 dp[3] = dp[2] + dp[1]。而 dp[2] 和 dp[1] 是不能再分解的了，所以我们必须要能够直接获得 dp[2] 和 dp[1] 的值，而这，就是所谓的初始值。 由了初始值，并且有了数组元素之间的关系式，那么我们就可以得到 dp[n] 的值了，而 dp[n] 的含义是由你来定义的，你想求什么，就定义它是什么，这样，这道题也就解出来了。 二、案例详解案例一、简单的一维 DP 问题描述：一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 (1)、定义数组元素的含义按我上面的步骤说的，首先我们来定义 dp[i] 的含义，我们的问题是要求青蛙跳上 n 级的台阶总共由多少种跳法，那我们就定义 dp[i] 的含义为：跳上一个 i 级的台阶总共有 dp[i] 种跳法。这样，如果我们能够算出 dp[n]，不就是我们要求的答案吗？所以第一步定义完成。 （2）、找出数组元素间的关系式我们的目的是要求 dp[n]，动态规划的题，如你们经常听说的那样，就是把一个规模比较大的问题分成几个规模比较小的问题，然后由小的问题推导出大的问题。也就是说，dp[n] 的规模为 n，比它规模小的是 n-1, n-2, n-3…. 也就是说，dp[n] 一定会和 dp[n-1], dp[n-2]….存在某种关系的。我们要找出他们的关系。 那么问题来了，怎么找？ 这个怎么找，是最核心最难的一个，我们必须回到问题本身来了，来寻找他们的关系式，dp[n] 究竟会等于什么呢？ 对于这道题，由于情况可以选择跳一级，也可以选择跳两级，所以青蛙到达第 n 级的台阶有两种方式 一种是从第 n-1 级跳上来 一种是从第 n-2 级跳上来 由于我们是要算所有可能的跳法的，所以有 dp[n] = dp[n-1] + dp[n-2]。 （3）、找出初始条件当 n = 1 时，dp[1] = dp[0] + dp[-1]，而我们是数组是不允许下标为负数的，所以对于 dp[1]，我们必须要直接给出它的数值，相当于初始值，显然，dp[1] = 1。一样，dp[0] = 0.（因为 0 个台阶，那肯定是 0 种跳法了）。于是得出初始值： dp[0] = 0.dp[1] = 1.即 n &lt;= 1 时，dp[n] = n. 三个步骤都做出来了，那么我们就来写代码吧，代码会详细注释滴。 int f( int n )&#123; if(n &lt;= 1) return n; // 先创建一个数组来保存历史数据 int[] dp = new int[n+1]; // 给出初始值 dp[0] = 0; dp[1] = 1; // 通过关系式来计算出 dp[n] for(int i = 2; i &lt;= n; i++)&#123; dp[i] = dp[i-1] + dp[i-2]; &#125; // 把最终结果返回 return dp[n];&#125; （4）、再说初始化大家先想以下，你觉得，上面的代码有没有问题？ 答是有问题的，还是错的，错在对初始值的寻找不够严谨，这也是我故意这样弄的，意在告诉你们，关于初始值的严谨性。例如对于上面的题，当 n = 2 时，dp[2] = dp[1] + dp[0] = 1。这显然是错误的，你可以模拟一下，应该是 dp[2] = 2。 也就是说，在寻找初始值的时候，一定要注意不要找漏了，dp[2] 也算是一个初始值，不能通过公式计算得出。有人可能会说，我想不到怎么办？这个很好办，多做几道题就可以了。 下面我再列举三道不同的例题，并且，再在未来的文章中，我也会持续按照这个步骤，给大家找几道有难度且类型不同的题。下面这几道例题，不会讲的特性详细哈。实际上 ，上面的一维数组是可以把空间优化成更小的，不过我们现在先不讲优化的事，下面的题也是，不讲优化版本。 案例二：二维数组的 DP我做了几十道 DP 的算法题，可以说，80% 的题，都是要用二维数组的，所以下面的题主要以二维数组为主，当然有人可能会说，要用一维还是二维，我怎么知道？这个问题不大，接着往下看。 问题描述一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 问总共有多少条不同的路径？ 这是 leetcode 的 62 号题：https://leetcode-cn.com/problems/unique-paths/ 还是老样子，三个步骤来解决。 步骤一、定义数组元素的含义由于我们的目的是从左上角到右下角一共有多少种路径，那我们就定义 dp[i] [j]的含义为：当机器人从左上角走到(i, j) 这个位置时，一共有 dp[i] [j] 种路径。那么，dp[m-1] [n-1] 就是我们要的答案了。 注意，这个网格相当于一个二维数组，数组是从下标为 0 开始算起的，所以 右下角的位置是 (m-1, n - 1)，所以 dp[m-1] [n-1] 就是我们要找的答案。 步骤二：找出关系数组元素间的关系式想象以下，机器人要怎么样才能到达 (i, j) 这个位置？由于机器人可以向下走或者向右走，所以有两种方式到达 一种是从 (i-1, j) 这个位置走一步到达 一种是从(i, j - 1) 这个位置走一步到达 因为是计算所有可能的步骤，所以是把所有可能走的路径都加起来，所以关系式是 dp[i] [j] = dp[i-1] [j] + dp[i] [j-1]。 步骤三、找出初始值显然，当 dp[i] [j] 中，如果 i 或者 j 有一个为 0，那么还能使用关系式吗？答是不能的，因为这个时候把 i - 1 或者 j - 1，就变成负数了，数组就会出问题了，所以我们的初始值是计算出所有的 dp[0] [0….n-1] 和所有的 dp[0….m-1] [0]。这个还是非常容易计算的，相当于计算机图中的最上面一行和左边一列。因此初始值如下： dp[0] [0….n-1] = 1; // 相当于最上面一行，机器人只能一直往左走 dp[0…m-1] [0] = 1; // 相当于最左面一列，机器人只能一直往下走 撸代码三个步骤都写出来了，直接看代码 public static int uniquePaths(int m, int n) &#123; if (m &lt;= 0 || n &lt;= 0) &#123; return 0; &#125; int[][] dp = new int[m][n]; // // 初始化 for(int i = 0; i &lt; m; i++)&#123; dp[i][0] = 1; &#125; for(int i = 0; i &lt; n; i++)&#123; dp[0][i] = 1; &#125; // 推导出 dp[m-1][n-1] for (int i = 1; i &lt; m; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; dp[i][j] = dp[i-1][j] + dp[i][j-1]; &#125; &#125; return dp[m-1][n-1];&#125; O(n*m) 的空间复杂度可以优化成 O(min(n, m)) 的空间复杂度的，不过这里先不讲。 案例三、二维数组 DP写到这里，有点累了，，但还是得写下去，所以看的小伙伴，你们可得继续看呀。下面这道题也不难，比上面的难一丢丢，不过也是非常类似 问题描述给定一个包含非负整数的 m x n 网格，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。 说明：每次只能向下或者向右移动一步。 举例：输入:arr = [ [1,3,1], [1,5,1], [4,2,1]]输出: 7解释: 因为路径 1→3→1→1→1 的总和最小。 和上面的差不多，不过是算最优路径和，这是 leetcode 的第64题：https://leetcode-cn.com/problems/minimum-path-sum/ 还是老样子，可能有些人都看烦了，哈哈，但我还是要按照步骤来写，让那些不大懂的加深理解。有人可能觉得，这些题太简单了吧，别慌，小白先入门，这些属于 medium 级别的，后面在给几道 hard 级别的。 步骤一、定义数组元素的含义由于我们的目的是从左上角到右下角，最小路径和是多少，那我们就定义 dp[i] [j]的含义为：当机器人从左上角走到(i, j) 这个位置时，最下的路径和是 dp[i] [j]。那么，dp[m-1] [n-1] 就是我们要的答案了。 注意，这个网格相当于一个二维数组，数组是从下标为 0 开始算起的，所以 由下角的位置是 (m-1, n - 1)，所以 dp[m-1] [n-1] 就是我们要走的答案。 步骤二：找出关系数组元素间的关系式想象以下，机器人要怎么样才能到达 (i, j) 这个位置？由于机器人可以向下走或者向右走，所以有两种方式到达 一种是从 (i-1, j) 这个位置走一步到达 一种是从(i, j - 1) 这个位置走一步到达 不过这次不是计算所有可能路径，而是计算哪一个路径和是最小的，那么我们要从这两种方式中，选择一种，使得dp[i] [j] 的值是最小的，显然有 dp[i] [j] = min(dp[i-1][j]，dp[i][j-1]) + arr[i][j];// arr[i][j] 表示网格种的值 步骤三、找出初始值显然，当 dp[i] [j] 中，如果 i 或者 j 有一个为 0，那么还能使用关系式吗？答是不能的，因为这个时候把 i - 1 或者 j - 1，就变成负数了，数组就会出问题了，所以我们的初始值是计算出所有的 dp[0] [0….n-1] 和所有的 dp[0….m-1] [0]。这个还是非常容易计算的，相当于计算机图中的最上面一行和左边一列。因此初始值如下： dp[0] [j] = arr[0] [j] + dp[0] [j-1]; // 相当于最上面一行，机器人只能一直往左走 dp[i] [0] = arr[i] [0] + dp[i] [0]; // 相当于最左面一列，机器人只能一直往下走 代码如下public static int uniquePaths(int[][] arr) &#123; int m = arr.length; int n = arr[0].length; if (m &lt;= 0 || n &lt;= 0) &#123; return 0; &#125; int[][] dp = new int[m][n]; // // 初始化 dp[0][0] = arr[0][0]; // 初始化最左边的列 for(int i = 1; i &lt; m; i++)&#123; dp[i][0] = dp[i-1][0] + arr[i][0]; &#125; // 初始化最上边的行 for(int i = 1; i &lt; n; i++)&#123; dp[0][i] = dp[0][i-1] + arr[0][i]; &#125; // 推导出 dp[m-1][n-1] for (int i = 1; i &lt; m; i++) &#123; for (int j = 1; j &lt; n; j++) &#123; dp[i][j] = Math.min(dp[i-1][j], dp[i][j-1]) + arr[i][j]; &#125; &#125; return dp[m-1][n-1];&#125; O(n*m) 的空间复杂度可以优化成 O(min(n, m)) 的空间复杂度的 案例 4：编辑距离这次给的这道题比上面的难一些，在 leetcdoe 的定位是 hard 级别。好像是 leetcode 的第 72 号题。 问题描述 给定两个单词 word1 和 word2，计算出将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符删除一个字符替换一个字符 示例 1:输入: word1 = \"horse\", word2 = \"ros\"输出: 3解释: horse -&gt; rorse (将 'h' 替换为 'r')rorse -&gt; rose (删除 'r')rose -&gt; ros (删除 'e') 解答 还是老样子，按照上面三个步骤来，并且我这里可以告诉你，90% 的字符串问题都可以用动态规划解决，并且90%是采用二维数组。 步骤一、定义数组元素的含义由于我们的目的求将 word1 转换成 word2 所使用的最少操作数 。那我们就定义 dp[i] [j]的含义为：当字符串 word1 的长度为 i，字符串 word2 的长度为 j 时，将 word1 转化为 word2 所使用的最少操作次数为 dp[i] [j]。 有时候，数组的含义并不容易找，所以还是那句话，我给你们一个套路，剩下的还得看你们去领悟。 步骤二：找出关系数组元素间的关系式接下来我们就要找 dp[i] [j] 元素之间的关系了，比起其他题，这道题相对比较难找一点，但是，不管多难找，大部分情况下，dp[i] [j] 和 dp[i-1] [j]、dp[i] [j-1]、dp[i-1] [j-1] 肯定存在某种关系。因为我们的目标就是，从规模小的，通过一些操作，推导出规模大的。对于这道题，我们可以对 word1 进行三种操作 插入一个字符删除一个字符替换一个字符 由于我们是要让操作的次数最小，所以我们要寻找最佳操作。那么有如下关系式： 一、如果我们 word1[i] 与 word2 [j] 相等，这个时候不需要进行任何操作，显然有 dp[i] [j] = dp[i-1] [j-1]。（别忘了 dp[i] [j] 的含义哈）。 二、如果我们 word1[i] 与 word2 [j] 不相等，这个时候我们就必须进行调整，而调整的操作有 3 种，我们要选择一种。三种操作对应的关系试如下（注意字符串与字符的区别）： （1）、如果把字符 word1[i] 替换成与 word2[j] 相等，则有 dp[i] [j] = dp[i-1] [j-1] + 1; （2）、如果在字符串 word1末尾插入一个与 word2[j] 相等的字符，则有 dp[i] [j] = dp[i] [j-1] + 1; （3）、如果把字符 word1[i] 删除，则有 dp[i] [j] = dp[i-1] [j] + 1; 那么我们应该选择一种操作，使得 dp[i] [j] 的值最小，显然有 dp[i] [j] = min(dp[i-1] [j-1]，dp[i] [j-1]，dp[[i-1] [j]]) + 1; 于是，我们的关系式就推出来了， 步骤三、找出初始值显然，当 dp[i] [j] 中，如果 i 或者 j 有一个为 0，那么还能使用关系式吗？答是不能的，因为这个时候把 i - 1 或者 j - 1，就变成负数了，数组就会出问题了，所以我们的初始值是计算出所有的 dp[0] [0….n] 和所有的 dp[0….m] [0]。这个还是非常容易计算的，因为当有一个字符串的长度为 0 时，转化为另外一个字符串，那就只能一直进行插入或者删除操作了。 代码如下public int minDistance(String word1, String word2) &#123; int n1 = word1.length(); int n2 = word2.length(); int[][] dp = new int[n1 + 1][n2 + 1]; // dp[0][0...n2]的初始值 for (int j = 1; j &lt;= n2; j++) dp[0][j] = dp[0][j - 1] + 1; // dp[0...n1][0] 的初始值 for (int i = 1; i &lt;= n1; i++) dp[i][0] = dp[i - 1][0] + 1; // 通过公式推出 dp[n1][n2] for (int i = 1; i &lt;= n1; i++) &#123; for (int j = 1; j &lt;= n2; j++) &#123; // 如果 word1[i] 与 word2[j] 相等。第 i 个字符对应下标是 i-1 if (word1.charAt(i - 1) == word2.charAt(j - 1))&#123; dp[i][j] = dp[i - 1][j - 1]; &#125;else &#123; dp[i][j] = Math.min(Math.min(dp[i - 1][j - 1], dp[i][j - 1]), dp[i - 1][j]) + 1; &#125; &#125; &#125; return dp[n1][n2]; &#125; 最后说下，如果你要练习，可以去 leetcode，选择动态规划专题，然后连续刷几十道，保证你以后再也不怕动态规划了。当然，遇到很难的，咱还是得挂。 Leetcode 动态规划直达：https://leetcode-cn.com/tag/dynamic-programming/","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"动态规划","slug":"动态规划","permalink":"http://hofe.work/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"部署多个git账户且实现ssh代理","slug":"工具/一台电脑部署多个git账户且实现ssh代理","date":"2020-05-17T10:57:00.000Z","updated":"2020-05-17T11:41:15.455Z","comments":true,"path":"2020/05/17/工具/一台电脑部署多个git账户且实现ssh代理/","link":"","permalink":"http://hofe.work/2020/05/17/%E5%B7%A5%E5%85%B7/%E4%B8%80%E5%8F%B0%E7%94%B5%E8%84%91%E9%83%A8%E7%BD%B2%E5%A4%9A%E4%B8%AAgit%E8%B4%A6%E6%88%B7%E4%B8%94%E5%AE%9E%E7%8E%B0ssh%E4%BB%A3%E7%90%86/","excerpt":"","text":"之前注册了github没弄ssh，每次需要push代码或者部署hexo博客都需要输入用户名密码，很麻烦。在体会一次ssh之后，简直太香了。最近又注册了个github账号，发现ssh key会相互覆盖，在网上找了一些资料后在这里做个总结，防止下次有相同需求又得百度了。 一、生成sshKey如果有多个账户的，建议就别设置全局的用户名和邮箱了 git config user.name \"yourgithubname\"git config user.email \"yourgithubemail\"git config --global user.name \"yourgithubname\" # 不要全局git config --global user.email \"yourgithubemail\" # 不要全局 如果设置了全局的话，可以删除~/.gitconfig文件；局部的不设置好像也行。 生成公钥和私钥 ssh-keygen -t rsa -C \"youremail\" 注意这里需要重写默认生成的sshKey名称 同理你可以按照命令生成其它sshKey 二、配置ssh代理在生成ssh的同目录下也存在一个config文件，如果没有就新建。 配置config文件 # 这里用的user1、user2都是举例，可以自定义，和上面图片我命令的没啥关系Host github_user1 # ssh -T git@github_user1 !!待会测试的时候记住是这个 HostName github.com # 就是github.com或者你使用的其他git工具的网站 User git # 这个同一git就行 IdentityFile C:\\Users\\h\\.ssh\\id_rsa_1 # 你刚重命名的sshKey的私钥！(不是以.pub结尾的那个) Host github_user2 # 反正主机名不重复就行 HostName github.com User git IdentityFile C:\\Users\\h\\.ssh\\id_rsa_2 代理配置完成之后，就可以进行测试了 #生成后填到github或其他git上#验证是否成功ssh -T git@github_user1# 结果是下面这样就行了Hi user1! You've successfully authenticated, but GitHub does not provide shell access. 所以ssh -T git就是会从代理文件中通过你给的主机名找到私钥，通过这个私钥再和你给的ssh地址通信验证。 三、ssh地址这里就是主要注意的一点了 我们通常在git上获取的地址都是这样的 git@github.com:用户名/仓库名.git# 报错git@github.com: Permission denied (publickey). 所以你可能一没注意就把ssh地址填这个了，这在默认情况下是可以的，但现在执行ssh git@github.com就会出错了。换言之，仓库给的默认地址不能用的。所以需要进行更改。 比如，现在要往用户为user1的仓库中push了，仓库给的地址还是git@github.com:user1/demo1.git，你需要将域名也就是github.com修改成github_user1，也就是地址为git@github_user1:user1/demo1.git。这样就行啦。","categories":[{"name":"工具","slug":"工具","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"工具/Git","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://hofe.work/tags/Git/"}]},{"title":"Oracle数据库项目实战","slug":"数据库/Oracle项目实战","date":"2020-05-16T08:49:33.000Z","updated":"2020-05-18T02:40:17.367Z","comments":true,"path":"2020/05/16/数据库/Oracle项目实战/","link":"","permalink":"http://hofe.work/2020/05/16/%E6%95%B0%E6%8D%AE%E5%BA%93/Oracle%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/","excerpt":"","text":"个人网站：点击Hofe’s blog可查看更多内容 Oracle数据库项目实战一、需求分析系统总共分为3个模块，分别是用户模块、商品模块、订单模块。 由于每次秒杀活动都会操作数据库，修改秒杀商品的字段，因为每次秒杀时候。对应的秒杀时间段，秒杀实现，秒杀功能，价格等都不同，商品表不易于维护。而且，秒杀商品的个数和普通商品的个数是独立计算的，不同于普通商品的库存字段。因此秒杀系统需要有商品表，秒杀商品表，订单表，秒杀订单，秒杀用户表。 用户模块：需要用到用户表。前端获取用户的id以及登录密码传递给后端进行验证，后端从秒杀用户表中查找是否存在该用户，以及验证账号密码是否正确。验证通过则登录系统。 商品模块：需要用到秒杀商品表、商品表。后端从秒杀商品表中查找参与秒杀活动的商品，并将其返回给前端显示在页面上；用户在点击秒杀商品后，前端会发起请求携带商品id给后端，后端从商品表中检索该条数据，若存在数据则会跳转至商品详情页面。 订单模块：需要用到秒杀订单表、订单表。用户在抢到秒杀商品后，首先会在订单表中生成一条只含有订单id号的记录，该订单id会与商品id、用户id一起存入秒杀订单表，成功存入后，再将详细记录存入订单表，该过程需要开启事务，如中间有步骤失败将回退。 二、系统E-R图 三、关系模型 3.1 goods表该表存放商品数据，含商品名、价格等，用于页面显示商品详情 3.2 miaosha_users表该表存放用户信息，含用户名、密码等，用于登陆 3.3 miaosha_goods表该表存放参与秒杀的商品信息，含秒杀价格、库存等信息 3.4 miaosha_order表该表存放秒杀订单的信息，含下单用户ID、商品ID等信息 3.5 order_info表该表存放订单的详细信息。 四、创建表并添加数据先使用命令行创建一个用户并授权 在Navicat工具中创建表并填入数据 插入的数据为了方便理解，统一规定为userId从15960000001开始递增，goodId从11开始递增，orderId从101开始递增。 4.1 goods表执行创建表语句，并添加注释 插入数据 查看插入结果 添加主键约束和check约束 4.2 miaosha_users表创建表 添加数据 查看数据 添加主键约束和check约束 4.3 miaosha_goods表创建表 添加数据 查看数据 添加主键约束、唯一约束、check约束、外键约束 注：添加外键约束时，外键只能参考主键或者唯一键，故如果参考的不是主键，需要先在参考列上加上唯一约束。由于goods_id不是该表主键，但是它需要作为order_info表的外键参考，故需要加上唯一约束。 4.4 order_info表创建表 添加数据 添加主键和check约束 注：这里的user_id和goods_id并没有依赖外键，但是当用户下单时会先在该表生成只含id的记录，id将作为miaosha_order的order_id的参考外键。 4.5 miaosha_order表创建表 添加数据 查看数据 添加主键、check和外键约束 五、索引/视图的创建与应用5.1 添加索引在order_info.goods_id添加索引，可以在数据量大的情况下加快连接查询的速度 5.2 添加视图考虑到当查看秒杀商品的时候，会点击进去查看商品的详情，故添加goods和miaosha_goods的查询视图。 六、游标的创建与应用6.1 使用游标检索miaosha_user表中所有数据 七、存储过程的创建与应用7.1 使用存储过程将指定用户登录次数+1这里执行完存储过程后，生成函数P1 测试：原login_count=2 执行完存储过程p1后，login_count=3 八、触发器的创建与应用8.1 使用触发器实现下订单后库存减1执行触发器之前，gooid_id为16的stock_count为5 触发器成功执行 再次查询，gooid_id为16的stock_count为4 九、函数的创建与应用9.1 计算iPhoneX全部销售完的总金额iPhoneX总库存100台，每台售价5000 函数执行完后会将值返回给s，最后输出s 十、软件各模块功能实现10.1 用户模块用户登录功能 输错密码或者数据库中不存在该用户，都会提示 数据库中存在该数据则跳转至商品列表页 10.2 商品模块秒杀商品展示功能 用户成功登录后即跳转至该页面 商品详情页 为了方便计算秒杀倒计时，这里直接换算了时间单位，所以开始时间显示数字，表示1970 年 1 月 1 日开始计算到 Date 对象中的时间之间的毫秒数 未到达秒杀时间不可抢购 达到秒杀时间，秒杀后加入秒杀订单 10.3 订单模块在秒杀进行的时候对商品进行下单 确定秒杀成功后，将记录加入订单详情表 同时列表页显示库存减1","categories":[{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Oracle","slug":"数据库/Oracle","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://hofe.work/tags/Oracle/"}]},{"title":"Python分类预测泰坦尼克号生存率","slug":"Python/Python分类预测泰坦尼克号生存率","date":"2020-05-15T06:37:45.000Z","updated":"2020-06-06T15:52:07.546Z","comments":true,"path":"2020/05/15/Python/Python分类预测泰坦尼克号生存率/","link":"","permalink":"http://hofe.work/2020/05/15/Python/Python%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E7%94%9F%E5%AD%98%E7%8E%87/","excerpt":"","text":"可访问个人网站进行阅读最新版本 二、泰坦尼克号乘客生存预测（分类分析）1.1 环境 python(3.6) matplotlib (3.1.3) numpy (1.18.1) pandas (1.0.0) scikit-learn (0.22.2.post1) 数据集点击下载 1.2 功能 进行必要数据预处理和特征工程 使用两种以上分类算法建立分类模型（不限编程语言） 随机将数据划分为训练集和测试集，进行模型训练和测试 应用交叉验证法训练和测试分类模型 分别对第(3)和(4)小题，用F1-Measure评价分类模型的性能 二、结果（1）使用ID3决策树进行分类 （2）使用K近邻进行分类 三、源码# -*- coding: utf-8 -*-import pandas as pdimport numpy as npfrom sklearn.metrics import f1_scorefrom sklearn.model_selection import train_test_split, cross_val_scorefrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.tree import DecisionTreeClassifierdef KNebClf(x, y): # 使用ID3决策树进行分类 clf = KNeighborsClassifier(n_neighbors=3) clf.fit(x, y) pred = clf.predict(x) return clf, preddef DTreeClf(x, y): # 使用K近邻进行分类 clf = DecisionTreeClassifier(criterion=\"entropy\") clf.fit(x, y) pred = clf.predict(x) return clf, predif __name__ == '__main__': data = pd.read_csv(\"C:\\\\Users\\\\h\\\\Grade3\\\\Grade3-2\\\\dataset\\\\Titanic_train.csv\") size = data.shape index = data[\"PassengerId\"] # PassengerID、Name、Ticket、Cabin 对预测结果影响不大，可以删除 data.drop([\"PassengerId\", \"Cabin\",\"Name\",\"Ticket\"],inplace=True,axis=1) # print(data.isnull().sum()) # 处理空值 data['Age'].fillna(data['Age'].mean(), inplace=True) # 年龄 均值填充 data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True) # 登船港口 众数填充 # 处理非数值型数据，可以采用onehot也可以替换，这里采用替换的方式 data['Sex'] = data['Sex'].map(&#123;'male': 0, 'female': 1&#125;).astype(int) data['Embarked'] = data['Embarked'].map(&#123;'S': 0, 'C': 1, 'Q': 2&#125;).astype(int) # index = data[:, 0] # 第一列是PassengerId，不应加入预测 y = data[\"Survived\"] # Survived待预测 x = data.drop(['Survived'], axis=1) # 第8列为Survived， 即需要预测的列，故不加入到模型中 X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2) clf, y_pred = DTreeClf(x, y) #使用ID3决策树进行分类 # clf, y_pred = KNebClf(x, y) # 使用K近邻进行分类 scores = cross_val_score(clf, x, y, cv=5) print(\"交叉验证准确率： \", np.mean(scores), scores) clf, y_pred_train = DTreeClf(X_train, Y_train) # clf, y_pred_train = KNebClf(X_train, Y_train) print(\"训练集的F1-Measure： \", f1_score(Y_train, y_pred_train, average='weighted')) # print(\"训练集上的准确率： \", np.mean(Y_train == y_pred_train)) clf, y_pred_test = DTreeClf(X_test, Y_test) # clf, y_pred_test = KNebClf(X_test, Y_test) print(\"测试集的F1-Measure： \", f1_score(Y_test, y_pred_test, average='weighted')) # print(\"测试集上的准确率： \", np.mean(Y_test == y_pred_test))","categories":[{"name":"Python","slug":"Python","permalink":"http://hofe.work/categories/Python/"},{"name":"数据分析","slug":"Python/数据分析","permalink":"http://hofe.work/categories/Python/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"DTree","slug":"DTree","permalink":"http://hofe.work/tags/DTree/"},{"name":"KNN","slug":"KNN","permalink":"http://hofe.work/tags/KNN/"}]},{"title":"Python回归分析预测房屋价格","slug":"Python/Python回归分析预测房屋价格","date":"2020-05-14T06:37:45.000Z","updated":"2020-06-06T15:52:27.750Z","comments":true,"path":"2020/05/14/Python/Python回归分析预测房屋价格/","link":"","permalink":"http://hofe.work/2020/05/14/Python/Python%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E9%A2%84%E6%B5%8B%E6%88%BF%E5%B1%8B%E4%BB%B7%E6%A0%BC/","excerpt":"","text":"可访问个人网站进行阅读最新版本 一、房价预测（回归分析）1.1 环境 python(3.6) matplotlib (3.1.3) numpy (1.18.1) pandas (1.0.0) scikit-learn (0.22.2.post1) 数据集点击下载 1.2 功能 进行必要的数据预处理和特征工程 应用两种以上回归分析算法建立预测模型（不限编程语言） 随机将数据划分为训练集和测试集，进行模型训练和测试 应用交叉验证法训练和测试预测模型 评价第(3)和(4)所建立预测模型的性能（用MSE（均方误差）和R2 Score（R2决定系数）等指标评价） 二、结果逻辑回归的准确率一直不高；多项式回归准确率还可以，这里用的只是1阶，2阶就开始过拟合了，可以用交叉验证看误差在多少。数据集共有2000条，图片只显示前100条。 三、源码# -*- coding: utf-8 -*-import warningswarnings.filterwarnings(\"ignore\")import datetimeimport matplotlib.pyplot as pltimport pandas as pdimport numpy as npfrom numpy import nan as NAfrom sklearn.impute import SimpleImputerfrom sklearn.linear_model import LogisticRegression, LinearRegressionfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_split, cross_val_scorefrom sklearn.preprocessing import OneHotEncoder, PolynomialFeaturesdef days(str2): # 考虑到时间列变成onehot编码不太合理，故将其转为房屋建造时间距离现在的时间，即房屋建了多少天 date1 = datetime.datetime.strptime(\"2020/5/15\", \"%Y/%m/%d\") date2 = datetime.datetime.strptime(str2[0:10], \"%Y/%m/%d\") num = (date1 - date2).days return int(num)def crossValid(lr, x, y): X_train, X_test, Y_train, Y_test = train_test_split(x, y) scores = cross_val_score(lr, X_train, Y_train, cv=3) print(\"交叉验证准确率： \", np.mean(scores), scores)def PredByLogisticRegression(x, y): # 逻辑回归预测 lr = LogisticRegression(solver='liblinear') lr.fit(x,y) print(\"模型的平均准确度为： %s \" % lr.score(x,y)) y_pred = lr.predict(x) crossValid(lr, x, y) return y_preddef PredByPolynomialRegression(x, y): # 多项式回归预测 poly = PolynomialFeatures(degree=1) X_poly = poly.fit_transform(x) poly.fit(X_poly, y) lin = LinearRegression() lin.fit(X_poly, y) print(\"R2 Score： %s \" % lin.score(X_poly, y)) y_pred = lin.predict(poly.fit_transform(x)) crossValid(lin, X_poly, y) return y_predif __name__ == '__main__': data = pd.read_csv(\"C:\\\\Users\\\\h\\\\Grade3\\\\Grade3-2\\\\dataset\\\\house_train.csv\") size = data.shape # 时间列无法做onehot编码，会有很多个值，故转换成与某个时间之间的差值 for i in range(size[0]): data[\"built_date\"][i] = days(data[\"built_date\"][i]) # 将id列抽取出来，价格与id没关系 X_id = pd.DataFrame(data['id']) data = data.drop(['id'], axis=1) # 获取非数值型数据 X_sym = pd.DataFrame(data['floor']) imp_mean = SimpleImputer(missing_values=NA, strategy='most_frequent', copy=True) # 对非数值型数据用众数填充 X_symfilled = imp_mean.fit_transform(X_sym) # 对非数值型数据进行one-hot编码,X_onehot是ndarray对象 X_onehot = OneHotEncoder(sparse=False).fit_transform(X_symfilled) # 获取数值型数据 X_num = data.drop(['floor'], axis=1) imp_mean = SimpleImputer(missing_values=NA, strategy='mean') # 对数值型数据用均值填充 X_numfilled = imp_mean.fit_transform(X_num) # 缺失处理 # 组合成新的数据 data2 = np.hstack((X_id, X_onehot, X_numfilled)) index = data2[:, 0].astype(int) # 第一列是id，不应加入预测 x = data2[:, 1:14] y = data2[:, 14].astype(int) y_pred = PredByLogisticRegression(x, y) # 调用逻辑回归 # y_pred = PredByPolynomialRegression(x, y) # 调用多项式线性回归 print(\"MSE均方误差： %.2f\" % mean_squared_error(y, y_pred)) # 均方差 # 2000行数据太多，只显示前100个数据的拟合情况 plt.scatter(index[0:100], y[0:100], color='blue') plt.plot(index[0:100], y_pred[0:100], color='red') plt.title('Logistic Regression') plt.show()","categories":[{"name":"Python","slug":"Python","permalink":"http://hofe.work/categories/Python/"},{"name":"数据分析","slug":"Python/数据分析","permalink":"http://hofe.work/categories/Python/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"tags":[{"name":"回归","slug":"回归","permalink":"http://hofe.work/tags/%E5%9B%9E%E5%BD%92/"}]},{"title":"秒杀架构的设计","slug":"项目/秒杀项目记录","date":"2020-05-12T13:53:45.000Z","updated":"2020-06-06T15:54:58.186Z","comments":true,"path":"2020/05/12/项目/秒杀项目记录/","link":"","permalink":"http://hofe.work/2020/05/12/%E9%A1%B9%E7%9B%AE/%E7%A7%92%E6%9D%80%E9%A1%B9%E7%9B%AE%E8%AE%B0%E5%BD%95/","excerpt":"","text":"项目描述：本系统是使用SpringBoot开发的高并发限时抢购秒杀系统，除了实现基本的登录、查看商品列表、秒杀、下单等功能，项目中还针对高并发情况实现了系统缓存和限流。 项目架构：SpringBoot+Mybatis+Redis+RabbitMQ 技术要点： 基于Cookie实现Session共享，实现单点登录及双重md5密码校验 使用Redis做缓存提高访问速度和并发量，库存预减防止超卖，利用内存标记减少对Redis的访问 使用页面静态化，加快用户访问速度，缓存页面至浏览器降低服务器压力 使用消息队列完成异步下单，提升用户体验，实现高并发场景下的削峰和降流 安全性优化：秒杀接口地址的隐藏，接口限流防刷，数学公式验证码 1.基于Cookie实现Session共享，实现单点登录及双重md5密码校验单点登录将登陆功能单独抽取出来，每个子系统登陆都把请求发送到该登陆系统。登陆系统在用户第一次登陆之后，随机生成uuid作为key，user信息作为value存入redis，设置过期时间，这样就保留了这次的登陆信息，同时把这个uuid写入客户端的Cookie。之后其它子系统在发起请求时携带着cookie，会先向登陆系统发起请求，如果登陆系统中的redis存在这条uuid对应的用户信息，说明目前已经登录了，则放行直接登陆，否则重定向到登陆页面。 双重md5密码校验前端输入用户名密码之后进行MD5加密（明文密码+固定盐值），再将请求发送给后端，为了防止数据库被入侵，被人通过彩虹表反查出密码，后端再使用随机salt与加过一次密后的密码再做一次加密，最终把加密后的值与随机salt存在数据库里。 2.使用Redis做缓存提高访问速度和并发量，库存预减防止超卖，利用内存标记减少对Redis的访问库存预减Controller实现InitializingBean接口，重写afterPropertiesSet方法。容器启动的时候，检测到了实现了接口InitializingBean之后，就会去回调afterPropertiesSet方法,将每种商品的库存数量加载到redis里面去。后端收到请求之后，就先在redis中减库存，当redis中库存为0的时候，就直接返回失败。成功之后，判断秒杀订单是否形成了，如果已经形成了，说明之前已经秒杀过了，返回失败，同时库存也要更新回去，这样就可以防止重复秒杀。之后就可以将请求封装入队列。 会有个问题，就是一个用户一直点击，redis库存就没了，所以可以把判断是否下单的逻辑放在预减库存之前（或者将已经秒杀的用户存在缓存中） 内存标记设一个标志位，如果库存已经减为0了，就不再接受请求，减少不必要的redis访问 3.使用消息队列完成异步下单，提升用户体验，实现高并发场景下的削峰和降流RabbitMQ监听秒杀MIAOSHA_QUEUE的这名字的通道，如果有消息过来，获取到传入的信息，执行真正的秒杀之前，要判断数据库的库存，判断是否重复秒杀，然后执行秒杀事务（秒杀事务是一个原子操作：库存减1，下订单，写入秒杀订单） 系统初始化，把商品库存数量stock加载到Redis上面来。 后端收到秒杀请求，Redis预减库存，如果库存已经到达临界值的时候，就不需要继续请求下去，直接返回失败，即后面的大量请求无需给系统带来压力。 判断这个秒杀订单形成没有，判断是否已经秒杀到了，避免一个账户秒杀多个商品，判断是否重复秒杀。（将秒杀过的用户缓存在redis中） 库存充足，且无重复秒杀，将秒杀请求封装后消息入队，同时给前端返回一个code (0)，即代表返回排队中。（返回的并不是失败或者成功，此时还不能判断） 前端接收到数据后，显示排队中，并根据商品id轮询请求服务器（考虑200ms轮询一次）。 后端RabbitMQ监听秒杀MIAOSHA_QUEUE的这名字的通道，如果有消息过来，获取到传入的信息，执行真正的秒杀之前，要判断数据库的库存，判断是否重复秒杀，然后执行秒杀事务（秒杀事务是一个原子操作：库存减1，下订单，写入秒杀订单）。 此时，前端根据商品id轮询请求接口MiaoshaResult,查看是否生成了商品订单，如果请求返回-1代表秒杀失败，返回0代表排队中，返回&gt;0代表商品id说明秒杀成功。返回结果说明：前端会根据后端返回的值来判断是秒杀结果。 -1 ：库存不足秒杀失败 0 ：排队中，继续轮询 取消订单利用延迟消息。下单未付款设定一个过期时间如15分钟，超过这个时间未付款则自动取消。 4.使用页面静态化，加快用户访问速度，缓存页面至浏览器降低服务器压力页面缓存商品列表页：访问页面的时候先从redis中查找是否已经缓存了，若缓存直接获取html页面，否则从数据库中取数据，并使用模板引擎手动渲染（thymeleafViewResolver.getTemplateEngine().process()）成html，将html存入redis。 URL缓存用在详情页，其实和页面缓存本质是一样的，只是url可以根据商品id更换页面，进而继续缓存。 对象缓存前端发来了获取某个用户信息的请求，从数据库获取后，就将这个对象缓存至redis 页面静态化做了页面缓存：第一次访问页面，生成缓存，之后再访问直接取redis中缓存的html，那么这种方式每次都还是需要请求服务器，所以还可以继续优化，就是用页面静态化的方式。 页面静态化之后，第一次是去请求后台要渲染好的html页面，之后的请求都是直接访问用户本地浏览器的缓存的html页面 ，静态资源，然后前端通过Ajax异步来访问后端，只去获取页面需要显示的数据返回即可。 5.安全性优化：秒杀接口地址的隐藏，接口限流防刷，数学公式验证码接口地址的隐藏只有用户在秒杀时间到了之后点击按钮发起请求才能获取到秒杀路径，后端随机生成路径path(url : “/miaosha/” + path + “/do_miaosha”)并设置过期时间写入redis后返回给前端，前端再向该路径发起请求，后端验证是否与redis中的路径一致，一致则可以进行抢购 图形验证码后端随机生成验证码，并计算结果以及用户id、商品id存入redis，前端在请求秒杀地址之前需要先写入验证码计算结果，与redis中记录的进行对比，通过才动态生成地址给前端 需要注意的是请求可能会去从缓存中拿，所以url加个timestamp参数 接口限流防刷利用缓存实现，用户每次点击之后访问接口的时候，在缓存中生成一个计数器（需要用户id以及url作为key），第一次将这个计数器置1后存入缓存，并给其设定有效期，比如一分钟，一分钟之内再访问，那么数值加一。一分钟之内访问次数超过限定数值，直接返回失败。下一个一分钟，数据重新从0开始计算。因为缓存具有一个有效期，一分钟之后自动失效。","categories":[{"name":"项目","slug":"项目","permalink":"http://hofe.work/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"秒杀","slug":"秒杀","permalink":"http://hofe.work/tags/%E7%A7%92%E6%9D%80/"}]},{"title":"动态规划题目合集","slug":"数据结构与算法/动态规划题目合集","date":"2020-05-08T14:25:11.000Z","updated":"2020-05-29T13:24:31.929Z","comments":true,"path":"2020/05/08/数据结构与算法/动态规划题目合集/","link":"","permalink":"http://hofe.work/2020/05/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86/","excerpt":"","text":"1、买卖股票最大收益 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 如果你最多只允许完成一笔交易（即买入和卖出一支股票一次），设计一个算法来计算你所能获取的最大利润。 注意：你不能在买入股票前卖出股票。 示例： 输入: [7,1,5,3,6,4]输出: 5解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock class Solution &#123; //只要记录前面的最小价格，将这个最小价格作为买入价格，然后将当前的价格作为售出价格，查看当前收益是不是最大收益。 public int maxProfit(int[] prices) &#123; if(prices.length == 0)&#123; return 0; &#125; int min = prices[0]; int max = 0; for(int i = 1; i &lt; prices.length; i++)&#123; if(prices[i] &lt; min)&#123; min = prices[i]; &#125;else&#123; max = Math.max(max, prices[i] - min); &#125; &#125; return max; &#125;&#125; 2、买卖股票最大收益（可多次买卖） Leetcode / 力扣 题目描述：可以进行多次交易，多次交易之间不能交叉进行，可以进行多次交易。 贪心：对于 [a, b, c, d]，如果有 a &lt;= b &lt;= c &lt;= d ，那么最大收益为 d - a。而 d - a = (d - c) + (c - b) + (b - a) ，因此当访问到一个 prices[i] 且 prices[i] - prices[i-1] &gt; 0，那么就把 prices[i] - prices[i-1] 添加到收益中。 class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length == 0) return 0; int profit = 0; for(int i = 1; i &lt; prices.length; i++)&#123; profit += Math.max(prices[i] - prices[i-1],0); &#125; return profit; &#125;&#125; 动态规划：第i天只有两种状态，不持有或持有股票，当天不持有股票的状态可能来自昨天卖出或者昨天也不持有，同理，当天持有股票的状态可能来自昨天买入或者昨天也持有中，取最后一天的不持有股票状态就是问题的解 class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length == 0) return 0; int dp[][] = new int[prices.length][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for(int i = 1; i &lt; prices.length; i++)&#123; dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1]+prices[i]); dp[i][1] = Math.max(dp[i-1][1], dp[i-1][0]-prices[i]); &#125; return dp[prices.length - 1][0]; &#125;&#125; 3、最大连续子数组求和 Leetcode / 力扣 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 class Solution &#123; public int maxSubArray(int[] nums) &#123; int presum[] = new int[nums.length]; presum[0] = nums[0]; int max = presum[0]; for(int i = 1; i &lt; nums.length; i++)&#123; presum[i] = presum[i-1] &gt; 0 ? presum[i-1]+nums[i] : nums[i]; max = Math.max(presum[i], max); &#125; return max; &#125;&#125;// 可以用presum代替presum进行优化，这样好理解写 4、矩阵的最小路径和 Leetcode / 力扣 [[1,3,1], [1,5,1], [4,2,1]]Given the above grid map, return 7. Because the path 1→3→1→1→1 minimizes the sum. 求从矩阵的左上角到右下角的最小路径和，每次只能向右和向下移动 class Solution &#123; public int minPathSum(int[][] grid) &#123; if(grid.length == 0 || grid[0].length == 0)&#123; return 0; &#125; int row = grid.length; int col = grid[0].length; int dp[][] = new int[row][col]; dp[0][0] = grid[0][0]; for(int i = 1; i &lt; row; i++)&#123; dp[i][0] = dp[i-1][0] + grid[i][0]; &#125; for(int j = 1; j &lt; col; j++)&#123; dp[0][j] = dp[0][j-1] + grid[0][j]; &#125; for(int i = 1; i &lt; row; i++)&#123; for(int j = 1; j &lt; col; j++)&#123; dp[i][j] = Math.min(dp[i-1][j], dp[i][j-1])+grid[i][j]; &#125; &#125; return dp[row-1][col-1]; &#125;&#125; 优化 public int minPathSum(int[][] grid) &#123; if (grid.length == 0 || grid[0].length == 0) &#123; return 0; &#125; int m = grid.length, n = grid[0].length; int[] dp = new int[n]; for (int i = 0; i &lt; m; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (j == 0) &#123; dp[j] = dp[j]; // 只能从上侧走到该位置 &#125; else if (i == 0) &#123; dp[j] = dp[j - 1]; // 只能从左侧走到该位置 &#125; else &#123; dp[j] = Math.min(dp[j - 1], dp[j]); &#125; dp[j] += grid[i][j]; &#125; &#125; return dp[n - 1];&#125; 5、数组中等差递增子区间的个数 Leetcode / 力扣 如果一个数列至少有三个元素，并且任意两个相邻元素之差相同，则称该数列为等差数列。 示例: A = [1, 2, 3, 4] 返回: 3, A 中有三个子等差数组: [1, 2, 3], [2, 3, 4] 以及自身 [1, 2, 3, 4]。 dp[i] 表示以 A[i] 为结尾的等差递增子区间的个数。 当 A[i] - A[i-1] == A[i-1] - A[i-2]，那么 [A[i-2], A[i-1], A[i]] 构成一个等差递增子区间。而且在以 A[i-1] 为结尾的递增子区间的后面再加上一个 A[i]，一样可以构成新的递增子区间。 class Solution &#123; public int numberOfArithmeticSlices(int[] A) &#123; if(A.length == 0 || A == null)&#123; return 0; &#125; int dp[] = new int[A.length]; for(int i = 2; i &lt; A.length; i++)&#123; if(A[i] - A[i-1] == A[i-1] - A[i-2])&#123; dp[i] = dp[i-1] + 1; &#125; &#125; int res = 0; for(int i = 1; i &lt; A.length; i++)&#123; res += dp[i]; &#125; return res; &#125;&#125; 6、分割整数计算最大乘积 Leetcode / 力扣 class Solution &#123; public int integerBreak(int n) &#123; if(n&lt;=1)return 0; if(n&lt;=3)&#123; return n-1; &#125; int res=1; while(n&gt;4)&#123; n-=3; res*=3; &#125; return res*n; &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"http://hofe.work/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}]},{"title":"递归题目合集","slug":"数据结构与算法/递归总结","date":"2020-05-08T14:25:11.000Z","updated":"2020-05-29T13:24:59.752Z","comments":true,"path":"2020/05/08/数据结构与算法/递归总结/","link":"","permalink":"http://hofe.work/2020/05/08/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E9%80%92%E5%BD%92%E6%80%BB%E7%BB%93/","excerpt":"","text":"矩形覆盖我们可以用2*1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2*1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 比如n=3时，2*3的矩形块有3种覆盖方法： public class Solution &#123; public int RectCover(int target) &#123; // 2*n的矩阵，宽是固定的 // 如果是竖着摆的，右边可以横着也可以竖着 // 如果是横着摆的，下方那块也必须是横着摆的 if(target &lt; 1)&#123; // 递归会到RectCover(0)的情况，所以题目即使不会给0，也要考虑 return 0; &#125; if(target == 1 || target == 2)&#123; return target; &#125; return RectCover(target - 1) + RectCover(target - 2); &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"递归","slug":"递归","permalink":"http://hofe.work/tags/%E9%80%92%E5%BD%92/"}]},{"title":"TCP与UDP详解","slug":"网络/TCP与UDP","date":"2020-05-08T05:57:00.000Z","updated":"2020-05-29T04:21:08.267Z","comments":true,"path":"2020/05/08/网络/TCP与UDP/","link":"","permalink":"http://hofe.work/2020/05/08/%E7%BD%91%E7%BB%9C/TCP%E4%B8%8EUDP/","excerpt":"","text":"欢迎来我的个人网站，里面有最新的版本 这篇介绍了下TCP与UDP面试题常考内容，并做了延伸，解释了原理。 面试题tcp和udp的区别 tcp如何实现可靠性 怎么实现拥塞控制 cookie、session 一、TCP/IP协议 TCP/IP（Transmission Control Protocol/Internet Protocol，传输控制协议/网际协议）是指能够在多个不同网络间实现信息传输的协议簇。TCP/IP协议不仅仅指的是TCP 和IP两个协议，而是指一个由FTP、SMTP、TCP、UDP、IP等协议构成的协议簇。TCP/IP 定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准。我的理解： 互联网中的设备要相互通信，必须基于相同的方式，比如由哪一方发起通讯，使用什么语言进行通讯，怎么结束通讯这些都要事先确定，不同设备之间的通讯都需要一种规则，我们将这种规则成为协议。 1.1 TCP/IP 的分层管理图 1.1.1 应用层TCP/IP模型将OSI参考模型中的会话层和表示层的功能合并到应用层实现。这一层主要的代表有DNS域名解析/http协议 1.1.2 传输层在TCP/IP模型中，传输层的功能是使源端主机和目标端主机上的对等实体可以进行会话。在传输层定义了两种服务质量不同的协议。即：传输控制协议TCP和用户数据报协议UDP. 1.1.3 网络层网络层是整个TCP/IP协议栈的核心。它的功能是把分组发往目标网络或主机。同时，为了尽快地发送分组，可能需要沿不同的路径同时进行分组传递。因此，分组到达的顺序和发送的顺序可能不同，这就需要上层必须对分组进行排序。网络层定义了分组格式和协议，即IP协议（Internet Protocol ）。 1.1.4 数据链路层控制网络层与物理层之间的通信，主要功能是保证物理线路上进行可靠的数据传递。为了保证传输，从网络层接收到的数据被分割成特定的可被物理层传输的帧。帧是用来移动数据结构的结构包，他不仅包含原始数据，还包含发送方和接收方的物理地址以及纠错和控制信息。其中的地址确定了帧将发送到何处，而纠错和控制信息则确保帧无差错到达。如果在传达数据时，接收点检测到所传数据中有差错，就要通知发送方重发这一帧。 1.1.5 物理层该层负责 比特流在节点之间的传输，即负责物理传输，这一层的协议既与链路有关，也与传输的介质有关。通俗来说就是把计算机连接起来的物理手段。 二、UDP2.1 首部 UDP用户数据报有两个字段：首部字段和数据字段，数据字段很简单，只有8个字节；首部由四个字段组成，每个字段的长度都是两个字节。各字段意义如下： 源端口： 源端口号，在需要给对方回信时使用。不需要是可全用0. 目的端口号： 这在终点交付报文时必须使用。 长度： 用户数据报UDP的长度，最小为8（仅首部）。 校验和： 用于校验用户数据报在传输过程是否出错，出错则丢弃该报文 三、TCP3.1 首部 源端口和目的端口: 各占两个字节，分别写入源端口号和目的端口号。序号 ： 占4个字节；用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。确认号 ： 占4个字节；期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。数据偏移 ： 占4位；指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。确认 ACK ： 当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。同步 SYN ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。终止 FIN ： 用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。窗口 ： 占2字节；窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。检验和： 占2个字节；检验和字段检验的范围包括首部和数据这两个部分。在计算检验和时，在TCP报文段的前面加上12字节的伪首部。 紧急指针字段：占 16 位，指出在本报文段中紧急数据共有多少个字节(紧急数据放在本报文段数据的最前面)； 套接字： TCP连接的端点叫做套接字或插口。端口号拼接到IP地址即构成了套接字。 3.2 三次握手 第一次握手：Client将SYN置1，随机产生一个初始序列号seq发送给Server，进入SYN_SENT状态； 第二次握手：Server收到Client的SYN=1之后，知道客户端请求建立连接，将自己的SYN置1，ACK置1，产生一个acknowledge number=sequence number+1，并随机产生一个自己的初始序列号，发送给客户端；进入SYN_RCVD状态； 第三次握手：客户端检查acknowledge number是否为序列号+1，ACK是否为1，检查正确之后将自己的ACK置为1，产生一个acknowledge number=服务器发的序列号+1，发送给服务器；进入ESTABLISHED状态；服务器检查ACK为1和acknowledge number为序列号+1之后，也进入ESTABLISHED状态；完成三次握手，连接建立。 3.2.1 TCP建立连接可以两次握手吗？为什么?不可以。有两个原因： 可能会出现已失效的连接请求报文段又传到了服务器端。 client 发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达 server。本来这是一个早已失效的报文段。但 server 收到此失效的连接请求报文段后，就误认为是 client 再次发出的一个新的连接请求。于是就向 client 发出确认报文段，同意建立连接。假设不采用 “三次握手”，那么只要 server 发出确认，新的连接就建立了。由于现在 client 并没有发出建立连接的请求，因此不会理睬 server 的确认，也不会向 server 发送数据。但 server 却以为新的运输连接已经建立，并一直等待 client 发来数据。这样，server 的很多资源就白白浪费掉了。采用 “三次握手” 的办法可以防止上述现象发生。例如刚才那种情况，client 不会向 server 的确认发出确认。server 由于收不到确认，就知道 client 并没有要求建立连接。 其次，两次握手无法保证Client正确接收第二次握手的报文（Server无法确认Client是否收到），也无法保证Client和Server之间成功互换初始序列号。 3.2.2 可以采用四次握手吗？为什么？可以。但是会降低传输的效率。 四次握手是指：第二次握手：Server只发送ACK和acknowledge number；而Server的SYN和初始序列号在第三次握手时发送；原来协议中的第三次握手变为第四次握手。出于优化目的，四次握手中的二、三可以合并。 3.2.3 第三次握手中，如果客户端的ACK未送达服务器，会怎样？Server端：由于Server没有收到ACK确认，因此会重发之前的SYN+ACK（默认重发五次，之后自动关闭连接进入CLOSED状态），Client收到后会重新传ACK给Server。 3.2.4 如果已经建立了连接，但客户端出现了故障怎么办？服务器每收到一次客户端的请求后都会重新复位一个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 3.2.5 初始序列号是什么？TCP连接的一方A，随机选择一个32位的序列号（Sequence Number）作为发送数据的初始序列号（Initial Sequence Number，ISN），比如为1000，以该序列号为原点，对要传送的数据进行编号：1001、1002…三次握手时，把这个初始序列号传送给另一方B，以便在传输数据时，B可以确认什么样的数据编号是合法的；同时在进行数据传输时，A还可以确认B收到的每一个字节，如果A收到了B的确认编号（acknowledge number）是2001，就说明编号为1001-2000的数据已经被B成功接受。 3.3 四次挥手 第一次挥手：Client将FIN置为1，发送一个序列号seq给Server；进入FIN_WAIT_1状态； 第二次挥手：Server收到FIN之后，发送一个ACK=1，acknowledge number=收到的序列号+1；进入CLOSE_WAIT状态。此时客户端已经没有要发送的数据了，但仍可以接受服务器发来的数据。 第三次挥手：Server将FIN置1，发送一个序列号给Client；进入LAST_ACK状态； 第四次挥手：Client收到服务器的FIN后，进入TIME_WAIT状态；接着将ACK置1，发送一个acknowledge number=序列号+1给服务器；服务器收到后，确认acknowledge number后，变为CLOSED状态，不再向客户端发送数据。客户端等待2*MSL（报文段最长寿命）时间后，也进入CLOSED状态。完成四次挥手。 3.3.1 为什么不能把服务器发送的ACK和FIN合并起来，变成三次挥手（CLOSE_WAIT状态意义是什么）？因为服务器收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复ACK，表示接收到了断开连接的请求。等到数据发完之后再发FIN，断开服务器到客户端的数据传送。 3.3.2 如果第二次挥手时服务器的ACK没有送达客户端，会怎样？客户端没有收到ACK确认，会重新发送FIN请求。 3.3.3 客户端TIME_WAIT状态的意义是什么？第四次挥手时，客户端发送给服务器的ACK有可能丢失，TIME_WAIT状态就是用来重发可能丢失的ACK报文。如果Server没有收到ACK，就会重发FIN，如果Client在2*MSL的时间内收到了FIN，就会重新发送ACK并再次等待2MSL，防止Server没有收到ACK而不断重发FIN。 MSL(Maximum Segment Lifetime)，指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。 3.4 流量控制 使用滑动窗口协议实现流量控制。防止发送方发送速率太快，接收方缓存区不够导致溢出。接收方会维护一个接收窗口 receiver window（窗口大小单位是字节），接受窗口的大小是根据自己的资源情况动态调整的，在返回ACK时将接受窗口大小放在TCP报文中的窗口字段告知发送方。发送窗口的大小不能超过接受窗口的大小，只有当发送方发送并收到确认之后，才能将发送窗口右移。 发送窗口的上限为接受窗口和拥塞窗口中的较小值。接受窗口表明了接收方的接收能力，拥塞窗口表明了网络的传送能力。 3.4.1 什么是零窗口（接收窗口为0时会怎样）？接收方没有能力接收数据，就会将接收窗口设置为0，这时发送方必须暂停发送数据，但是会启动一个持续计时器(persistence timer)，到期后发送一个大小为1字节的探测数据包，以查看接收窗口状态。如果接收方能够接收数据，就会在返回的报文中更新接收窗口大小，恢复数据传送。 3.5 拥塞控制 拥塞控制主要由四个算法组成：慢启动（Slow Start）、拥塞避免（Congestion voidance）、快重传 （Fast Retransmit）、快恢复（Fast Recovery） 慢启动：刚开始发送数据时，先把拥塞窗口（congestion window）设置为一个最大报文段MSS的数值，每收到一个新的确认报文之后，就把拥塞窗口加1个MSS。这样每经过一个传输轮次（或者说是每经过一个往返时间RTT），拥塞窗口的大小就会加倍 拥塞避免：当拥塞窗口的大小达到慢启动门限(slow start threshold)时，开始执行拥塞避免算法，拥塞窗口大小不再指数增加，而是线性增加，即每经过一个传输轮次只增加1MSS. 无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限ssthresh设置为出现拥塞时的发送方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢启动算法。（这是不使用快重传的情况） 快重传：快重传要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快恢复：当发送方连续收到三个重复确认时，就把慢启动门限减半，然后执行拥塞避免算法。不执行慢开始算法的原因：因为如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方认为现在网络可能没有出现拥塞。也有的快重传是把开始时的拥塞窗口cwnd值再增大一点，即等于 ssthresh + 3*MSS 。这样做的理由是：既然发送方收到三个重复的确认，就表明有三个分组已经离开了网络。这三个分组不再消耗网络的资源而是停留在接收方的缓存中。可见现在网络中减少了三个分组。因此可以适当把拥塞窗口扩大些。 四、面试题4.1 TCP与UDP的区别 TCP是面向连接的，UDP是无连接的；（UDP发送数据之前不需要建立连接） TCP是可靠的，UDP不可靠；（UDP接收方收到报文后，不需要给出任何确认） TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多； TCP是面向字节流的，UDP是面向报文的；（面向字节流是指发送数据时以字节为单位，一个数据包可以拆分成若干组进行发送，而UDP一个报文只能一次发完。） TCP有拥塞控制机制，UDP没有。网络出现的拥塞不会使源主机的发送速率降低，这对某些实时应用是很重要的，比如媒体通信，游戏； TCP首部开销（20字节）比UDP首部开销（8字节）要大 UDP 的主机不需要维持复杂的连接状态表 什么时候选择TCP，什么时候选UDP？对某些实时性要求比较高的情况，选择UDP，比如游戏，媒体通信，实时视频流（直播），即使出现传输错误也可以容忍；其它大部分情况下，HTTP都是用TCP，因为要求传输的内容可靠，不出现丢失 HTTP可以使用UDP吗？HTTP不可以使用UDP，HTTP需要基于可靠的传输协议，而UDP不可靠 面向连接和无连接的区别无连接的网络服务（数据报服务）– 面向连接的网络服务（虚电路服务） 虚电路服务：首先建立连接，所有的数据包经过相同的路径，服务质量有较好的保证； 数据报服务：每个数据包含目的地址，数据路由相互独立（路径可能变化）；网络尽最大努力交付数据，但不保证不丢失、不保证先后顺序、不保证在时限内交付；网络发生拥塞时，可能会将一些分组丢弃； 4.2 TCP如何保证传输的可靠性 数据包校验 对失序数据包重新排序（TCP报文具有序列号） 丢弃重复数据 应答机制：接收方收到数据之后，会发送一个确认（通常延迟几分之一秒）； 超时重发：发送方发出数据之后，启动一个定时器，超时未收到接收方的确认，则重新发送这个数据； 流量控制：确保接收端能够接收发送方的数据而不会缓冲区溢出 4.3 UDP如何实现可靠传输UDP不属于连接协议，具有资源消耗少，处理速度快的优点，所以通常音频，视频和普通数据在传送时，使用UDP较多，因为即使丢失少量的包，也不会对接受结果产生较大的影响。 传输层无法保证数据的可靠传输，只能通过应用层来实现了。实现的方式可以参照tcp可靠性传输的方式，只是实现不在传输层，实现转移到了应用层。 最简单的方式是在应用层模仿传输层TCP的可靠性传输。下面不考虑拥塞处理，可靠UDP的简单设计。 1、添加seq/ack机制，确保数据发送到对端 2、添加发送和接收缓冲区，主要是用户超时重传。 3、添加超时重传机制。 详细说明：发送端发送数据时，生成一个随机seq=x，然后每一片按照数据大小分配seq。数据到达接收端后接收端放入缓存，并发送一个ack=x的包，表示对方已经收到了数据。发送端收到了ack包后，删除缓冲区对应的数据。时间到后，定时任务检查是否需要重传数据。 目前有如下开源程序利用udp实现了可靠的数据传输。分别为RUDP、RTP、UDT。 总结：UDP要想可靠，就要接收方收到UDP之后回复个确认包，发送方有个机制，收不到确认包就要重新发送；每个包有递增的序号，接收方发现中间丢了包就要发重传请求；当网络太差时候频繁丢包，防止越丢包越重传的恶性循环，所以要有个发送窗口的限制，发送窗口的大小根据网络传输情况调整，调整算法要有一定自适应性。","categories":[{"name":"网络","slug":"网络","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/"},{"name":"TCP/IP","slug":"网络/TCP-IP","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/TCP-IP/"}],"tags":[{"name":"TCP","slug":"TCP","permalink":"http://hofe.work/tags/TCP/"},{"name":"UDP","slug":"UDP","permalink":"http://hofe.work/tags/UDP/"}]},{"title":"Session与Cookie","slug":"网络/Session与Cookie","date":"2020-05-08T05:57:00.000Z","updated":"2020-05-30T09:09:49.263Z","comments":true,"path":"2020/05/08/网络/Session与Cookie/","link":"","permalink":"http://hofe.work/2020/05/08/%E7%BD%91%E7%BB%9C/Session%E4%B8%8ECookie/","excerpt":"","text":"欢迎来我的个人网站，里面有最新的版本 这篇介绍了下Session 和 Cookie面试题常考内容，并做了延伸，解释了原理。没有太过详细，之后会补充。 Cookie与Session一、CookieHTTP协议是无状态的协议。一旦数据交换完毕，客户端与服务器端的连接就会关闭，再次交换数据需要建立新的连接，这就意味着服务器无法从连接上跟踪会话。 Cookie通过在客户端记录信息确定用户身份，在客户端发起请求的时候，携带上cookie，服务器通过该cookie辨认用户信息。以此来弥补HTTP协议无状态的不足。 实现原理Cookie是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。 二、SessionSession是服务器端使用的一种记录客户端状态的机制，使用上比Cookie简单一些，相应的也增加了服务器的存储压力。 实现原理客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。 保存Sessionid的方式可以采用Cookie，如果禁用了Cookie，可以使用URL重写机制（把会话ID保存在URL中）。 三、核心概念Session和Cookie的区别？ 数据存储位置：cookie数据存放在客户的浏览器上，客户端请求服务器时会将Cookie一起提交；session数据放在服务器上，通过检索Sessionid查看状态。 安全性：cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗，考虑到安全应当使用session。 服务器性能：session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，考虑到减轻服务器性能方面，应当使用cookie。 数据大小：单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。 信息重要程度：可以考虑将登陆信息等重要信息存放为session，其他信息如果需要保留，可以放在cookie中。 Cookie、session和token的区别Cookie的原理 （1）客户端第一次请求时，发送数据到服务器。 （2）服务器返回响应信息的同时，还会传回一个cookie（cookie S-001） （3）客户端接收服务器的响应之后，浏览器会将cookie存放在一个统一的位置。 （4）客户端再次向服务器发送请求的时候，会把Cookie S-001再次发挥服务器。 Token的原理 （1）客户端第一次请求时，发送用户信息到服务器。服务器对用户信息使用HSA256算法及密钥进行签名，再将这个签名和数据一起作为token返回给客户户端。 （2）服务端不再保存token，客户端保存token。 （3）当客户端再次发送请求时，在请求信息中将token一起发送给服务器。 （4）服务器用同样的HSA256算法和密钥，对数据再计算一次签名，和token的签名做比较 （5）如果相同，服务器就知道客户端登录过，则反之。 session的原理 （1）服务器在处理客户端请求过程中会创建session，并且为该session生存唯一的session ID。（这个session ID在随后的请求中会被用来重新获得已经创建的session。在session被创建后，就可以调用session相关的方法向session中新增内容，这些内容只会保存在服务器中） （2）服务器将session ID发送到客户端 （3）当客户端再次请求时，就会带上这个session ID （4）服务器接收到请求之后就会一句Session ID 找到相应的Session ，完成请求","categories":[{"name":"网络","slug":"网络","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/"},{"name":"HTTP","slug":"网络/HTTP","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/HTTP/"}],"tags":[{"name":"Session","slug":"Session","permalink":"http://hofe.work/tags/Session/"},{"name":"Cookie","slug":"Cookie","permalink":"http://hofe.work/tags/Cookie/"}]},{"title":"设计模式","slug":"Java/设计模式","date":"2020-05-07T06:59:27.000Z","updated":"2020-05-23T13:02:53.440Z","comments":true,"path":"2020/05/07/Java/设计模式/","link":"","permalink":"http://hofe.work/2020/05/07/Java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"只完成了单例、代理、装饰三种，其余待完善 一、设计模式划分 创建型模式：对象实例化的模式，创建型模式用于解耦对象的实例化过程。 结构型模式：把类或对象结合在一起形成一个更大的结构。 行为型模式：类和对象如何交互，及划分责任和算法。 二、创建型模式2.1 单例模式 某个类只能有一个实例，提供一个全局的访问点。 2.1.1 实现原理使用一个私有构造函数、一个私有静态变量以及一个公有静态函数来实现。 私有构造函数保证了不能通过构造函数来创建对象实例，只能通过公有静态函数返回唯一的私有静态变量。 使用场景 ● 要求生成唯一序列号的环境； ● 在整个项目中需要一个共享访问点或共享数据，例如一个Web页面上的计数器，可以不用把每次刷新都记录到数据库中，使用单例模式保持计数器的值，并确保是线程安全的； ● 创建一个对象需要消耗的资源过多，如要访问IO和数据库等资源； ● 需要定义大量的静态常量和静态方法（如工具类）的环境，可以采用单例模式（当然，也可以直接声明为static的方式）。 2.1.2 懒汉式-线程不安全只要实例还未被初始化，则new Singleton() public class Singleton &#123; private static Singleton uniqueInstance; // 私有静态变量 private Singleton() &#123; // 私有构造函数 &#125; public static Singleton getUniqueInstance() &#123; // 公有静态函数 if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; 但是这种方式在多线程下是不安全的，多个线程能够同时进入 if (uniqueInstance == null) ，并且此时 uniqueInstance 为 null，那么会有多个线程执行 uniqueInstance = new Singleton(); 语句，这将导致实例化多次 uniqueInstance。 2.1.3 懒汉式-线程安全只需要对 getUniqueInstance() 方法加锁，那么在一个时间点只能有一个线程能够进入该方法，从而避免了实例化多次 uniqueInstance。 但是当一个线程进入该方法之后，其它试图进入该方法的线程都必须等待，即使 uniqueInstance 已经被实例化了。这会让线程阻塞时间过长，因此该方法有性能问题，不推荐使用。 public class Singleton &#123; // 私有静态变量 private static Singleton uniqueInstance; // 私有构造函数 private Singleton() &#123; &#125; // 公有静态函数 public static synchronized Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; return uniqueInstance; &#125;&#125; 2.1.4 饿汉式-线程安全该模式的特点是类一旦加载就创建一个单例，保证在调用 getInstance 方法之前单例已经存在了。这样多线程也就创建多个实例了 public class Singleton &#123; private static Singleton uniqueInstance = new Singleton(); // 私有静态变量 private Singleton() &#123; // 私有构造函数 &#125; public static Singleton getUniqueInstance() &#123; // 公有静态函数 return uniqueInstance; &#125;&#125; 直接实例化的方式也丢失了延迟实例化带来的节约资源的好处。 2.1.5 双重校验锁-线程安全只有当 uniqueInstance 没有被实例化时，才需要进行加synchronize锁，不对整个方法加锁。但这时候多个线程都可以进入该方法了，就可能获取到还没初始化的实例，所以同加volatile，防止指令重排序。 public class Singleton &#123; private volatile static Singleton uniqueInstance; private Singleton() &#123; &#125; public static Singleton getUniqueInstance() &#123; if (uniqueInstance == null) &#123; // 多个进程可进入，依次加锁new 对象 synchronized (Singleton.class) &#123; // 第二重校验，防止第二个线程再new 一次 if (uniqueInstance == null) &#123; uniqueInstance = new Singleton(); &#125; &#125; &#125; return uniqueInstance; &#125;&#125; 2.1.6 静态内部类-安全-延迟加载当 Singleton 类被加载时，静态内部类 SingletonHolder 没有被加载进内存。只有当调用 getUniqueInstance() 方法从而触发 SingletonHolder.INSTANCE 时 SingletonHolder 才会被加载，此时初始化 INSTANCE 实例，并且 JVM 能确保 INSTANCE 只被实例化一次。 这种方式不仅具有延迟初始化的好处，而且由 JVM 提供了对线程安全的支持。 public class Singleton &#123; private Singleton() &#123; &#125; // 静态内部类 private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; public static Singleton getUniqueInstance() &#123; return SingletonHolder.INSTANCE; &#125;&#125; 2.2 简单工厂一个工厂类根据传入的参量决定创建出那一种产品类的实例。 2.3 工厂方法定义一个创建对象的接口，让子类决定实例化那个类。 2.4 抽象工厂创建相关或依赖对象的家族，而无需明确指定具体类。 2.5 建造者模式封装一个复杂对象的构建过程，并可以按步骤构造。 2.6 原型模式通过复制现有的实例来创建新的实例。 三、结构型模式3.1 适配器模式将一个类的方法接口转换成客户希望的另外一个接口。 3.2 组合模式将对象组合成树形结构以表示“”部分-整体“”的层次结构。 3.3 装饰模式 动态的给对象添加新的功能。 3.3.1 实现原理 Component抽象构件：Component是一个接口或者是抽象类，就是定义我们最核心的对象，也就是最原始的对象 注意：在装饰模式中，必然有一个最基本、最核心、最原始的接口或抽象类充当Component抽象构件。 ConcreteComponent 具体构件 ConcreteComponent是最核心、最原始、最基本的接口或抽象类的实现，你要装饰的就是它。 Decorator装饰角色 一般是一个抽象类，做什么用呢？实现接口或者抽象方法，它里面可不一定有抽象的方法呀，在它的属性里必然有一个private变量指向Component抽象构件。 具体装饰角色 ConcreteDecoratorA和ConcreteDecoratorB是两个具体的装饰类，你要把你最核心的、最原始的、最基本的东西装饰成其他东西。 使用场景 需要扩展一个类的功能，或给一个类增加附加功能。 需要动态地给一个对象增加功能，这些功能可以再动态地撤销。 需要为一批的兄弟类进行改装或加装功能，当然是首选装饰模式。 当不能通过生成子类的方法进行扩充时 优点 无需通过继承增加子类就能扩展新功能。 使用关联关系比继承更加灵活，可以用添加和分离的方式，用装饰在运行时 增加和删除职责 避免在层次结构高的类有太多特征，用装饰器为其逐渐地添加功能 3.4 代理模式 代理模式给某一个对象提供一个代理对象，并由代理对象控制对原对象的引用。 代理有以下三类： 静态代理：被代理对象与代理对象需要一起实现相同的接口或者是继承相同父类，因此要定义一个接口或抽象类。可以做到在不修改目标对象的功能前提下，对目标功能扩展；但因为代理对象需要与目标对象实现一样的接口，所以会有很多代理类，类太多，同时，一旦接口增加方法，目标对象与代理对象都要维护。 动态代理：解决静态代理需要维护对象的问题。在程序运行时JVM才为被代理对象生成代理对象。常说的动态代理也叫做JDK代理也是一种接口代理，JDK中生成代理对象的代理类就是Proxy，所在包是java.lang.reflect。代理对象不需要实现接口，但是目标对象一定要实现接口，否则不能使用动态代理，因此这也算是这种方式的缺陷。 cglib代理：弥补静态代理和动态代理的目标对象都要实现接口的不足。并不是任何对象都会实现一个接口，也存在没有实现任何的接口的对象，这时就可以使用继承目标类作为目标对象子类的方式实现代理,这种方法就叫做:Cglib代理，也叫作子类代理，它是在内存中构建一个子类对象从而实现对目标对象功能的扩展。 总结： 静态代理，代理对象与目标对象都需要实现相同接口，一旦接口增加方法，代理对象和目标对象都要维护。 动态代理，只需要目标对象实现接口，程序运行时JVM才生成代理对象。 cglib代理，目标对象和代理对象都可以不实现接口，从内存中构建出一个子类来扩展目标对象的功能。 3.4.1 实现原理 记住以下关键点，简单易实现： 代理类与委托类实现同一接口 在委托类中实现功能，在代理类的方法中中引用委托类的同名方法 外部类调用委托类某个方法时，直接以接口指向代理类的实例，这正是代理的意义所在：屏蔽。 使用场景 当我们想要隐藏某个类时，可以为其提供代理类 当一个类需要对不同的调用者提供不同的调用权限时，可以使用代理类来实现（代理类不一定只有一个，我们可以建立多个代理类来实现，也可以在一个代理类中金进行权限判断来进行不同权限的功能调用） 当我们要扩展某个类的某个功能时，可以使用代理模式，在代理类中进行简单扩展（只针对简单扩展，可在引用委托类的语句之前与之后进行） 3.4.2 静态代理// 接口 interface IStar &#123; void sing(); &#125; // 真实对象 class LDHStar implements IStar &#123; @Override public void sing() &#123; System.out.println(\"刘德华唱歌\"); &#125; &#125; // 代理类需要有真实对象的控制权 (引用) class ProxyManger implements IStar &#123; // 真实对象的引用 private IStar star; public ProxyManger() &#123; super(); &#125; public ProxyManger(IStar star) &#123; super(); this.star = star; &#125; @Override public void sing() &#123; System.out.println(\"唱歌前准备\"); star.sing(); System.out.println(\"善后工作\"); &#125; &#125; class Test&#123; public static void main(String[] args) &#123; // 创建明星对象 IStar ldh = new LDHStar(); ProxyManger proxy = new ProxyManger(ldh); proxy.sing(); &#125;&#125; 3.4.2 动态代理//目标类接口interface IDog&#123; void run();&#125;//目标类class GunDog implements IDog&#123; @Override public void run() &#123; System.out.println(\"猎狗在跑\"); &#125;&#125;class DogUtils&#123; public static void method1() &#123; System.out.println(\"增强方式一\"); &#125; public static void method2() &#123; System.out.println(\"增强方式二\"); &#125;&#125;class MyInvocationHandle implements InvocationHandler&#123; private Object target; public void setTarget(Object target) &#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; DogUtils.method1(); method.invoke(target, args); DogUtils.method2(); return null; &#125;&#125; //生产代理对象的工厂 class MyProxyFactory&#123; public static Object getProxy(Object target) &#123; MyInvocationHandle handle = new MyInvocationHandle(); handle.setTarget(target); Object proxy = Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), handle); return proxy; &#125; &#125;public class ProxyDemo &#123; public static void main(String[] args) &#123; IDog dog = new GunDog(); IDog proxy =(IDog) MyProxyFactory.getProxy(dog); proxy.run(); &#125;&#125; 3.4.3 Cglib代理public class CglibProxy &#123; public static void main(String[] args) &#123; int[] arr = new int[100000]; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] = (int) (Math.random() * 1000); &#125; //实例化一个增强器，也就是cglib中的一个class generator Enhancer enhancer = new Enhancer(); //设置目标类 enhancer.setSuperclass(ArraySort2.class); //设置拦截对象，这里直接使用匿名内部类写法 enhancer.setCallback(new MethodInterceptor() &#123; @Override public Object intercept(Object object , Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; String sortName = method.getName(); switch (sortName) &#123; case \"bubbleSort\": sortName = \"冒泡排序\"; break; case \"selectSort\": sortName = \"选择排序\"; break; case \"quickSort\": sortName = \"快速排序\"; break; default: break; &#125; long start = System.currentTimeMillis(); //此处一定要使用proxy的invokeSuper方法来调用目标类的方法 proxy.invokeSuper(object, args); long end = System.currentTimeMillis(); System.out.println(\"本次\" + sortName + \"的执行时间为: \" + (end -start) + \"ms\"); return null; &#125; &#125;); //生成代理类并返回一个实例 ArraySort2 arraySort = (ArraySort2) enhancer.create(); arraySort.bubbleSort(arr); arraySort.selectSort(arr); arraySort.quickSort(arr); &#125; &#125;class ArraySort2&#123; public void quickSort(int[] arr) &#123; Arrays.sort(arr); &#125; public void selectSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = i+1; j &lt; arr.length; j++) &#123; if (arr[i] &gt; arr[j]) &#123; int temp = 0; temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; &#125; &#125; &#125; public void bubbleSort(int[] arr) &#123; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int temp = 0; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; &#125; &#125;&#125; 3.5 亨元模式通过共享技术来有效的支持大量细粒度的对象。 3.6 外观模式对外提供一个统一的方法，来访问子系统中的一群接口。 3.7 桥接模式将抽象部分和它的实现部分分离，使它们都可以独立的变化。 四、行为型模式4.1 模板模式定义一个算法结构，而将一些步骤延迟到子类实现。 4.2 解释器模式给定一个语言，定义它的文法的一种表示，并定义一个解释器。 4.3 策略模式定义一系列算法，把他们封装起来，并且使它们可以相互替换。 4.4 状态模式允许一个对象在其对象内部状态改变时改变它的行为。 4.5 观察者模式对象间的一对多的依赖关系。 4.6 备忘录模式在不破坏封装的前提下，保持对象的内部状态。 4.7 中介者模式用一个中介对象来封装一系列的对象交互。 4.8 命令模式将命令请求封装为一个对象，使得可以用不同的请求来进行参数化。 4.9 访问者模式在不改变数据结构的前提下，增加作用于一组对象元素的新功能。 4.10 责任链模式 将请求的发送者和接收者解耦，使的多个对象都有处理这个请求的机会。，将 这些处理器对象连成一条链，并沿这条链传递请求，直到有一个处理器对象处理它为止 4.10.1 实现原理适用场景 1）有多个处理器对象可以处理一个请求，哪个处理器对象处理该请求在运行时动态确定 2）在不明确指定接收者的情况下，向多个处理器对象中的一个提交请求 3）可以动态指定一组处理器对象处理请求 优点 1）降低耦合，使得 请求发送者无需知道是哪个处理器对象处理请求 2）简化对象的相互连接 3）增强了给对象指派责任的灵活性 4）方便添加新的请求处理类 4.11 迭代器模式一种遍历访问聚合对象中各个元素的方法，不暴露该对象的内部结构。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"设计模式","slug":"Java/设计模式","permalink":"http://hofe.work/categories/Java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"http://hofe.work/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"秒杀架构的设计","slug":"微服务分布式集群/秒杀架构的设计","date":"2020-05-07T03:43:45.000Z","updated":"2020-05-09T13:35:22.160Z","comments":true,"path":"2020/05/07/微服务分布式集群/秒杀架构的设计/","link":"","permalink":"http://hofe.work/2020/05/07/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/%E7%A7%92%E6%9D%80%E6%9E%B6%E6%9E%84%E7%9A%84%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"可访问个人网站进行阅读最新版本 转载：https://mp.weixin.qq.com/s?__biz=MzA3ODQ0Mzg2OA==&amp;mid=2649051049&amp;idx=2&amp;sn=ac38a47f2e5f6f6700a82c4dfda5961b&amp;chksm=87534b9ab024c28c8cdaebdf4800ab741c8780d6a185850043b7ac573f58f0cf24c5e51ddea1&amp;mpshare=1&amp;scene=23&amp;srcid=0509QxlDOcqtDO61HcLNaKQm&amp;sharer_sharetime=1589020470217&amp;sharer_shareid=d476d18cbe4a83b141ea1ff413565f8c#rd 一、秒杀应该考虑哪些问题1.1 超卖问题这就不用解释啥意思了趴 1.2 高并发秒杀具有时间短、并发量大的特点，秒杀持续时间只有几分钟，而一般公司都为了制造轰动效应，会以极低的价格来吸引用户，因此参与抢购的用户会非常的多。短时间内会有大量请求涌进来，后端如何防止并发过高造成缓存击穿或者失效，击垮数据库都是需要考虑的问题。 1.3 接口防刷现在的秒杀大多都会出来针对秒杀对应的软件，这类软件会模拟不断向后台服务器发起请求，一秒几百次都是很常见的，如何防止这类软件的重复无效请求，防止不断发起的请求也是需要我们针对性考虑的 1.4 秒杀url对于普通用户来讲，看到的只是一个比较简单的秒杀页面，在未达到规定时间，秒杀按钮是灰色的，一旦到达规定时间，灰色按钮变成可点击状态。这部分是针对小白用户的，如果是稍微有点电脑功底的用户，会通过F12看浏览器的network看到秒杀的url，通过特定软件去请求也可以实现秒杀。或者提前知道秒杀url的人，一请求就直接实现秒杀了。这个问题我们需要考虑解决 1.5 数据库设计秒杀有把我们服务器击垮的风险，如果让它与我们的其他业务使用在同一个数据库中，耦合在一起，就很有可能牵连和影响其他的业务。如何防止这类问题发生，就算秒杀发生了宕机、服务器卡死问题，也应该让他尽量不影响线上正常进行的业务 1.6 大量请求问题按照1.2的考虑，就算使用缓存还是不足以应对短时间的高并发的流量的冲击。如何承载这样巨大的访问量，同时提供稳定低时延的服务保证，是需要面对的一大挑战。我们来算一笔账，假如使用的是redis缓存，单台redis服务器可承受的QPS大概是10W+左右，如果一个秒杀吸引的用户量足够多的话，单QPS可能达到几十万，单体redis还是不足以支撑如此巨大的请求量。缓存会被击穿，直接渗透到DB,从而击垮mysql.后台会将会大量报错 二、秒杀系统的设计和技术方案 2.1 秒杀页面静态化将商品的描述、参数、成交记录、图像、评价等全部写入到一个静态页面，用户请求不需要通过访问后端服务器，不需要经过数据库，直接在前台客户端生成，这样可以最大可能的减少服务器的压力。 具体的方法可以使用freemarker模板技术，建立网页模板，填充数据，然后渲染网页 2.2 接口限流秒杀最终的本质是数据库的更新，但是有很多大量无效的请求，我们最终要做的就是如何把这些无效的请求过滤掉，防止渗透到数据库。限流的话，需要入手的方面很多： 2.2.1：前端限流首先第一步就是通过前端限流，用户在秒杀按钮点击以后发起请求，那么在接下来的5秒是无法点击(通过设置按钮为disable)。这一小举措开发起来成本很小，但是很有效。 2.2.2：同一个用户xx秒内重复请求直接拒绝具体多少秒需要根据实际业务和秒杀的人数而定，一般限定为10秒。具体的做法就是通过redis的键过期策略，首先对每个请求都从String value = redis.get(userId);如果获取到这个value为空或者为null，表示它是有效的请求，然后放行这个请求。如果不为空表示它是重复性请求，直接丢掉这个请求。如果有效,采用redis.setexpire(userId,value,10).value可以是任意值，一般放业务属性比较好,这个是设置以userId为key，10秒的过期时间(10秒后,key对应的值自动为null) 2.3 令牌桶算法限流接口限流的策略有很多，我们这里采用令牌桶算法。令牌桶算法的基本思路是每个请求尝试获取一个令牌，后端只处理持有令牌的请求，生产令牌的速度和效率我们都可以自己限定，guava提供了RateLimter的api供我们使用。 以下做一个简单的例子,注意需要引入guava public class TestRateLimiter &#123; public static void main(String[] args) &#123; //1秒产生1个令牌 final RateLimiter rateLimiter = RateLimiter.create(1); for (int i = 0; i &lt; 10; i++) &#123; //该方法会阻塞线程，直到令牌桶中能取到令牌为止才继续向下执行。 double waitTime= rateLimiter.acquire(); System.out.println(\"任务执行\" + i + \"等待时间\" + waitTime); &#125; System.out.println(\"执行结束\"); &#125;&#125; 上面代码的思路就是通过RateLimiter来限定我们的令牌桶每秒产生1个令牌(生产的效率比较低)，循环10次去执行任务。 acquire会阻塞当前线程直到获取到令牌，也就是如果任务没有获取到令牌，会一直等待。那么请求就会卡在我们限定的时间内才可以继续往下走，这个方法返回的是线程具体等待的时间。执行如下; 可以看到任务执行的过程中，第1个是无需等待的，因为已经在开始的第1秒生产出了令牌。接下来的任务请求就必须等到令牌桶产生了令牌才可以继续往下执行。 如果没有获取到就会阻塞(有一个停顿的过程)。不过这个方式不太好，因为用户如果在客户端请求，如果较多的话,直接后台在生产token就会卡顿(用户体验较差)，它是不会抛弃任务的，我们需要一个更优秀的策略:如果超过某个时间没有获取到，直接拒绝该任务。接下来再来个案例： public class TestRateLimiter2 &#123; public static void main(String[] args) &#123; final RateLimiter rateLimiter = RateLimiter.create(1); for (int i = 0; i &lt; 10; i++) &#123; long timeOut = (long) 0.5; boolean isValid = rateLimiter.tryAcquire(timeOut, TimeUnit.SECONDS); System.out.println(\"任务\" + i + \"执行是否有效:\" + isValid); if (!isValid) &#123; continue; &#125; System.out.println(\"任务\" + i + \"在执行\"); &#125; System.out.println(\"结束\"); &#125;&#125; 其中用到了tryAcquire方法，这个方法的主要作用是设定一个超时的时间，如果在指定的时间内预估(注意是预估并不会真实的等待)，如果能拿到令牌就返回true，如果拿不到就返回false. 然后我们让无效的直接跳过，这里设定每秒生产1个令牌，让每个任务尝试在0.5秒获取令牌，如果获取不到,就直接跳过这个任务(放在秒杀环境里就是直接抛弃这个请求)；程序实际运行如下： 只有第1个获取到了令牌，顺利执行了，下面的基本都直接抛弃了，因为0.5秒内，令牌桶(1秒1个)来不及生产就肯定获取不到返回false了。 这个限流策略的效率有多高呢？假如我们的并发请求是400万瞬间的请求,将令牌产生的效率设为每秒20个，每次尝试获取令牌的时间是0.05秒，那么最终测试下来的结果是，每次只会放行4个左右的请求,大量的请求会被拒绝,这就是令牌桶算法的优秀之处。 2.4 秒杀url的设计为了避免有程序访问经验的人通过下单页面url直接访问后台接口来秒杀货品，我们需要将秒杀的url实现动态化，即使是开发整个系统的人都无法在秒杀开始前知道秒杀的url。 具体的做法就是通过md5加密一串随机字符作为秒杀的url，然后前端访问后台获取具体的url，后台校验通过之后才可以继续秒杀。 2.5 单体redis升级为集群redis秒杀是一个读多写少的场景，使用redis做缓存再合适不过。不过考虑到缓存击穿问题，我们应该构建redis集群，采用哨兵模式，可以提升redis的性能和可用性。 2.6 redis预减库存很多请求进来，都需要后台查询库存,这是一个频繁读的场景。可以使用redis来预减库存，在秒杀开始前可以在redis设值，比如redis.set(goodsId,100),这里预放的库存为100可以设值为常量),每次下单成功之后,Integer stock = (Integer)redis.get(goosId); 然后判断sock的值，如果小于常量值就减去1; 不过注意当取消的时候,需要增加库存，增加库存的时候也得注意不能大于之间设定的总库存数(查询库存和扣减库存需要原子操作，此时可以借助lua脚本)下次下单再获取库存的时候,直接从redis里面查就可以了。 2.7 异步下单为了提升下单的效率，并且防止下单服务的失败。需要将下单这一操作进行异步处理。最常采用的办法是使用队列，队列最显著的三个优点：异步、削峰、解耦。这里可以采用rabbitmq，在后台经过了限流、库存校验之后，流入到这一步骤的就是有效请求。然后发送到队列里，队列接受消息，异步下单。下完单，入库没有问题可以用短信通知用户秒杀成功。假如失败的话,可以采用补偿机制，重试。 2.8 秒杀系统数据库设计针对1.5提出的秒杀数据库的问题，因此应该单独设计一个秒杀数据库，防止因为秒杀活动的高并发访问拖垮整个网站。 2.9 精简sql典型的一个场景是在进行扣减库存的时候，传统的做法是先查询库存，再去update。这样的话需要两个sql，而实际上一个sql我们就可以完成的。本质是利用数据库锁 可以用这样的做法： update miaosha_goods set stock = stock-1 where goos_id =&#123;#goods_id&#125; and version = #&#123;version&#125; and stock&gt;0; 这样的话，就可以保证库存不会超卖并且一次更新库存,还有注意一点这里使用了版本号的乐观锁，相比较悲观锁，它的性能较好。 2.10 使用nginxnginx是一个高性能web服务器，它的并发能力可以达到几万，而tomcat只有几百。通过nginx映射客户端请求，再分发到后台tomcat服务器集群中可以大大提升并发能力。 2.11 服务降级假如在秒杀过程中出现了某个服务器宕机，或者服务不可用，应该做好后备工作。之前的博客里有介绍通过Hystrix进行服务熔断和降级，可以开发一个备用服务，假如服务器真的宕机了，直接给用户一个友好的提示返回，而不是直接卡死，服务器错误等生硬的反馈。 三、面试题3.1 为什么会超卖超卖的原因在于减库存操作是一个事务操作，需要先select，然后insert，最后update -1。最后这个-1操作是不能出现负数的，但是当多用户在有库存的情况下并发操作，出现负数这是无法避免的。 3.2 怎么解决超卖少卖的问题超卖问题是用redis+mq解决的，先将mysql中商品库存信息写入redis；然后每次下单都从redis中做库存预减1，如果库存值减1后大于0，则判断该用户秒杀成功，进入消息队列，做异步减库存，下订单操作。因为redis是单线程的，所以不会存在超卖并发问题。 Redis分布式锁Redis预减库存就不会超卖吗了解分布式事务吗说一下分布式唯一ID生成","categories":[{"name":"分布式","slug":"分布式","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"秒杀","slug":"分布式/秒杀","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E7%A7%92%E6%9D%80/"}],"tags":[{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"秒杀","slug":"秒杀","permalink":"http://hofe.work/tags/%E7%A7%92%E6%9D%80/"}]},{"title":"MySQL学习笔记进阶篇","slug":"数据库/通过面试题学MySQL进阶篇","date":"2020-05-06T02:42:33.000Z","updated":"2020-05-26T06:26:56.131Z","comments":true,"path":"2020/05/06/数据库/通过面试题学MySQL进阶篇/","link":"","permalink":"http://hofe.work/2020/05/06/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%80%9A%E8%BF%87%E9%9D%A2%E8%AF%95%E9%A2%98%E5%AD%A6MySQL%E8%BF%9B%E9%98%B6%E7%AF%87/","excerpt":"","text":"欢迎来我的个人网站，里面有最新的版本 这篇介绍了下MySQL的主从同步及分库分表 常见面试题数据库如何实现并发（主从、分库） mysql 主从同步怎么搞的？分哪几个过程？如果有一台新机器要加到从机里，怎么个过程。 binlog 日志是 master 推的还是 salve 来拉的？ 一、主从复制（同步/分离） 1.1 主从复制原理复制功能并不是copy文件来实现的，而是借助binlog日志文件里面的SQL命令实现的主从复制，可以理解为我再Master端执行了一条SQL命令，那么在Salve端同样会执行一遍，从而达到主从复制的效果； 主库会做哪些事？ 1)、在master机器上的操作：当master上的数据发生变化时，该事件变化会按照顺序写入bin-log中； 当slave链接到master的时候，master机器会为slave开启binlog dump线程； 当master的binlog发生变化的时候，bin-log dump线程会通知slave，并将相应的binlog内容发送给slave。 主库会做哪些事？ 2)、在slave机器上操作： 当主从同步开启的时候，slave上会创建两个线程：I/O线程和SQL线程。 I/O线程。该线程连接到master机器，master机器上的binlog dump 线程会将binlog的内容发送给该I\\O线程，该I/O线程接收到binlog内容后，再将内容写入到本地的relay log； Sql线程。该线程读取到I/O线程写入的ralay log。并且根据relay log 的内容对slave数据库做相应的操作。 1.2 主从复制流程分为同步复制和异步复制，实际复制架构中大部分为异步复制。 复制的基本过程如下： 在Master机器上，主从同步事件会被写到特殊的log文件中(binary-log); 主从同步事件有3种形式:statement、row、mixed。 statement：会将对数据库操作的sql语句写入到binlog中。 row：会将每一条数据的变化写入到binlog中。 mixed：statement与row的混合。Mysql决定什么时候写statement格式的，什么时候写row格式的binlog。 1）当master上的数据发生改变的时候，该事件(insert、update、delete)变化会按照顺序写入到binlog中。 2）当slave连接到master的时候，master机器会为slave开启binlog dump线程。当master 的 binlog发生变化的时候，binlog dump线程会通知slave。 在Slave机器上 1) Slave上面的IO进程连接上Master，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容； 2) Master接收到来自Slave的IO进程的请求后，通过负责复制的IO进程根据请求信息读取制定日志指定位置之后的日志信息，返回给Slave 的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置； 3) Slave的IO进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到的Master端的 bin-log的文件名和位置记录到master-info文件中，以便在下一次读取的时候能够清楚的告诉Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我”； 4) Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时候的那些可执行的内容，并在自身执行。 1.3 主从复制优点主从复制可以实现数据备份、故障转移、MySQL集群、高可用、读写分离 实现服务器负载均衡(读写分离) 通过复制实现数据的异地备份(数据备份) 提高数据库系统的可用性(故障转移) 1.4 数据不一致问题主从同步延迟 导致主库从库数据不一致问题的及解决方案 具体请参考 https://www.dgstack.cn/archives/887.html 1.忽略错误后，继续同步该方法适用于主从库数据相差不大，或者要求数据可以不完全统一的情况，数据要求不严格的情况 2.强制读主 使用一个高可用主库提供数据库服务，读和写都落在主库上；采用缓存来提升系统性能 3.选择性读主 在缓存中记录哪些数据发生过写请求，来路由读主还是读从 二、分库分表当DB的数据量级到达一个阶段，写入和读取的速度会出现瓶颈，即使是有索引，索引也会变得很大，而且数据库的物理文件会使备份和恢复等操作变的很困难。这个时候由于DB的瓶颈已经严重危害到了业务，最有效的解决方案莫过于DB的分库分表了。 数据库分布式核心内容无非就是数据切分（Sharding），以及切分后对数据的定位、整合。数据切分就是将数据分散存储到多个数据库中，使得单一数据库中的数据量变小，通过扩充主机的数量缓解单一数据库的性能问题，从而达到提升数据库操作性能的目的。 数据切分根据其切分类型，可以分为两种方式：垂直（纵向）切分和水平（横向）切分 垂直（纵向）切分垂直切分常见有垂直分库和垂直分表两种。 垂直分库就是根据业务耦合性，将关联度低的不同表存储在不同的数据库。与”微服务治理”的做法相似，每个微服务使用单独的一个数据库。 概念：以 表为依据，按照业务归属不同，将不同的 表拆分到不同的 库中 。 结果： 每个 库的 结构都不一样； 每个 库的 数据也不一样，没有交集； 所有 库的 并集是全量数据； 场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。 分析：到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。 垂直分表是基于数据库中的”列”进行，将不经常用或字段长度较大的字段拆分出去到扩展表中。 在字段很多的情况下（例 如一个大表有100多个字段），通过”大表拆小表”，更便于开发与维护，也能避免跨页问题，MySQL底层是通过数据页存储的，一条记录占用空间过大会导 致跨页，造成额外的性能开销。另外数据库以行为单位将数据加载到内存中，这样表中字段长度较短且访问频率较高，内存能加载更多的数据，命中率更高，减少了 磁盘IO，从而提升了数据库性能。 概念：以 字段为依据，按照字段的活跃性，将 表中字段拆到不同的 表（主表和扩展表）中。 结果： 每个 表的 结构都不一样； 每个 表的 数据也不一样，一般来说，每个表的 字段至少有一列交集，一般是主键，用于关联数据； 所有 表的 并集是全量数据； 场景：系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。 分析：可以用列表页和详情页来帮助理解。垂直分表的拆分原则是将热点数据（可能会冗余经常一起查询的数据）放在一起作为主表，非热点数据放在一起作为扩展表。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。拆了之后，要想获得全部数据就需要关联两个表来取数据。但记住，千万别用join，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）。关联数据，应该在业务Service层做文章，分别获取主表和扩展表数据然后用关联字段关联得到全部数据。 水平（横向）切分当一个应用难以再细粒度的垂直切分，或切分后数据量行数巨大，存在单库读写、存储性能瓶颈，这时候就需要进行水平切分了。 水平切分分为库内分表和分库分表，是根据表内数据内在的逻辑关系，将同一个表按不同的条件分散到多个数据库或多个表中，每个表中只包含一部分数据，从而使得单个表的数据量变小，达到分布式的效果。 水平分库 概念：以 字段为依据 ，按照一定策略（hash、range等），将一个 库中的数据拆分到多个 库中。 结果： 每个 库的 结构都一样； 每个 库的 数据都不一样，没有交集； 所有 库的 并集是全量数据； 场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。 分析：库多了，io和cpu的压力自然可以成倍缓解。 水平分表 概念：以 字段为依据 ，按照一定策略（hash、range等），将一个 表中的数据拆分到多个 表中。 结果： 每个 表的 结构都一样； 每个 表的 数据都不一样，没有交集； 所有 表的 并集是全量数据； 场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。 分析：表的数据量少了，单次SQL执行效率高，自然减轻了CPU的负担。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://hofe.work/tags/MySQL/"}]},{"title":"Redis学习笔记进阶篇","slug":"数据库/通过面试题学Redis进阶篇","date":"2020-05-05T03:49:33.000Z","updated":"2020-05-30T13:16:57.831Z","comments":true,"path":"2020/05/05/数据库/通过面试题学Redis进阶篇/","link":"","permalink":"http://hofe.work/2020/05/05/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%80%9A%E8%BF%87%E9%9D%A2%E8%AF%95%E9%A2%98%E5%AD%A6Redis%E8%BF%9B%E9%98%B6%E7%AF%87/","excerpt":"","text":"欢迎来我的个人网站，里面有最新的版本 上篇篇介绍了下单体Redis的知识，这篇介绍分布式集群下的Redis。 还未全部完成，遗留集群和异步更新策略还未解决 面试题上篇博客解决了以下问题： Redis是什么，用在哪？Redis 的缺点？ Redis常见数据类型 用在什么场景 底层数据结构是啥 Zset底层为什么要用两个数据结构 Redis的持久化 说一下 Redis 的数据淘汰策略 Redis和MySql的区别？ redis为什么不能代替mysql? redis能存大量的数据呢为什么不能？说到了事务 Redis和memcached有什么区别？ 这篇博客主要解决以下问题 Redis的并发竞争问题如何解决？Redis的缓存穿透，缓存雪崩，缓存击穿？怎么解决？怎么保证缓存和数据库数据的一致性？Redis集群！集群是如何判断是否有某个节点挂掉？集群进入fail状态的必要条件？Redis哨兵Redis主从复制和一致性保证！Redis热key问题 一、redis缓存与数据库一致性在高并发的业务场景下，数据库大多数情况都是用户并发访问最薄弱的环节。所以，就需要使用redis做一个缓冲操作，让请求先访问到redis，而不是直接访问MySQL等数据库。 1.1 关于缓存加入缓存后，客户端发来的请求一般经过以下步骤 从缓存中读数据会出现以下情况： 读数据的时候首先去缓存里读 没有读到再去MySQL里读 读回来之后更新到缓存 下一次从缓存中获取数据 1.2 更新操作带来的一致性问题写数据的时候会产生数据不一致的问题，无论是先写到Redis里再写MySQL还是先写MySQL再写Redis，这两步写操作不能保证原子性，所以会出现Redis和MySQL里的数据不一致。 数据库中数据更新了，显然缓存也是要更新的，那么并发场景下该先写数据库还是先更新缓存？ 1.2.1 先更新数据库，然后再删除缓存到我们最常用的方案了，但是也会导致一致性问题，不过产生脏数据比较少。 我们假设有两个请求，请求A是读请求，请求B是写请求，那么可能会出现下述情形： 请求B更新数据库 请求A从缓存中读出旧数据 请求B删除缓存 期间只有请求B更新数据库，还没来得及删除缓存这段时间内会有脏数据，导致数据不一致。但是后面更新操作完成后，立马将缓存删除了，在后面的读请求获取到的就是新的数据了。 1.2.2 先删除缓存，然后再更新数据库这个方案的问题很明显，假设现在并发两个请求，一个是写请求A，一个是读请求B，那么可能出现如下的执行序列： 请求A删除缓存 请求B读取缓存，发现不存在，从数据库中读取到旧值 请求A将新值写入数据库 请求B将旧值写入缓存 这样就会导致缓存中存的还是旧值，在缓存过期之前都无法读到新值。这个问题在数据库读写分离的情况下会更明显，因为主从同步需要时间，请求B获取到的数据很可能还是旧值，那么写入缓存中的也会是旧值。这样会产生大量脏数据。 1.3 解决方案1.3.1 采用延时双删策略在写库前后都进行redis.del(key)操作，并且设定合理的超时时间。 伪代码如下 public void write(String key,Object data)&#123; redis.delKey(key); // 请求A删除缓存 // 请求B读取缓存，发现不存在，从数据库中读取到旧值 db.updateData(data); // 请求A将新值写入数据库 Thread.sleep(500); // 请求B将旧值写入缓存 redis.delKey(key); // 删除B写入的旧值 &#125; 1.具体步骤 先删除缓存 再写数据库 休眠500毫秒 再次删除缓存 2.确定休眠时间需要评估自己的项目的读数据业务逻辑的耗时。这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 当然这种策略还要考虑redis和数据库主从同步的耗时。最后的写数据的休眠时间则在读数据业务逻辑的耗时基础上，加几百ms即可。比如：休眠1秒。 3.设置缓存过期时间从理论上来说，给缓存设置过期时间，是保证最终一致性的解决方案。所有的写操作以数据库为准，只要到达缓存过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存。 4.该方案的弊端结合双删策略+缓存超时设置，这样最差的情况就是在超时时间内数据存在不一致，而且又增加了写请求的耗时。 1.3.2 异步更新缓存(基于订阅binlog的同步机制)1.技术整体思路：MySQL binlog增量订阅消费+消息队列+增量数据更新到redis 1）读Redis：热数据基本都在Redis 2）写MySQL:增删改都是操作MySQL 3）更新Redis数据：MySQ的数据操作binlog，来更新到Redis 2.Redis更新1）数据操作主要分为两大块： 一个是全量(将全部数据一次写入到redis) 一个是增量（实时更新） 这里说的是增量,指的是mysql的update、insert、delate变更数据。 2）读取binlog后分析 ，利用消息队列,推送更新各台的redis缓存数据。 这样一旦MySQL中产生了新的写入、更新、删除等操作，就可以把binlog相关的消息推送至Redis，Redis再根据binlog中的记录，对Redis进行更新。 其实这种机制，很类似MySQL的主从备份机制，因为MySQL的主备也是通过binlog来实现的数据一致性。 这里可以结合使用canal(阿里的一款开源框架)，通过该框架可以对MySQL的binlog进行订阅，而canal正是模仿了mysql的slave数据库的备份请求，使得Redis的数据更新达到了相同的效果。 当然，这里的消息推送工具你也可以采用别的第三方：kafka、rabbitMQ等来实现推送更新Redis。 看到这里我想大家还有点疑惑，感觉上比延时双删更浪费时间，以及需要等待的更久，并发场景下可行嘛？我的理解是多个线程的读或写操作都放进了队列，这样可以保证写库并且删了缓存之后再执行下一个线程的读操作，通过这种方式保证了操作的原子性，另外只要异步消息处理的足够快，那么是不会有问题的 二、缓存穿透、击穿与雪崩使用缓存的主要目是提升查询速度和减轻数据库压力。而缓存最常见的问题是缓存穿透、击穿和雪崩，在高并发下这三种情况都会有大量请求落到数据库，导致数据库资源占满，引起数据库故障。 2.1 缓存穿透在高并发下，查询一个缓存和数据库中都不存在的值时，缓存不会被命中，导致大量请求直接落到数据库上，这会给持久层数据库造成很大的压力。 2.1.1 解决方案 布隆过滤器 布隆过滤器是一种数据结构，用于数据量大的情况下代替hashmap判断某个元素是否存在，拦截对不存在数据的请求。 它是一个bit 向量或者说 bit 数组 对于要查找的值，将其用多个不同的hash函数映射到这个bit数组中。如hash1(baidu)=1，hash2(baidu)=4，hash3(baidu)=8，将这些位置的元素置为1 这时，如果查询“ten”是否存在，哈希函数返回了 1、5、8三个值，发现5这个位置没有被置为1，那么这个元素肯定不存在。 将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被 这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 缓存空对象 当存储层不命中后，即使返回的空对象也将其缓存起来，同时会设置一个过期时间，之后再访问这个数据将会从缓存中获取，保护了后端数据源 但是这种方法会存在两个问题： 如果空值能够被缓存起来，这就意味着缓存需要更多的空间存储更多的键，因为这当中可能会有很多的空值的键； 即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响。 在接口层增加校验，比如用户鉴权，参数做校验，不合法的校验直接 return，比如 id 做基础校验，id&lt;=0 直接拦截 2.2 缓存击穿（热点key）一个 Key 非常热点，在不停地扛着大量的请求，大并发集中对这一个点进行访问，当这个 Key 在失效的瞬间，持续的大并发直接落到了数据库上，就在这个 Key 的点上击穿了缓存。 2.2.1 解决方案 使用锁，单机用synchronized,lock等，分布式用分布式锁。 缓存过期时间不设置，而是设置在key对应的value里。如果检测到存的时间超过过期时间则异步更新缓存。 在value设置一个比过期时间t0小的过期时间值t1，当t1过期的时候，延长t1并做更新缓存操作。 设置标签缓存，标签缓存设置过期时间，标签缓存过期后，需异步地更新实际缓存 2.3 缓存雪崩在高并发下，大量的缓存key在同一时间失效，导致大量的请求落到数据库上。 2.2.1 解决方案 redis高可用 这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续工作，其实就是搭建的集群。 限流降级 这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 数据预热 数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。(批量往 Redis 存数据的时候，把每个 Key 的失效时间都加个随机值就好了，这样可以保证数据不会再同一时间大面积失效。) 三、复制3.1 主从复制 一个Redis服务可以有多个该服务的复制品，这个Redis服务称为Master，其它复制称为Slaves 主库只负责写数据，每次有数据更新都将更新的数据同步到它所有的从库，而从库只负责读数据。这样一来，就有了两个好处： 读写分离，不仅可以提高服务器的负载能力，并且可以根据读请求的规模自由增加或者减少从库的数量。 容灾恢复，数据被复制成了了好几份，就算有一台机器出现故障，也可以使用其他机器的数据快速恢复。 Redis的主从结构可以采用一主多从或者级联结构，Redis主从复制可以根据是否是全量分为全量同步和增量同步。下图为级联结构。 3.1.1 全量复制Redis全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份。具体步骤如下： 从服务器连接主服务器，发送SYNC命令； 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后执行的所有写命令； 主服务器BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令； 从服务器收到快照文件后丢弃所有旧数据，载入收到的快照； 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令； 完成上面几个步骤后就完成了从服务器数据初始化的所有操作，从服务器此时可以接收来自用户的读请求。 3.1.2 增量复制Redis增量复制是指Slave初始化后开始正常工作时主服务器发生的写操作同步到从服务器的过程。增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发送相同的写命令，从服务器接收并执行收到的写命令。 3.2 Redis主从同步策略主从刚刚连接的时候，进行全量同步；全量同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。 注意 从Redis 2.8开始，如果遭遇连接断开，重新连接之后可以从中断处继续进行复制，而不必重新同步。 部分同步的实现依赖于在master服务器内存中给每个slave服务器维护了一份同步日志offest和同步标识，每个slave服务器在跟master服务器进行同步时都会携带自己的同步标识和上次同步的最后位置offest。当主从连接断掉之后，slave服务器隔断时间内（默认1s）主动尝试和master服务器进行连接，如果从服务器携带的偏移量标识还在master服务器上的同步备份日志中，那么就从slave发送的偏移量开始继续上次的同步操作，如果slave发送的偏移量已经不再master的同步备份日志中（可能由于主从之间断掉的时间比较长或者在断掉的短暂时间内master服务器接收到大量的写操作），则必须进行一次全量更新。在部分同步过程中，master会将本地记录的同步备份日志中记录的指令依次发送给slave服务器从而达到数据一致。 3.3 Redis主从复制特点 采用异步复制； 一个主redis可以含有多个从redis；每个从redis可以接收来自其他从redis服务器的连接； 主从复制对于主redis服务器来说是非阻塞的，这意味着当从服务器在进行主从复制同步过程中，主redis仍然可以处理外界的访问请求； 主从复制对于从redis服务器来说也是非阻塞的，这意味着，即使从redis在进行主从复制过程中也可以接受外界的查询请求，只不过这时候从redis返回的是以前老的数据，如果你不想这样，那么在启动redis时，可以在配置文件中进行设置，那么从redis在复制同步过程中来自外界的查询请求都会返回错误给客户端； 主从复制提高了redis服务的扩展性，避免单个redis服务器的读写访问压力过大的问题，同时也可以给为数据备份及冗余提供一种解决方案； 四、Sentinel4.1 哨兵模式–Redis提供的高可用方案主从模式的弊端就是不具备高可用性，当master挂掉以后，Redis将不能再对外提供写入操作，因此sentinel应运而生。sentinel中文含义为哨兵，它的作用就是监控redis集群的运行状况。有多个sentinel实例组成的sentinel系统可以监视多个主服务器以及下属的从服务器，并在主服务器挂掉之后，升级从服务器为主服务器，由新的主服务器代替已下线的主服务器继续处理命令请求。 工作机制： 每个sentinel以每秒钟一次的频率向它所知的master，slave以及其他sentinel实例发送一个 PING 命令 如果一个实例距离最后一次有效回复 PING 命令的时间超过 down-after-milliseconds 选项所指定的值， 则这个实例会被sentinel标记为主观下线。 如果一个master被标记为主观下线，则正在监视这个master的所有sentinel要以每秒一次的频率确认master的确进入了主观下线状态 当有足够数量的sentinel（大于等于配置文件指定的值）在指定的时间范围内确认master的确进入了主观下线状态， 则master会被标记为客观下线 在一般情况下， 每个sentinel会以每 10 秒一次的频率向它已知的所有master，slave发送 INFO 命令 当master被sentinel标记为客观下线时，sentinel向下线的master的所有slave发送 INFO 命令的频率会从 10 秒一次改为 1 秒一次 若没有足够数量的sentinel同意master已经下线，master的客观下线状态就会被移除； 若master重新向sentinel的 PING 命令返回有效回复，master的主观下线状态就会被移除 当使用sentinel模式的时候，客户端就不要直接连接Redis，而是连接sentinel的ip和port，由sentinel来提供具体的可提供服务的Redis实现，这样当master节点挂掉以后，sentinel就会感知并将新的master节点提供给使用者。 特点： sentinel模式是建立在主从模式的基础上，如果只有一个Redis节点，sentinel就没有任何意义 当master挂了以后，sentinel会在slave中选择一个做为master，并修改它们的配置文件，其他slave的配置文件也会被修改，比如slaveof属性会指向新的master 当master重新启动后，它将不再是master而是做为slave接收新的master的同步数据 sentinel因为也是一个进程有挂掉的可能，所以sentinel也会启动多个形成一个sentinel集群 多sentinel配置的时候，sentinel之间也会自动监控 当主从模式配置密码时，sentinel也会同步将配置信息修改到配置文件中，不需要担心 一个sentinel或sentinel集群可以管理多个主从Redis，多个sentinel也可以监控同一个redis sentinel最好不要和Redis部署在同一台机器，不然Redis的服务器挂了以后，sentinel也挂了 缺点： sentinel模式基本可以满足一般生产的需求，具备高可用性。但无法满足数据量大的场景。 五、集群5.1 分布式数据库方案所谓的集群，就是通过添加服务器的数量，提供相同的服务，从而让服务器达到一个稳定、高效的状态。集群通过分片（sharding）来进行数据共享，并提供复制和故障转移功能。 可参考：https://www.jianshu.com/p/161e66611fe9 5.3 集群模式5.3.1 主从模式在主从复制中，数据库分为两类：主数据库(master)和从数据库(slave) 工作机制： 当slave启动后，主动向master发送SYNC命令。 master接收到SYNC命令后在后台保存快照（RDB持久化）和缓存保存快照这段时间的命令，然后将保存的快照文件和缓存的命令发送给slave。 slave接收到快照文件和命令后加载快照文件和缓存的执行命令。 复制初始化后，master每次接收到的写命令都会同步发送给slave，保证主从数据一致性 主从复制特点： 主数据库可以进行读写操作，当读写操作导致数据变化时会自动将数据同步给从数据库 从数据库一般都是只读的，并且接收主数据库同步过来的数据 一个master可以拥有多个slave，但是一个slave只能对应一个master slave挂了不影响其他slave的读和master的读和写，重新启动后会将数据从master同步过来 master挂了以后，不影响slave的读，但redis不再提供写服务，master重启后redis将重新对外提供写服务 master挂了以后，不会在slave节点中重新选一个master 缺点： 从上面可以看出，master节点在主从模式中唯一，若master挂掉，则redis无法对外提供写服务。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://hofe.work/tags/Redis/"}]},{"title":"贪心算法题目总结","slug":"数据结构与算法/贪心算法题目总结","date":"2020-05-04T03:25:11.000Z","updated":"2020-06-06T15:47:25.946Z","comments":true,"path":"2020/05/04/数据结构与算法/贪心算法题目总结/","link":"","permalink":"http://hofe.work/2020/05/04/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/","excerpt":"","text":"1、买卖股票最大收益 给定一个数组，它的第 i 个元素是一支给定股票第 i 天的价格。 如果你最多只允许完成一笔交易（即买入和卖出一支股票一次），设计一个算法来计算你所能获取的最大利润。 注意：你不能在买入股票前卖出股票。 示例： 输入: [7,1,5,3,6,4]输出: 5解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock class Solution &#123; //只要记录前面的最小价格，将这个最小价格作为买入价格，然后将当前的价格作为售出价格，查看当前收益是不是最大收益。 public int maxProfit(int[] prices) &#123; if(prices.length == 0)&#123; return 0; &#125; int min = prices[0]; int max = 0; for(int i = 1; i &lt; prices.length; i++)&#123; if(prices[i] &lt; min)&#123; min = prices[i]; &#125;else&#123; max = Math.max(max, prices[i] - min); &#125; &#125; return max; &#125;&#125; 2、买卖股票最大收益（可多次买卖） Leetcode / 力扣 题目描述：可以进行多次交易，多次交易之间不能交叉进行，可以进行多次交易。 贪心：对于 [a, b, c, d]，如果有 a &lt;= b &lt;= c &lt;= d ，那么最大收益为 d - a。而 d - a = (d - c) + (c - b) + (b - a) ，因此当访问到一个 prices[i] 且 prices[i] - prices[i-1] &gt; 0，那么就把 prices[i] - prices[i-1] 添加到收益中。 class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length == 0) return 0; int profit = 0; for(int i = 1; i &lt; prices.length; i++)&#123; profit += Math.max(prices[i] - prices[i-1],0); &#125; return profit; &#125;&#125; 动态规划：第i天只有两种状态，不持有或持有股票，当天不持有股票的状态可能来自昨天卖出或者昨天也不持有，同理，当天持有股票的状态可能来自昨天买入或者昨天也持有中，取最后一天的不持有股票状态就是问题的解 class Solution &#123; public int maxProfit(int[] prices) &#123; if(prices.length == 0) return 0; int dp[][] = new int[prices.length][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for(int i = 1; i &lt; prices.length; i++)&#123; dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1]+prices[i]); dp[i][1] = Math.max(dp[i-1][1], dp[i-1][0]-prices[i]); &#125; return dp[prices.length - 1][0]; &#125;&#125; 3、最大连续子数组求和 Leetcode / 力扣 给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 class Solution &#123; public int maxSubArray(int[] nums) &#123; int presum[] = new int[nums.length]; presum[0] = nums[0]; int max = presum[0]; for(int i = 1; i &lt; nums.length; i++)&#123; presum[i] = presum[i-1] &gt; 0 ? presum[i-1]+nums[i] : nums[i]; max = Math.max(presum[i], max); &#125; return max; &#125;&#125;// 可以用presum代替presum进行优化，这样好理解写","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"贪心","slug":"贪心","permalink":"http://hofe.work/tags/%E8%B4%AA%E5%BF%83/"}]},{"title":"栈、队列题目总结","slug":"数据结构与算法/栈、队列题目总结","date":"2020-05-04T03:25:11.000Z","updated":"2020-06-06T15:48:14.301Z","comments":true,"path":"2020/05/04/数据结构与算法/栈、队列题目总结/","link":"","permalink":"http://hofe.work/2020/05/04/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E6%A0%88%E3%80%81%E9%98%9F%E5%88%97%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/","excerpt":"","text":"包含min函数的栈 用辅助栈保存栈中每个值对应状态下的最小值。 需要注意的是Java的Stack类 push( num) //入栈 pop() //栈顶元素出栈 empty() //判定栈是否为空 peek() //获取栈顶元素 search(num) //判端元素num是否在栈中，如果在返回1，不在返回-1 import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); Stack&lt;Integer&gt; minstack = new Stack&lt;&gt;(); public void push(int node) &#123; stack.push(node); int m; if(minstack.size() == 0)&#123; m = node; &#125;else&#123; m = minstack.peek(); // java的stack原来没有top方法而是peek？？ &#125; if(m &lt; node)&#123; minstack.push(m); &#125;else&#123; minstack.push(node); &#125; &#125; public void pop() &#123; stack.pop(); minstack.pop(); &#125; public int top() &#123; return stack.peek(); &#125; public int min() &#123; return minstack.peek(); &#125;&#125; 栈的压入、弹出序列 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。 import java.util.*;public class Solution &#123; public boolean IsPopOrder(int [] pushA,int [] popA) &#123; if(pushA.length == 0 || popA.length == 0)&#123; return false; &#125; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); int j = 0; for(int i = 0; i &lt; pushA.length;i++)&#123; stack.push(pushA[i]); while(!stack.isEmpty() &amp;&amp; stack.peek() == popA[j])&#123; // stack.isEmpty()判断语句需在前 stack.pop(); j++; &#125; &#125; if(j == popA.length)&#123; return true; &#125;else&#123; return false; &#125; &#125;&#125; 两个栈实现队列 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 用一个辅助栈。当调用pop方法的时候也就是需要栈最底下的数，那就将其上的数都pop到辅助栈中直到空位置，这时候pop辅助栈的最顶上元素即可；push元素需要跟在栈最顶上元素之后，故把辅助栈的元素重新pop回到原栈中，这时候在push进栈中。 import java.util.Stack;public class Solution &#123; Stack&lt;Integer&gt; stack1 = new Stack&lt;Integer&gt;(); Stack&lt;Integer&gt; stack2 = new Stack&lt;Integer&gt;(); public void push(int node) &#123; while(!stack2.isEmpty())&#123; stack1.push(stack2.pop()); &#125; stack1.push(node); &#125; public int pop() &#123; while(!stack1.isEmpty())&#123; stack2.push(stack1.pop()); &#125; return stack2.pop(); &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"栈","slug":"栈","permalink":"http://hofe.work/tags/%E6%A0%88/"},{"name":"队列","slug":"队列","permalink":"http://hofe.work/tags/%E9%98%9F%E5%88%97/"}]},{"title":"Redis学习笔记基础篇","slug":"数据库/通过面试题学Redis基础篇","date":"2020-05-04T02:42:33.000Z","updated":"2020-05-30T09:37:57.827Z","comments":true,"path":"2020/05/04/数据库/通过面试题学Redis基础篇/","link":"","permalink":"http://hofe.work/2020/05/04/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%80%9A%E8%BF%87%E9%9D%A2%E8%AF%95%E9%A2%98%E5%AD%A6Redis%E5%9F%BA%E7%A1%80%E7%AF%87/","excerpt":"","text":"这篇介绍了下单体Redis的知识，还会再写一篇介绍分布式集群下的Redis。 参考：https://blog.nowcoder.net/n/f62aef5a98804489a7e7bd27cfd7b542 ​ https://www.cnblogs.com/ysocean/p/9080942.html 重点这篇博客会解决以下问题： Redis是什么，用在哪？Redis 的缺点？ Redis常见数据类型 用在什么场景 底层数据结构是啥 Zset底层为什么要用两个数据结构 Redis的持久化 说一下 Redis 的数据淘汰策略 Redis和MySql的区别？ redis为什么不能代替mysql? redis能存大量的数据呢为什么不能？说到了事务 Redis和memcached有什么区别？ 以下内容放在另外一篇博客 Redis的并发竞争问题如何解决？Redis的缓存穿透，缓存雪崩，缓存击穿？怎么解决？怎么保证缓存和数据库数据的一致性？Redis集群！集群是如何判断是否有某个节点挂掉？集群进入fail状态的必要条件？Redis哨兵Redis主从复制和一致性保证！Redis热key问题 一、Redis简介1.1 Redis是什么Redis 是完全开源免费的，遵守 BSD 协议，是一个高性能的 key-value 数据库，基于内存，可持久化。 它支持符串，哈希，列表，集合等多种数据结构，应用于缓存、消息队列等多种场景。 Redis 与其他 key - value 缓存产品有以下三个特点： （1）Redis 支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 （2）Redis 不仅仅支持简单的 key-value 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。 （3）Redis 支持数据的备份，即 master-slave 模式的数据备份。 Redis 优势 （1）性能极高 – Redis 能读的速度是 110000 次/s,写的速度是 81000 次/s 。 （2）丰富的数据类型 – Redis 支持二进制案例的 Strings, Lists, Hashes, Sets 及Ordered Sets 数据类型操作。 （3）原子 – Redis 的所有操作都是原子性的，意思就是要么成功执行要么失败完全不执行。单个操作是原子性的。多个操作也支持事务，即原子性，通过 MULTI 和 EXEC指令包起来。 （4）丰富的特性 – Redis 还支持 publish/subscribe, 通知, key 过期等等特性。 Redis 与其他 key-value 存储有什么不同？ （1）Redis 有着更为复杂的数据结构并且提供对他们的原子性操作，这是一个不同于其他数据库的进化路径。Redis 的数据类型都是基于基本数据结构的同时对程序员透明，无需进行额外的抽象。 （2）Redis 运行在内存中但是可以持久化到磁盘，所以在对不同数据集进行高速读写时需要权衡内存，因为数据量不能大于硬件内存。在内存数据库方面的另一个优点是，相比在磁盘上相同的复杂的数据结构，在内存中操作起来非常简单，这样 Redis可以做很多内部复杂性很强的事情。同时，在磁盘格式方面他们是紧凑的以追加的方式产生的，因为他们并不需要进行随机访问。 1.2 常见数据类型了解下Redis内部内存管理中是如何描述这些不同数据类型的 首先Redis内部使用一个redisObject对象来表示所有的key和value,redisObject最主要的信息如上图所示： type代表一个value对象具体是何种数据类型， encoding是不同数据类型在redis内部的存储方式 redis支持五种数据类型作为其Value，redis的Key都是字符串类型的。Redis支持string（字符串），hash（哈希），list（列表），set（集合）和zset（sorted set有序集合） String 类型是二进制安全的，在传输数据时，保证二进制数据的信息安全，也就是不被篡改、破译等，如果被攻击，能够及时检测出来。 实现方式: String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。 应用场景： 常用于保存单个字符串或JSON字符串数据 因String是二进制安全的，可以把一个图片文件的内容作为字符串来存储 计数器（常规key-value缓存应用。常规计数: 微博数, 粉丝数） INCR等指令本身就具有原子操作的特性，所以我们完全可以利用redis的INCR、INCRBY、DECR、DECRBY等指令来实现原子计数的效果。实现业务上的统计计数需求。 常用命令: set,get,decr,incr,mget 等 Hash 类是一个string类型的field和value的映射表，hash特别适合用于存储对象。Redis 中每个 hash 可以存储 2^32^ - 1 键值对（40多亿），可以看成具有KEY和VALUE的MAP容器，该类型非常适合于存储值对象的信息， 如：uname,upass,age等 实现方式： 上面已经说到Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。 应用场景： 常用于存储一个对象 常用命令：hget,hset,hgetall 等 为什么不用String存对象？ hash是最接近关系数据库结构的数据类型，可以将数据库一条记录或程序中一个对象转换成hashmap存放在redis中。用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有以下2种存储方式：第一种方式将用户ID作为查找key,把其他信息封装成一个对象以序列化的方式存储，这种方式的缺点是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。 第二种方法是这个用户信息对象有多少成员就存成多少个key-value对儿，用用户ID+对应属性的名称作为唯一标识来取得对应属性的值，虽然省去了序列化开销和并发问题，但是用户ID为重复存储，如果存在大量这样的数据，内存还是挺浪费的。 List Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）一个列表最多可以包含 2^32^ - 1 个元素 (4294967295, 每个列表超过40亿个元素) 类似JAVA中的LinkedList。 实现方式： Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。 应用场景 对数据量大的集合数据删减 列表数据显示、关注列表、粉丝列表、留言评价等…分页、热点新闻（Top5)等 利用LRANGE还可以很方便的实现分页的功能，在博客系统中，每片博文的评论也可以存入一个单独的list中。 任务队列 list通常用来实现一个消息队列，而且可以确保先后顺序。 常用命令：lpush,rpush,lpop,rpop,lrange等。 Set Redis 的 Set 是 String 类型的无序集合。 集合成员是唯一的，这就意味着集合中不能出现重复的数据。Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。集合中最大的成员数为 2^32^ - 1 。类似于JAVA中的 Hashtable集合set的底层存储结构使用了intset和hashtable两种数据结构存储的，intset我们可以理解为数组，hashtable就是普通的哈希表（key为set的值，value为null）。intset内部其实是一个数组（int8_t coentents[]数组），存储数据的时候是有序的，因为在查找数据的时候是通过二分查找来实现的。 实现方式： set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。 应用场景 对两个集合间的数据计算进行交集、并集、差集运算 以非常方便的实现如共同关注、共同喜好、二度好友等功能。 利用唯一性，可以统计访问网站的所有独立 IP 常用命令：sadd,spop,smembers,sunion 等 Zset有序集合(sorted set) 1、 Redis 有序集合和集合一样也是string类型元素的集合,且不允许重复的成员。2、不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。3、Redis的ZSet是有序、且不重复 实现方式： Redis sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。 应用场景 排行榜：成绩排行 带权重的队列，让重要的任务优先执行。 常用命令：zadd,zrange,zrem,zcard等 二、底层数据结构参考https://segmentfault.com/a/1190000020770894?utm_source=tag-newest 2.1 简单动态字符串（SDS）struct sdshdr&#123; //等于 SDS 保存字符串的长度 int len; //记录 buf 数组中未使用字节的数量 int free; //字节数组，用于保存字符串 char buf[];&#125; 不使用C语言字符串实现，而是使用 SDS的好处 常数复杂度获取字符串长度 由于 len 属性的存在，我们获取 SDS 字符串的长度只需要读取 len 属性，时间复杂度为 O(1)。而对于 C 语言，获取字符串的长度通常是经过遍历计数来实现的，时间复杂度为 O(n)。通过strlen key 命令可以获取 key 的字符串长度。 杜绝缓冲区溢出 我们知道在 C 语言中使用 strcat 函数来进行两个字符串的拼接，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。而对于 SDS 数据类型，在进行字符修改的时候，会首先根据记录的 len 属性检查内存空间是否满足需求，如果不满足，会进行相应的空间扩展，然后在进行修改操作，所以不会出现缓冲区溢出。 减少修改字符串的内存重新分配次数 C语言由于不记录字符串的长度，所以如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。 而对于SDS，由于len属性和free属性的存在，对于修改字符串SDS实现了空间预分配和惰性空间释放两种策略： 空间预分配：对字符串进行空间扩展的时候，扩展的内存比实际需要的多，这样可以减少连续执行字符串增长操作所需的内存重分配次数。 惰性空间释放：对字符串进行缩短操作时，程序不立即使用内存重新分配来回收缩短后多余的字节，而是使用 free 属性将这些字节的数量记录下来，等待后续使用。（当然SDS也提供了相应的API，当我们有需要时，也可以手动释放这些未使用的空间。） 二进制安全 因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此C字符串无法正确存取；而所有 SDS 的API 都是以处理二进制的方式来处理 buf 里面的元素，并且 SDS 不是以空字符串来判断是否结束，而是以 len 属性表示的长度来判断字符串是否结束。 兼容部分 C 字符串函数 虽然 SDS 是二进制安全的，但是一样遵从每个字符串都是以空字符串结尾的惯例，这样可以重用 C 语言库&lt;string.h&gt; 中的一部分函数。 2.2 双端链表typedef struct listNode&#123; //前置节点 struct listNode *prev; //后置节点 struct listNode *next; //节点的值 void *value; &#125;listNode typedef struct list&#123; //表头节点 listNode *head; //表尾节点 listNode *tail; //链表所包含的节点数量 unsigned long len; //节点值复制函数 void (*free) (void *ptr); //节点值释放函数 void (*free) (void *ptr); //节点值对比函数 int (*match) (void *ptr,void *key);&#125;list; Redis链表特性： 双端：链表具有前置节点和后置节点的引用，获取这两个节点时间复杂度都为O(1)。 无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL,对链表的访问都是以 NULL 结束。 带链表长度计数器：通过 len 属性获取链表长度的时间复杂度为 O(1)。 多态：链表节点使用 void* 指针来保存节点值，可以保存各种不同类型的值 2.3 字典字典又称为符号表或者关联数组、或映射（map），是一种用于保存键值对的抽象数据结构。字典中的每一个键 key 都是唯一的，通过 key 可以对值来进行查找或修改。C 语言中没有内置这种数据结构的实现，所以字典依然是 Redis自己构建的。 Redis 的字典使用哈希表作为底层实现 // 哈希表结构typedef struct dictht&#123; //哈希表数组 dictEntry **table; //哈希表大小 unsigned long size; //哈希表大小掩码，用于计算索引值 //总是等于 size-1 unsigned long sizemask; //该哈希表已有节点的数量 unsigned long used; &#125;dictht// 哈希表是由数组 table 组成，table 中每个元素都是指向 dict.h/dictEntry 结构，dictEntry 结构定义如下：typedef struct dictEntry&#123; //键 void *key; //值 union&#123; void *val; uint64_tu64; int64_ts64; &#125;v; //指向下一个哈希表节点，形成链表 struct dictEntry *next;&#125;dictEntry 哈希表最大的问题是存在哈希冲突，如何解决哈希冲突，有开放地址法和链地址法。这里采用的便是链地址法，通过next这个指针可以将多个哈希值相同的键值对连接在一起，用来解决哈希冲突。 2.4 跳跃表跳跃表（skiplist）是一种有序数据结构，它通过在每个节点中维持多个指向其它节点的指针，从而达到快速访问节点的目的。具有如下性质： 由很多层结构组成； 每一层都是一个有序的链表，排列顺序为由高层到底层，都至少包含两个链表节点，分别是前面的head节点和后面的nil节点； 最底层的链表包含了所有的元素； 如果一个元素出现在某一层的链表中，那么在该层之下的链表也全都会出现（上一层的元素是当前层的元素的子集）； 链表中的每个节点都包含两个指针，一个指向同一层的下一个链表节点，另一个指向下一层的同一个链表节点； Redis中跳跃表节点定义如下： typedef struct zskiplistNode &#123; //层 struct zskiplistLevel&#123; //前进指针 struct zskiplistNode *forward; //跨度 unsigned int span; &#125;level[]; //后退指针 struct zskiplistNode *backward; //分值 double score; //成员对象 robj *obj; &#125; zskiplistNode 多个跳跃表节点构成一个跳跃表： typedef struct zskiplist&#123; //表头节点和表尾节点 structz skiplistNode *header, *tail; //表中节点的数量 unsigned long length; //表中层数最大的节点的层数 int level; &#125;zskiplist; ①、搜索：从最高层的链表节点开始，如果比当前节点要大和比当前层的下一个节点要小，那么则往下找，也就是和当前层的下一层的节点的下一个节点进行比较，以此类推，一直找到最底层的最后一个节点，如果找到则返回，反之则返回空。 ②、插入：首先确定插入的层数，有一种方法是假设抛一枚硬币，如果是正面就累加，直到遇见反面为止，最后记录正面的次数作为插入的层数。当确定插入的层数k后，则需要将新元素插入到从底层到k层。 ③、删除：在各个层中找到包含指定值的节点，然后将节点从链表中删除即可，如果删除以后只剩下头尾两个节点，则删除这一层。 2.5 整数集合整数集合（intset）是Redis用于保存整数值的集合抽象数据类型，它可以保存类型为int16_t、int32_t 或者int64_t 的整数值，并且保证集合中不会出现重复元素。 定义如下： typedef struct intset&#123; //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[]; &#125;intset; 整数集合的每个元素都是 contents 数组的一个数据项，它们按照从小到大的顺序排列，并且不包含任何重复项。 length 属性记录了 contents 数组的大小。 需要注意的是虽然 contents 数组声明为 int8_t 类型，但是实际上contents 数组并不保存任何 int8_t 类型的值，其真正类型有 encoding 来决定。 升级 当我们新增的元素类型比原集合元素类型的长度要大时，需要对整数集合进行升级，才能将新元素放入整数集合中。具体步骤： 根据新元素类型，扩展整数集合底层数组的大小，并为新元素分配空间。 将底层数组现有的所有元素都转成与新元素相同类型的元素，并将转换后的元素放到正确的位置，放置过程中，维持整个元素顺序都是有序的。 将新元素添加到整数集合中（保证有序）。 升级能极大地节省内存。 降级 整数集合不支持降级操作，一旦对数组进行了升级，编码就会一直保持升级后的状态。 2.6 压缩列表压缩列表（ziplist）是Redis为了节省内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结构，一个压缩列表可以包含任意多个节点（entry），每个节点可以保存一个字节数组或者一个整数值。 压缩列表的原理：压缩列表并不是对数据利用某种算法进行压缩，而是将数据按照一定规则编码在一块连续的内存区域，目的是节省内存。 压缩列表的每个节点构成如下： previous_entry_ength：记录压缩列表前一个字节的长度。previous_entry_ength的长度可能是1个字节或者是5个字节，如果上一个节点的长度小于254，则该节点只需要一个字节就可以表示前一个节点的长度了，如果前一个节点的长度大于等于254，则previous length的第一个字节为254，后面用四个字节表示当前节点前一个节点的长度。利用此原理即当前节点位置减去上一个节点的长度即得到上一个节点的起始位置，压缩列表可以从尾部向头部遍历。这么做很有效地减少了内存的浪费。 encoding：节点的encoding保存的是节点的content的内容类型以及长度，encoding类型一共有两种，一种字节数组一种是整数，encoding区域长度为1字节、2字节或者5字节长。 content：content区域用于保存节点的内容，节点内容类型和长度由encoding决定。 三、持久化机制数据保存在内存中，高效但也容易发生丢失。于是需要一种持久化的机制，Redis提供了RDB(Redis DataBase)和AOF(Append Only File)。 3.1 数据持久化的过程（1）客户端向服务端发送写操作(数据在客户端的内存中)。 （2）数据库服务端接收到写请求的数据(数据在服务端的内存中)。 （3）服务端调用write方法，将数据从系统内存的缓冲区往磁盘缓存中写。 （4）磁盘控制器将数据写到磁盘的物理介质中(数据真正落到磁盘上)。 3.2 RDBRDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘。也是默认的持久化方式，这种方式是就是将内存中数据以快照的方式写入到二进制文件中,默认的文件名为dump.rdb。 既然RDB机制是通过把某个时刻的所有数据生成一个快照来保存，那么就应该有一种触发机制，是实现这个过程。对于RDB来说，提供了三种机制：save、bgsave、自动化 3.2.1 save触发方式该命令会阻塞当前Redis服务器，执行save命令期间，Redis不能处理其他命令，直到RDB过程完成为止。具体流程如下： 执行完成时候如果存在老的RDB文件，就把新的替代掉旧的。我们的客户端可能都是几万或者是几十万，这种方式显然不可取。 3.2.2 bgsave触发方式执行该命令时，Redis会在后台异步进行快照操作，快照同时还可以响应客户端请求。具体流程如下： 具体操作是Redis进程执行fork操作创建子进程，RDB持久化过程由子进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短。基本上 Redis 内部所有的RDB操作都是采用 bgsave 命令。 3.2.3 save与bgsave对比我们可以修改这些配置来实现我们想要的效果。因为第三种方式是配置的，所以我们对前两种进行一个对比： 3.2.4 自动触发自动触发是在配置文件中配置bgsave相关操作，比如触发条件、失败停止写入等来完成的。 在redis.conf配置文件中，可以设置： ①save：这里是用来配置触发 Redis的 RDB 持久化条件，也就是什么时候将内存中的数据保存到硬盘。比如“save m n”。表示m秒内数据集存在n次修改时，自动触发bgsave。 不需要持久化，那么你可以注释掉所有的 save 行来停用保存功能。 ②stop-writes-on-bgsave-error ：默认值为yes。当启用了RDB且最后一次后台保存数据失败，Redis是否停止接收数据。这会让用户意识到数据没有正确持久化到磁盘上，否则没有人会注意到灾难（disaster）发生了。如果Redis重启了，那么又可以重新开始接收数据了 ③rdbcompression ；默认值是yes。对于存储到磁盘中的快照，可以设置是否进行压缩存储。 ④rdbchecksum ：默认值是yes。在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。 ⑤dbfilename ：设置快照的文件名，默认是 dump.rdb ⑥dir：设置快照文件的存放路径，这个配置项一定是个目录，而不能是文件名。 3.2.5 RDB 的优势和劣势 优势 （1）RDB文件紧凑，全量备份，非常适合用于进行备份和灾难恢复。 （2）生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作。 （3）RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 劣势 当进行快照持久化时，会开启一个子进程专门负责快照持久化，子进程会拥有父进程的内存数据，父进程修改内存子进程不会反应出来，所以在快照持久化期间修改的数据不会被保存，可能丢失数据。 3.3 AOF全量备份总是耗时的，有时候我们提供一种更加高效的方式AOF，工作机制很简单，redis会将每一个收到的写命令都通过write函数追加到文件中。通俗的理解就是日志记录。 3.3.1 持久化原理他的原理看下面这张图： 每当有一个写命令过来时，就直接保存在我们的AOF文件中。 3.3.2 文件重写原理AOF的方式也同时带来了另一个问题。持久化文件会变的越来越大。为了压缩aof的持久化文件。redis提供了bgrewriteaof命令。将内存中的数据以命令的方式保存到临时文件中，同时会fork出一条新进程来将文件重写。 重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似。 3.3.3 AOF也有三种触发机制（1）always：同步持久化 每次发生数据变更会被立即记录到磁盘 性能较差但数据完整性比较好 （2）everysec：异步操作，每秒记录 如果一秒内宕机，有数据丢失 （3）no：从不同步 3.3.4 优点（1）AOF可以更好的保护数据不丢失，一般AOF会每隔1秒，通过一个后台线程执行一次fsync操作，最多丢失1秒钟的数据。 （2）AOF日志文件没有任何磁盘寻址的开销，写入性能非常高，文件不容易破损。 （3）AOF日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。 （4）AOF日志文件的命令通过可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝AOF文件，将最后一条flushall命令给删了，然后再将该AOF文件放回去，就可以通过恢复机制，自动恢复所有数据 3.3.5 缺点（1）对于同一份数据来说，AOF日志文件通常比RDB数据快照文件更大 （2）AOF开启后，支持的写QPS会比RDB支持的写QPS低，因为AOF一般会配置成每秒fsync一次日志文件，当然，每秒一次fsync，性能也还是很高的 （3）以前AOF发生过bug，就是通过AOF记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。 3.4 RDB和AOF到底该如何选择选择的话，两者加一起才更好。因为两个持久化机制你明白了，剩下的就是看自己的需求了，需求不同选择的也不一定，但是通常都是结合使用。有一张图可供总结： 四、数据淘汰策略减少内存紧张的情况，由此获取更为稳健的服务 4.1 lru、lfu（redis 4新增）、random、ttlLRU： （1）volatile-lru:从设置了过期时间的数据集中，选择最近最久未使用的数据释放； （2）allkeys-lru:从数据集中(包括设置过期时间以及未设置过期时间的数据集中)，选择最近最久未使用的数据释放； random： （3）volatile-random:从设置了过期时间的数据集中，随机选择一个数据进行释放； （4）allkeys-random:从数据集中随机选择一个数据进行释放 LFU： （5）volatile-lfu：从设置过期时间的数据集挑选使用频率最低的数据淘汰。 （6）allkeys-lfu：从数据集中挑选使用频率最低的数据淘汰。 （7）volatile-ttl：从设置了过期时间的数据集中，选择马上就要过期的数据进行释放操作； （8）no-eviction（默认策略）：不删除任意数据(但redis还会根据引用计数器进行释放),这时如果内存不够时，会直接返回错误。采用no-enviction策略可以保证数据不被丢失 4.2 淘汰机制的实现删除失效主键 既然是淘汰，那就需要把这些数据给删除，然后保存新的。Redis 删除失效键的方法主要有两种： （1）消极方法（passive way），在主键被访问时如果发现它已经失效，那么就删除它。redis在实现GET、MGET、HGET、LRANGE等所有涉及到读取数据的命令时都会调用 expireIfNeeded，它存在的意义就是在读取数据之前先检查一下它有没有失效，如果失效了就删除它。 （2）积极方法（active way），周期性地探测，发现失效就删除。消极方法的缺点是，如果key 迟迟不被访问，就会占用很多内存空间，所以才有积极方式。 （3）主动删除：当内存超过maxmemory限定时，触发主动清理策略，该策略由启动参数的配置决定 4.3 淘汰数据的量既然是淘汰数据，那么淘汰多少合适呢？ 为了避免频繁的触发淘汰策略，每次会淘汰掉一批数据，淘汰的数据的大小其实是和置换的大小来确定的，如果置换的数据量大，淘汰的肯定也多。 4.4 置换策略是如何工作理解置换策略的执行方式是非常重要的，比如： （1）客户端执行一条新命令，导致数据库需要增加数据（比如set key value） （2）Redis会检查内存使用，如果内存使用超过maxmemory，就会按照置换策略删除一些key （3）新的命令执行成功 五、Mutil5.1 redis事务特性我并没有把mutil称为事务，我更倾向于称作Redis的多任务命令。 事务需要具备ACID四个特性，redis提供的并不是严格的事务 不保证原子性 若在待执行队列中存在语法性错误,exec提交之后,其他正确命令也会被执行,这是单单的错误命令抛出异常。 Redis 开始事务 multi 命令后，Redis 会为这个事务生成一个队列，每次操作的命令都会按照顺序插入到这个队列中。 这个队列里面的命令不会被马上执行，直到 exec 命令提交事务，所有队列里面的命令会被一次性，并且排他的进行执行。 但原子性有一个特点就是要么全部成功，要么全部失败，也就是我们传统 DB 里面说的回滚。 可以发现，就算中间出现了失败，set abc x 这个操作也已经被执行了，并没有进行回滚，从严格的意义上来说 Redis 并不具备原子性。 没有隔离级别的概念 开启事务之后的操作全部是在待执行队列中缓存,并没有真正执行,也就不存在事务内部的查询要看到事务即将的更新,事务外部也不知道 5.2 watch操作语法:watch keywatch类似于乐观锁 如果在watch命令观测一个key之后,开启事务后修改该key.这个时候如果有其它连接修改了key,则会导致事务执行失败,在这个事务的其他操作也是失败exec之后,watch命令监控取消 在使用了watch之后可以保证一定的原子性和数据安全 六、Redis 发布订阅简介 Redis 发布订阅(pub/sub)是一种消息通信模式：发送者(pub)发送消息，订阅者(sub)接收消息。 Redis 客户端可以订阅任意数量的频道。 示例 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时，这个消息就会被发送给订阅它的三个客户端： 应用场景 这一功能最明显的用法就是构建实时消息系统，比如普通的即时聊天，群聊等功能 在一个博客网站中，有100个粉丝订阅了你，当你发布新文章，就可以推送消息给粉丝们。 七、面试题7.1 Redis为什么是单线程的？ 官方：因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）。 redis 核心就是 如果我的数据全都在内存里，我单线程的去操作 就是效率最高的，为什么呢，因为多线程的本质就是 CPU 模拟出来多个线程的情况，这种模拟出来的情况就有一个代价，就是上下文的切换，对于一个内存的系统来说，它没有上下文的切换就是效率最高的。redis 用 单个CPU 绑定一块内存的数据，然后针对这块内存的数据进行多次读写的时候，都是在一个CPU上完成的，所以它是单线程处理这个事。在内存的情况下，这个方案就是最佳方案 7.2 Redis为什么这么快官方提供的数据可以达到 100000+ 的 QPS 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路I/O复用模型，非阻塞IO，采用多路 I/O 复用技术可以让单个线程高效的处理多个连接请求； 使用底层模型不同，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 7.3 Redis应用场景因为Redis的性能十分优越，可以支持每秒十几万次的读/写操作，并且它还支持持久化、集群部署、分布式、主从同步等，Redis在高并发的场景下数据的安全和一致性，所以它经常用于这些场景： 经常要被查询，但是CUD操作频率低的数据；比如数据字典，确定了之后很少被修改，是可以放到缓存中的；还有热点数据，查询极为频繁的数据，放到Redis中可以减少MySQL的压力； 经常被查询，但是实时性要求不高数据，比如购物网站的热销排行榜，定时统计一次后把统计结果放到Redis中提供查询。 缓存还可以做数据共享（Session共享），在分布式的架构中，把用户的Session数据放到Redis中。 高并发场景下的计数器，比如秒杀，把商品库存数量放到Redis中（秒杀的场景会比较复杂，Redis只是其中之一，例如如果请求超过某个数量的时候，多余的请求就会被限流）； 因为Redis对高并发的支持和单线程机制，它也经常用作分布式锁； 7.4 Redis和MySql的区别？ 类型上 从类型上来说，mysql是关系型数据库，redis是缓存数据库 作用上 mysql用于持久化的存储数据到硬盘，功能强大，但是速度较慢 redis用于存储使用较为频繁的数据到缓存中，读取速度快 需求上 mysql和redis因为需求的不同，一般都是配合使用。 7.5 redis为什么不能代替mysql?在使用一项技术的时候，不是看它能不能，而是要看它适合不适合；而在大部分场景下，Redis是无法替代MySQL的。 MySQL是关系型数据库，数据储存在磁盘上，数据的格式是我们熟知的二维表格的样式。关系型数据库具有很多强大的功能；大部分都支持SQL语句查询，对事务也有很好的支持。 Redis被称作非关系型数据库，属于内存数据库，数据都储存在内存中（Redis有RDB持久化策略），Redis支持的数据类型也比较多，比如字符串，HASH，List等。 MySQL和Redis没有竞争的关系，通常当并发访问量比较大的时候，特别是读操作很多，架构中可以引入Redis，帮助提升架构的整体性能，减少Mysql(或其他关系型数据库)的压力； 不是MySQL or Redis；而是MySQL + Redis ； 7.6 Redis和memcached有什么区别？ 性能都比较高，性能对我们来说应该都不是瓶颈总体来讲，TPS方面redis和memcache差不多 操作的便利性memcache数据结构单一redis丰富一些，数据操作方面，redis更好一些 可靠性（持久化） 对于数据持久化和数据恢复， redis支持（快照、AOF）：依赖快照进行持久化，aof增强了可靠性的同时，对性能有所影响 memcache不支持，通常用在做缓存,提升性能； 数据一致性（事务支持） Memcache 在并发场景下，用cas保证一致性 redis事务支持比较弱，只能保证事务中的每个操作连续执行 存储方式上：Memcache 会把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。Redis 有部分数据存在硬盘上，这样能保证数据的持久性。 数据支持类型上：Memcache 对数据类型的支持简单，只支持简单的 key-value，，而 Redis 支持五种数据类型。 使用底层模型不同：它们之间底层实现方式以及与客户端之间通信的应用协议不一样。Redis 直接自己构建了 VM 机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 Value 的大小：Redis 可以达到 1GB，而 Memcache 只有 1MB。 7.7 redis为什么不能存大量的数据呢？","categories":[{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Redis","slug":"数据库/Redis","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://hofe.work/tags/Redis/"}]},{"title":"JVM内存模型与GC机制","slug":"Java/一文详解JVM","date":"2020-05-03T12:00:00.000Z","updated":"2020-06-04T14:16:23.616Z","comments":true,"path":"2020/05/03/Java/一文详解JVM/","link":"","permalink":"http://hofe.work/2020/05/03/Java/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3JVM/","excerpt":"","text":"可访问个人网站进行阅读最新版本 一、Java内存区域 1.程序计数器：程序计数器（Program Counter Register）是一块较小的内存空间，可以看作是当前线程所执行字节码的行号指示器，指向下一个将要执行的指令代码，由执行引擎来读取下一条指令。更确切的说，一个线程的执行，是通过字节码解释器改变当前线程的计数器的值，来获取下一条需要执行的字节码指令，从而确保线程的正确执行。每个线程都有一个独立的程序计数器，是线程私有的内存 2.虚拟机栈：描述Java方法执行的内存模型，每个方法在执行的同时都会创建一个栈帧（Stack Frame） 用于存储局部变量表、操作数栈、动态链接、方法出口等信息。每一个方法从调用直至执行完成 的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。 3.本地方法栈：本地方法栈则为 JVM 使用到的 Native 方法服务。Native 方法不是以 Java 语言实现的，而是以本地语言实现的（比如 C 或 C++）。个人理解Native 方法是与操作系统直接交互的。比如通知垃圾收集器进行垃圾回收的代码 System.gc()，就是使用 native 修饰的 4.堆：堆是Java虚拟机所管理的内存中最大的一块存储区域。堆内存被所有线程共享。主要存放使用new关键字创建的对象。所有对象实例以及数组都要在堆上分配。垃圾收集器就是根据GC算法，收集堆上对象所占用的内存空间（收集的是对象占用的空间而不是对象本身） Java堆分为年轻代（Young Generation）和老年代（Old Generation）；年轻代又分为伊甸园（Eden）和幸存区（Survivor区）；幸存区又分为From Survivor空间和 To Survivor空间。 年轻代存储“新生对象”，我们新创建的对象存储在年轻代中。当年轻内存占满后，会触发Minor GC，清理年轻代内存空间。 老年代存储长期存活的对象和大对象。年轻代中存储的对象，经过多次GC后仍然存活的对象会移动到老年代中进行存储。老年代空间占满后，会触发Full GC。 注：Full GC是清理整个堆空间，包括年轻代和老年代。如果Full GC之后，堆中仍然无法存储对象，就会抛出OutOfMemoryError异常。 5.方法区：方法区同 Java 堆一样是被所有线程共享的区间，用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码。更具体的说，静态变量+常量+类信息（版本、方法、字段等）+运行时常量池存在方法区中。常量池是方法区的一部分，常量池中存储编译器生成的各种字面量和符号引用。字面量就是Java中常量的意思。比如文本字符串，final修饰的常量等。方法引用则包括类和接口的全限定名，方法名和描述符，字段名和描述符等。 注：JDK1.8 使用元空间 MetaSpace 替代方法区，元空间并不在 JVM中，而是使用本地内存。元空间两个参数：MetaSpaceSize：初始化元空间大小，控制发生GC阈值MaxMetaspaceSize ： 限制元空间大小上限，防止异常占用过多物理内存 二、GC算法 2.1 确定垃圾算法2.1.1 引用计数法在 Java 中，引用和对象是有关联的。如果要操作对象则必须用引用进行。因此，很显然一个简单 的办法是通过引用计数来判断一个对象是否可以回收。简单说，给每个对象一个引用计数器，每当有一个地方引用它时，计数器就会加1；当引用失效时，计数器的值就会减1；任何时刻计数器的值为0的对象就是不可能再被使用的。 存在循环引用问题。 2.1.2 可达性分析通过一系列称为“GC Roots”的对象为起点，从这些节点开始向下搜索，搜索走过的路径称为引用链，当一个对象到GC Roots没有任何引用链相连（用图论的话来说，就是GC Roots到这个对象不可达）时，则说明此对象是不可用的。 要注意的是，不可达对象不等价于可回收对象，不可达对象变为可回收对象至少要经过两次标记过程。两次标记后仍然是可回收对象，则将面临回收。 那么那些对象可以作为GC Roots呢？以Java为例，有以下几种： 1、栈（栈帧中的本地变量表）中引用的对象。 2、方法区中的静态成员。 3、方法区中的常量引用的对象（全局变量）。 4、本地方法栈中JNI（一般说的Native方法）引用的对象。 注：第一和第四种都是指的方法的本地变量表，第二种表达的意思比较清晰，第三种主要指的是声明为final的常量值。 2.2 垃圾收集算法2.2.1 标记清除算法（mark-sweep）最基础的垃圾回收算法，分为两个阶段，标注和清除。标记阶段标记出所有需要回收的对象，清 除阶段回收被标记的对象所占用的空间缺陷：大的问题是内存碎片化严重，后续可能发生大对象不能找到可利用空间的问题 2.2.2 复制算法（coping）为了解决Mark-Sweep算法内存碎片化的缺陷而被提出的算法。按内存容量将内存划分为等大小 的两块。每次只使用其中一块，当这一块内存满后将尚存活的对象复制到另一块上去，把已使用的内存清掉。缺陷：这种算法虽然实现简单，内存效率高，不易产生碎片，但是大的问题是可用内存被压缩到了原本的一半。且存活对象增多的话，Copying算法的效率会大大降低 2.2.3 标记整理算法（mark-compact）结合了以上两个算法，为了避免缺陷而提出。标记阶段和Mark-Sweep算法相同，标记后不是清理对象，而是将存活对象移向内存的一端，然后清除端边界外的对象。 2.2.4 分代收集算法分代收集法是目前大部分JVM所采用的方法，其核心思想是根据对象存活的不同生命周期将内存划分为不同的域，一般情况下将GC堆划分为老生代(Tenured/Old Generation)和新生代(Young Generation)。老生代的特点是每次垃圾回收时只有少量对象需要被回收，新生代的特点是每次垃 圾回收时都有大量垃圾需要被回收，因此可以根据不同区域选择不同的算法 1.新生代是用来存放新生的对象。一般占据堆的1/3空间。由于频繁创建对象，所以新生代会频繁触发 MinorGC进行垃圾回收。新生代又分为 Eden区、ServivorFrom、ServivorTo三个区。 新生代-复制算法：目前大部分JVM的GC 对于新生代都采取复制算法，因为新生代中每次垃圾回收都要 回收大部分对象，即要复制的操作比较少，但通常并不是按照1：1来划分新生代。一般将新生代 划分为一块较大的Eden空间和两个较小的Survivor空间(From Space, To Space)，每次使用 Eden空间和其中的Survivor From空间，当进行回收时，将该两块空间中还存活的对象复制到Survivor To空间中 MinorGC的过程采用复制算法 当对象在Survivor区躲过一次GC 后，其年龄就会+1。默认情况下年龄到达15 的对象会被 移到老生代中 2.老年代老年代的对象比较稳定，所以 MajorGC 不会频繁执行。在进行 MajorGC 前一般都先进行 了一次 MinorGC，使得有新生代的对象晋身入老年代，导致空间不够用时才触发。当无法找到足 够大的连续空间分配给新创建的较大对象时也会提前触发一次MajorGC进行垃圾回收腾出空间。 老年代-标记整理算法：因为对象存活率高、没有额外空间对它进行分配担保, 就必须采用“标记—清理”或“标 记—整理”算法来进行回收, 不必进行内存复制, 且直接腾出空闲内存。 MajorGC 采用标记清除算法：首先扫描一次所有老年代，标记出存活的对象，然后回收没 有标记的对象。MajorGC的耗时比较长，因为要扫描再回收。MajorGC 会产生内存碎片，为了减 少内存损耗，我们一般需要进行合并或者标记出来方便下次直接分配。当老年代也满了装不下的 时候，就会抛出OOM（Out of Memory）异常。 2.2.5 分区收集算法分区算法则将整个堆空间划分为连续的不同小区间, 每个小区间独立使用, 独立回收. 这样做的 好处是可以控制一次回收多少个小区间 , 根据目标停顿时间, 每次合理地回收若干个小区间(而不是 整个堆), 从而减少一次GC 所产生的停顿。 三、垃圾回收器Java 堆内存被划分为新生代和年老代两部分，新生代主要使用复制和标记-清除垃圾回收算法； 年老代主要使用标记-整理垃圾回收算法，因此 java 虚拟中针对新生代和年老代分别提供了多种不 同的垃圾收集器，JDK1.6中Sun HotSpot虚拟机的垃圾收集器如下 3.1 新生代垃圾收集器3.1.1 Serial基本垃圾收集器，使用复制算法。Serial 是一个单线程的收集器，它不但只会使用一个 CPU 或一条线程去完成垃圾收集工 作，并且在进行垃圾收集的同时，必须暂停其他所有的工作线程，直到垃圾收集结束。 Serial 垃圾收集器虽然在收集垃圾过程中需要暂停所有其他的工作线程，但是它简单高效，对于限 定单个 CPU 环境来说，没有线程交互的开销，可以获得高的单线程垃圾收集效率，因此 Serial 垃圾收集器依然是java虚拟机运行在Client模式下默认的新生代垃圾收集器 3.1.2 ParNewParNew垃圾收集器其实是Serial收集器的多线程版本，也使用复制算法，除了使用多线程进行垃 圾收集之外，其余的行为和Serial收集器完全一样，ParNew垃圾收集器在垃圾收集过程中同样也 要暂停所有其他的工作线程。ParNew 收集器默认开启和 CPU 数目相同的线程数，可以通过-XX:ParallelGCThreads 参数来限 制垃圾收集器的线程数。 【Parallel：平行的】 ParNew虽然是除了多线程外和Serial收集器几乎完全一样，但是ParNew垃圾收集器是很多java 虚拟机运行在Server模式下新生代的默认垃圾收集器 3.1.3 Parallel ScavengeParallel Scavenge 收集器也是一个新生代垃圾收集器，同样使用复制算法，也是一个多线程的垃 圾收集器，它重点关注的是程序达到一个可控制的吞吐量（Thoughput，CPU 用于运行用户代码 的时间/CPU 总消耗时间，即吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)）， 高吞吐量可以高效率地利用 CPU 时间，尽快地完成程序的运算任务，主要适用于在后台运算而 不需要太多交互的任务。 自适应调节策略也是 ParallelScavenge 收集器与 ParNew 收集器的一个 重要区别 GC自适应调节策略：Parallel Scavenge收集器可设置-XX:+UseAdptiveSizePolicy参数。当开关打开时不需要手动指定新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX:SurvivorRation）、晋升老年代的对象年龄（-XX:PretenureSizeThreshold）等，虚拟机会根据系统的运行状况收集性能监控信息，动态设置这些参数以提供最优的停顿时间和最高的吞吐量，这种调节方式称为GC的自适应调节策略。 Parallel Scavenge收集器使用两个参数控制吞吐量： XX:MaxGCPauseMillis 控制最大的垃圾收集停顿时间 XX:GCRatio 直接设置吞吐量的大小。 3.2 老年代垃圾收集器3.2.1 Serial OldSerial Old 是 Serial 垃圾收集器年老代版本，它同样是个单线程的收集器，使用标记-整理算法， 这个收集器也主要是运行在Client默认的java虚拟机默认的年老代垃圾收集器。在Server模式下，主要有两个用途：（1）在JDK1.5之前版本中与新生代的Parallel Scavenge收集器搭配使用。（2）作为年老代中使用CMS收集器的后备垃圾收集方案。 新生代Serial与年老代Serial Old搭配垃圾收集过程图： （3）新生代Parallel Scavenge收集器与ParNew收集器工作原理类似，都是多线程的收集器，都使 用的是复制算法，在垃圾收集过程中都需要暂停所有的工作线程。新生代Parallel Scavenge/ParNew与年老代Serial Old搭配垃圾收集过程图： 3.2.2 Parallel OldParallel Old收集器是Parallel Scavenge的年老代版本，使用多线程的标记-整理算法，在JDK1.6 才开始提供。 在 JDK1.6 之前，新生代使用 Parallel Scavenge 收集器只能搭配年老代的 Serial Old 收集器，只 能保证新生代的吞吐量优先，无法保证整体的吞吐量，Parallel Old 正是为了在年老代同样提供吞 吐量优先的垃圾收集器，如果系统对吞吐量要求比较高，可以优先考虑新生代 Parallel Scavenge 和年老代Parallel Old收集器的搭配策略。 新生代Parallel Scavenge和年老代Parallel Old收集器搭配运行过程图： 3.2.3 (CMS)Concurrent mark sweep收集器是一种年老代垃圾收集器，其主要目标是获取短垃圾 回收停顿时间，和其他年老代使用标记-整理算法不同，它使用多线程的标记-清除算法。 短的垃圾收集停顿时间可以为交互比较高的程序提高用户体验。 CMS工作机制相比其他的垃圾收集器来说更复杂，整个过程分为以下4个阶段： 初始标记：只是标记一下GC Roots能直接关联的对象，速度很快，仍然需要暂停所有的工作线程。并发标记：进行GC Roots跟踪的过程，和用户线程一起工作，不需要暂停工作线程。重新标记：为了修正在并发标记期间，因用户程序继续运行而导致标记产生变动的那一部分对象的标记 记录，仍然需要暂停所有的工作线程。并发清除：清除GC Roots不可达对象，和用户线程一起工作，不需要暂停工作线程。由于耗时长的并 发标记和并发清除过程中，垃圾收集线程可以和用户现在一起并发工作，所以总体上来看 CMS收集器的内存回收和用户线程是一起并发地执行。 3.2.4 G1收集器G1收集器：通过把Java堆分成多个独立区域，回收时计算出每个区域回收所获得的空间以及所需时间的经验值，根据记录两个值来判断哪个区域最具有回收价值，所以叫Garbage First（垃圾优先） 初始标记（Initial Marking）：仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS 指针的值，让下一阶段用户线程并发运行时，能正确地在可用的Region中分配新对象。这个阶段需要 停顿线程，但耗时很短，而且是借用进行Minor GC的时候同步完成的，所以G1收集器在这个阶段实际 并没有额外的停顿。并发标记（Concurrent Marking）：从GC Root开始对堆中对象进行可达性分析，递归扫描整个堆 里的对象图，找出要回收的对象，这阶段耗时较长，但可与用户程序并发执行。当对象图扫描完成以 后，还要重新处理SATB记录下的在并发时有引用变动的对象。最终标记（Final Marking）：对用户线程做另一个短暂的暂停，用于处理并发阶段结束后仍遗留 下来的最后那少量的SATB记录。筛选回收（Live Data Counting and Evacuation）：负责更新Region的统计数据，对各个Region的回 收价值和成本进行排序，根据用户所期望的停顿时间来制定回收计划，可以自由选择任意多个Region 构成回收集，然后把决定回收的那一部分Region的存活对象复制到空的Region中，再清理掉整个旧 Region的全部空间。这里的操作涉及存活对象的移动，是必须暂停用户线程，由多条收集器线程并行 完成的。 TAMS： 程序要继续运行就肯定会持续有新对象被创建，G1为每一个Region设 计了两个名为TAMS（Top at Mark Start）的指针，把Region中的一部分空间划分出来用于并发回收过 程中的新对象分配，并发回收时新分配的对象地址都必须要在这两个指针位置以上 G1 收 集器两个突出的改进是：1.基于标记-整理算法，不产生内存碎片。2.可以非常精确控制停顿时间，在不牺牲吞吐量前提下，实现低停顿垃圾回收。 G1 收集器避免全区域垃圾收集，它把堆内存划分为大小固定的几个独立区域，并且跟踪这些区域 的垃圾收集进度，同时在后台维护一个优先级列表，每次根据所允许的收集时间，优先回收垃圾 多的区域。区域划分和优先级区域回收机制，确保 G1 收集器可以在有限时间获得高的垃圾收 集效率。 G1执行垃圾回收的处理方式与CMS相似. G1在全局标记阶段(global marking phase)并发执行, 以确定堆内存中哪些对象是存活的。标记阶段完成后,G1就可以知道哪些heap区的empty空间最大。它会首先回收这些区,通常会得到大量的自由空间. 被G1标记为适合回收的heap区将使用转移(evacuation)的方式进行垃圾回收. G1将一个或多个heap区域中的对象拷贝到其他的单个区域中,并在此过程中压缩和释放内存. 在多核CPU上转移是并行执行的(parallel on multi-processors), 这样能减少停顿时间并增加吞吐量. 因此,每次垃圾收集时, G1都会持续不断地减少碎片, 并且在用户给定的暂停时间内执行. 这比以前的方法强大了很多. CMS垃圾收集器(Concurrent Mark Sweep,并发标记清理)不进行压缩. ParallelOld 垃圾收集只对整个堆执行压缩,从而导致相当长的暂停时间。 需要强调的是, G1并不是一款实时垃圾收集器(real-time collector). 能以极高的概率在设定的目标暂停时间内完成,但不保证绝对在这个时间内完成。 基于以前收集的各种监控数据, G1会根据用户指定的目标时间来预估能回收多少个heap区. 因此,收集器有一个相当精确的heap区耗时计算模型,并根据该模型来确定在给定时间内去回收哪些heap区.","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://hofe.work/categories/Java/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"http://hofe.work/tags/JVM/"}]},{"title":"ConcurrentHashMap实现原理及源码阅读","slug":"Java/ConcurrentHashMap实现原理及源码阅读","date":"2020-05-03T02:47:53.000Z","updated":"2020-05-31T05:35:09.886Z","comments":true,"path":"2020/05/03/Java/ConcurrentHashMap实现原理及源码阅读/","link":"","permalink":"http://hofe.work/2020/05/03/Java/ConcurrentHashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"","text":"参考自 https://ddnd.cn/2019/03/10/jdk1-8-concurrenthashmap/ https://juejin.im/post/5c8276216fb9a049d51a4cd6 面试题并发安全机制分段锁机制synchronized + CAS内部数据结构 ConcurrentHashMap安全的原理ConcurrentHashMap是线程安全也就是说多线程下写操作不会发生数据不一致的问题，那么就以put方法举例。当要put(key)时，首先计算出要插入数组的哪个位置，如果该位置还没存值，则通过cas插入；如果存在值了，则将当前位置用synchronized锁住，并以拉链法的形式插入链表尾部。通过CAS和synchronized就保证了线程安全。 一、了解Hashtable 在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。 在讲ConcurrentHashMap之前，先讲下Hashtable，它使用 synchronized 关键字实现线程安全，比如 get 方法和 put 方法： public synchronized V get(Object key) &#123;&#125;public synchronized V put(K key, V value) &#123;&#125; 注意到，synchronized 关键字加在非静态方法上，说明同步锁对象即是 Hashtable 对象本身，只有一个锁。 1.1 Hashtable与ConcurrentHashMap区别线程安全的实现：Hashtable采用对象锁(synchronized修饰对象方法)来保证线程安全，也就是一个Hashtable对象只有一把锁，如果线程1拿了对象A的锁进行有synchronized修饰的put方法，其他线程是无法操作对象A中有synchronized修饰的方法的(如get方法、remove方法等)，竞争激烈所以效率低下。而ConcurrentHashMap采用CAS + synchronized来保证并发安全性，且synchronized关键字不是用在方法上而是用在了具体的对象上，实现了更小粒度的锁。 数据结构的实现：Hashtable采用的是数组 + 链表，当链表过长会影响查询效率，而ConcurrentHashMap采用数组 + 链表 + 红黑树，当链表长度超过某一个值，则将链表转成红黑树，提高查询效率。 二、底层数据结构jdk1.7中 ConcurrentHashMap 使用了锁分段技术 假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术 //每个Segment都是一个ReentrantLock锁，同时它内部保存着一个HashEntry数组final Segment&lt;K,V&gt;[] segments;static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; HashEntry(int hash, K key, V value, HashEntry&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; /** * Sets next field with volatile write semantics. (See above * about use of putOrderedObject.) * 设置next，注意unsafe的使用，ConcurrentHashMap中很多这种操作 */ final void setNext(HashEntry&lt;K,V&gt; n) &#123; UNSAFE.putOrderedObject(this, nextOffset, n); &#125; // Unsafe mechanics static final sun.misc.Unsafe UNSAFE; static final long nextOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class k = HashEntry.class; //计算 nextOffset ，以使用设置next nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(\"next\")); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; jdk1.7 在JDK1.7版本中，ConcurrentHashMap的数据结构是由一个Segment数组和多个HashEntry组成，主要实现原理是实现了锁分离的思路解决了多线程的安全问题，Segment在实现上继承了ReentrantLock，这样就自带了锁的功能。如下图所示： 一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 每个Segment元素 里包含一个HashEntry数组，每个HashEntry元素是一个链表结构的元素， 每个Segment守护着一个HashEntry数组里的元素，当对HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 jdk1.8 private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;private static final int DEFAULT_CAPACITY = 16;static final int TREEIFY_THRESHOLD = 8;static final int UNTREEIFY_THRESHOLD = 6;static final int MIN_TREEIFY_CAPACITY = 64;static final int MOVED = -1; // 表示正在转移static final int TREEBIN = -2; // 表示已经转换成树static final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hashtransient volatile Node&lt;K,V&gt;[] table;//默认没初始化的数组，用来保存元素private transient volatile Node&lt;K,V&gt;[] nextTable;//转移的时候用的数组/** * 用来控制表初始化和扩容的，默认值为0，当在初始化的时候指定了大小，这会将这个大小保存在sizeCtl中，大小为数组的0.75 * 当为负的时候，说明表正在初始化或扩张， * -1表示初始化 * -(1+n) n:表示活动的扩张线程 */private transient volatile int sizeCtl;/*Node是最核心的内部类，包装了key-value键值对，所有插入ConcurrentHashMap的数据都包装在这里面。 它与HashMap中的定义很相似，但是有一些差别它对value和next属性设置了volatile同步锁，它不允许调用setValue方法直接改变Node的value域，它增加了find方法辅助map.get()方法*/static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; //key的hash值 final K key; //key volatile V val; //value volatile Node&lt;K,V&gt; next; //表示链表中的下一个节点 Node(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; public final int hashCode() &#123; return key.hashCode() ^ val.hashCode(); &#125;&#125;/*树节点类，另外一个核心的数据结构。 当链表长度过长的时候，会转换为TreeNode。 但是与HashMap不相同的是，它并不是直接转换为红黑树，而是把这些结点包装成TreeNode放在TreeBin对象中，由TreeBin完成对红黑树的包装。 而且TreeNode在ConcurrentHashMap继承自Node类，而并非HashMap中的集成自LinkedHashMap.Entry*/static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125;&#125;// TreeBin 用作树的头结点，只存储root和first节点，不存储节点的key、value值。/*这个类并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。它代替了TreeNode的根节点，也就是说在实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象，这是与HashMap的区别*/static final class TreeBin&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; root; volatile TreeNode&lt;K,V&gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock&#125;// ForwardingNode在转移的时候放在头部的节点，是一个空节点/*一个用于连接两个table的节点类。它包含一个nextTable指针，用于指向下一张表。而且这个节点的key value next指针全部为null，它的hash值为-1. 这里面定义的find的方法是从nextTable里进行查询节点，而不是以自身为头节点进行查找*/static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125; JDK1.8的实现已经摒弃了Segment的概念，而是直接用Node数组+链表+红黑树的数据结构来实现，并发控制使用Synchronized和CAS来操作，整个看起来就像是优化过且线程安全的HashMap，虽然在JDK1.8中还能看到Segment的数据结构，但是已经简化了属性，只是为了兼容旧版本。 2.1 构造方法//无参构造函数，什么也不做，table的初始化放在了第一次插入数据时，默认容量大小是16和HashMap的一样，默认sizeCtl为0public ConcurrentHashMap() &#123;&#125;//传入容量大小的构造函数。public ConcurrentHashMap(int initialCapacity) &#123; //如果传入的容量大小小于0 则抛出异常。 if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //如果传入的容量大小大于允许的最大容量值 则cap取允许的容量最大值 否则cap = //((传入的容量大小 + 传入的容量大小无符号右移1位 + 1)的结果向上取最近的2幂次方)， //即如果传入的容量大小是12 则 cap = 32(12 + (12 &gt;&gt;&gt; 1) + 1=19 //向上取2的幂次方即32)，这里为啥一定要是2的幂次方，原因和HashMap的threshold一样，都是为 //了让位运算和取模运算的结果一样。 //MAXIMUM_CAPACITY即允许的最大容量值 为2^30。 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : //tableSizeFor这个函数即实现了将一个整数取2的幂次方。 tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); //将上面计算出的cap 赋值给sizeCtl，注意此时sizeCtl为正数，代表进行扩容的容量大小。 this.sizeCtl = cap;&#125;//包含指定Map的构造函数。//置sizeCtl为默认容量大小 即16。public ConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.sizeCtl = DEFAULT_CAPACITY; putAll(m);&#125;//传入容量大小和负载因子的构造函数。//默认并发数大小是1。public ConcurrentHashMap(int initialCapacity, float loadFactor) &#123; this(initialCapacity, loadFactor, 1);&#125;//传入容量大小、负载因子和并发数大小的构造函数public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0.0f) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); //如果传入的容量大小 小于 传入的并发数大小， //则容量大小取并发数大小，这样做的原因是确保每一个Node只会分配给一个线程，而一个线程则 //可以分配到多个Node，比如当容量大小为64，并发数大 //小为16时，则每个线程分配到4个Node。 if (initialCapacity &lt; concurrencyLevel) // Use at least as many bins initialCapacity = concurrencyLevel; // as estimated threads //size = 1.0 + (long)initialCapacity / loadFactor 这里计算方法和上面的构造函数不一样。 long size = (long)(1.0 + (long)initialCapacity / loadFactor); //如果size大于允许的最大容量值则 sizeCtl = 允许的最大容量值 否则 sizeCtl = //size取2的幂次方。 int cap = (size &gt;= (long)MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : tableSizeFor((int)size); this.sizeCtl = cap;&#125; ConcurrentHashMap的构造函数有5个，从数量上看就和HashMap、Hashtable(4个)的不同，多出的那个构造函数是public ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel)，即除了传入容量大小、负载因子之外还多传入了一个整型的concurrencyLevel，这个整型是我们预先估计的并发量，比如我们估计并发是30，那么就可以传入30。 其他的4个构造函数的参数和HashMap的一样，而具体的初始化过程却又不相同，HashMap和Hashtable传入的容量大小和负载因子都是为了计算出初始阈值(threshold)，而ConcurrentHashMap传入的容量大小和负载因子是为了计算出sizeCtl用于初始化table，这个sizeCtl即table数组的大小，不同的构造函数计算sizeCtl方法都不一样。 2.2 unSafe方法在ConcurrentHashMap中，大量使用了U.compareAndSwapXXX的方法，这个方法是利用一个CAS算法实现无锁化的修改值的操作，他可以大大降低锁代理的性能消耗。这个算法的基本思想就是不断地去比较当前内存中的变量值与你指定的一个变量值是否相等，如果相等，则接受你指定的修改的值，否则拒绝你的操作。因为当前线程中的值已经不是最新的值，你的修改很可能会覆盖掉其他线程修改的结果。这一点与乐观锁，SVN的思想是比较类似的。unsafe代码块控制了一些属性的修改工作，比如最常用的SIZECTL 。 在这一版本的concurrentHashMap中，大量应用来的CAS方法进行变量、属性的修改工作。 利用CAS进行无锁操作，可以大大提高性能。 /* * 用来返回节点数组的指定位置的节点的原子操作 */ @SuppressWarnings(\"unchecked\") static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE); &#125; /* * cas原子操作，在指定位置设定值 */ static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125; /* * 原子操作，在指定位置设定值 */ static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v); &#125; 三、存取机制3.1 put方法put()能将对应的key与value保存到map中。在ConcurrentHashMap中，key与value都不能为空，否则会抛出NullPointerException异常。如果put()时，key已经存在，则会返回put()前该key对应的value。 public V put(K key, V value) &#123; return putVal(key, value, false);&#125; final V putVal(K key, V value, boolean onlyIfAbsent) &#123; //不允许键值为null，这点与线程安全的Hashtable保持一致，和HashMap不同。 if (key == null || value == null) throw new NullPointerException(); //取键key的hashCode()和HashMap、Hashtable都一样，然后再执行spread()方法计算得到哈希地 //址，这个spread()方法和HashMap的hash()方法一样，都是将hashCode()做无符号右移16位，只不 //过spread()加多了 &amp;0x7fffffff，让结果为正数。 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果table数组为空或者长度为0(未初始化)，则调用initTable()初始化table，初始化函数 //下面介绍。 if (tab == null || (n = tab.length) == 0) tab = initTable(); //调用实现了CAS原子性操作的tabAt方法 //tabAt方法的第一个参数是Node数组的引用，第二个参数在Node数组的下标，实现的是在Nod //e数组中查找指定下标的Node，如果找到则返回该Node节点(链表头节点)，否则返回null， //这里的i = (n - 1)&amp;hash即是计算待插入的节点在table的下标，即table容量-1的结果和哈 //希地址做与运算，和HashMap的算法一样。 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; //如果该下标上并没有节点(即链表为空)，则直接调用实现了CAS原子性操作的 //casTable()方法， //casTable()方法的第一个参数是Node数组的引用，第二个参数是待操作的下标，第三 //个参数是期望值，第四个参数是待操作的Node节点，实现的是将Node数组下标为参数二 //的节点替换成参数四的节点，如果期望值和实际值不符返回false，否则参数四的节点成 //功替换上去，返回ture，即插入成功。注意这里：如果插入成功了则跳出for循环，插入 //失败的话(其他线程抢先插入了)，那么会执行到下面的代码。 if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //如果该下标上的节点的哈希地址为-1(即链表的头节点为ForwardingNode节点)，则表示 //table需要扩容，值得注意的是ConcurrentHashMap初始化和扩容不是用同一个方法，而 //HashMap和Hashtable都是用同一个方法，当前线程会去协助扩容，扩容过程后面介绍。 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //如果该下标上的节点既不是空也不是需要扩容，则表示这个链表可以插入值，将进入到链表 //中，将新节点插入或者覆盖旧值。 else &#123; V oldVal = null; //通过关键字synchroized对该下标上的节点加锁(相当于锁住锁住 //该下标上的链表)，其他下标上的节点并没有加锁，所以其他线程 //可以安全的获得其他下标上的链表进行操作，也正是因为这个所 //以提高了ConcurrentHashMap的效率，提高了并发度。 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; //如果该下标上的节点的哈希地址大于等于0，则表示这是 //个链表。 if (fh &gt;= 0) &#123; binCount = 1; //遍历链表。 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果哈希地址、键key相同 或者 键key不为空 //且键key相同，则表示存在键key和待插入的键 //key相同，则执行更新值value的操作。 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果找到了链表的最后一个节点都没有找到相 //同键Key的，则是插入操作，将插入的键值新建 //个节点并且添加到链表尾部，这个和HashMap一 //样都是插入到尾部。 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //如果该下标上的节点的哈希地址小于0 且为树节点 //则将带插入键值新增到红黑树 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; //如果插入的结果不为null，则表示为替换 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,value)) != null)&#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //判断链表的长度是否大于等于链表的阈值(8)，大于则将链表转成 //红黑树，提高效率。这点和HashMap一样。 if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125; 总结流程： 判断键值是否为null，为null抛出异常。 调用spread()方法计算key的hashCode()获得哈希地址，这个HashMap相似。 如果当前table为空，则初始化table，需要注意的是这里并没有加synchronized，也就是允许多个线程去尝试初始化table，但是在初始化函数里面使用了CAS保证只有一个线程去执行初始化过程。 使用 容量大小-1 &amp; 哈希地址 计算出待插入键值的下标，如果该下标上的bucket为null，则直接调用实现CAS原子性操作的casTabAt()方法将节点插入到table中，如果插入成功则完成put操作，结束返回。插入失败(被别的线程抢先插入了)则继续往下执行。 如果该下标上的节点(头节点)MOVED(-1)的哈希地址为-1，代表需要扩容，该线程执行helpTransfer()方法协助扩容。 如果该下标上的bucket不为空，且又不需要扩容，则进入到bucket中，同时synchronized锁住这个bucket，注意只是锁住该下标上的bucket而已，其他的bucket并未加锁，其他线程仍然可以操作其他未上锁的bucket，这个就是ConcurrentHashMap为什么高效的原因之一。 进入到bucket里面，首先判断这个bucket存储的是红黑树(哈希地址小于0，原因后面分析)还是链表。 如果是链表，则遍历链表看看是否有哈希地址和键key相同的节点，有的话则根据传入的参数进行覆盖或者不覆盖，没有找到相同的节点的话则将新增的节点插入到链表尾部。如果是红黑树，则将节点插入。到这里结束加锁。 最后判断该bucket上的链表长度是否大于链表转红黑树的阈值(8)，大于则调用treeifyBin()方法将链表转成红黑树，以免链表过长影响效率。 调用addCount()方法，作用是将ConcurrentHashMap的键值对数量+1，还有另一个作用是检查ConcurrentHashMap是否需要扩容。 补充说明： 为什么ConcurrentHashMap中，key与value都不能为空 ConcurrentHashMap的使用场景为多线程，如果有A、B两个线程，线程A调用concurrentHashMap.get(key)方法,返回为null，我们还是不知道这个null是没有映射的null还是存的值就是null。虽然可以用concurrentHashMap.containsKey(key)来判断，但是多线程下，如果A调用concurrentHashMap.get(key)方法之后，containsKey方法之前，有一个线程B执行了concurrentHashMap.put(key,null)的操作。那么我们调用containsKey方法返回的就是true了。这就与我们的假设的真实情况不符合了。也就是上面说的二义性。 spread()具体干了什么？有什么意义？ // 计算hash值// 让高16位 异或 低16位，再把高的16位置为0 static final int spread(int h) &#123; // &amp; HASH_BITS用于把hash值转化为正数 return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS; &#125; 这里我也实在是迷惑了，和hashmap的hash()方法实现一样的，只是多了个HASH_BITS=0x7fffffff static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 3.2 get方法public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; //运用键key的hashCode()计算出哈希地址 int h = spread(key.hashCode()); //如果table不为空 且 table长度大于0 且 计算出的下标上bucket不为空， //则代表这个bucket存在，进入到bucket中查找， //其中(n - 1) &amp; h为计算出键key相对应的数组下标的算法。 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; //如果哈希地址、键key相同则表示查找到，返回value，这里查找到的是头节点。 if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; //如果bucket头节点的哈希地址小于0，则代表bucket为红黑树，在红黑树中查找。 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; //如果bucket头节点的哈希地址不小于0，则代表bucket为链表，遍历链表，在链表中查找。 while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 总结流程： 调用spread()方法计算key的hashCode()获得哈希地址。 计算出键key所在的下标，算法是(n - 1) &amp; h，如果table不为空，且下标上的bucket不为空，则到bucket中查找。 如果bucket的头节点的哈希地址小于0，则代表这个bucket存储的是红黑树，否则是链表。 到红黑树或者链表中查找，找到则返回该键key的值，找不到则返回null。 3.3 initTable初始化方法调用ConcurrentHashMap的构造方法仅仅是设置了一些参数而已，而整个table的初始化是在向ConcurrentHashMap中插入元素的时候发生的。如调用put、computeIfAbsent、compute、merge等方法的时候，调用时机是检查table==null。初始化方法主要应用了关键属性sizeCtl 如果这个值 &lt; 0，表示其他线程正在进行初始化，就放弃这个操作。在这也可以看出ConcurrentHashMap的初始化只能由一个线程完成。如果获得了初始化权限，就用CAS方法将sizeCtl置为-1，防止其他线程进入。初始化数组后，将sizeCtl的值改为0.75*n sizeCtl含义 负数代表正在进行初始化或扩容操作 -1代表正在初始化 -N 表示有N-1个线程正在进行扩容操作 正数或0代表hash表还没有被初始化，这个数值表示初始化或下一次进行扩容的大小，这一点类似于扩容阈值的概念。还后面可以看到，它的值始终是当前ConcurrentHashMap容量的0.75倍，这与loadfactor是对应的。 private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; //如果table为null或者长度为0， //则一直循环试图初始化table(如果某一时刻别的线程将table初始化好了，那table不为null，该//线程就结束while循环)。 while ((tab = table) == null || tab.length == 0) &#123; //如果sizeCtl小于0， //即有其他线程正在初始化或者扩容，执行Thread.yield()将当前线程挂起，让出CPU时间， //该线程从运行态转成就绪态。 //如果该线程从就绪态转成运行态了，此时table可能已被别的线程初始化完成，table不为 //null，该线程结束while循环。 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin //如果此时sizeCtl不小于0，即没有别的线程在做table初始化和扩容操作， //那么该线程就会调用Unsafe的CAS操作compareAndSwapInt尝试将sizeCtl的值修改成 //-1(sizeCtl=-1表示table正在初始化，别的线程如果也进入了initTable方法则会执行 //Thread.yield()将它的线程挂起 让出CPU时间)， //如果compareAndSwapInt将sizeCtl=-1设置成功 则进入if里面，否则继续while循环。 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; //再次确认当前table为null即还未初始化，这个判断不能少。 if ((tab = table) == null || tab.length == 0) &#123; //如果sc(sizeCtl)大于0，则n=sc，否则n=默认的容量大 小16， //这里的sc=sizeCtl=0，即如果在构造函数没有指定容量 大小， //否则使用了有参数的构造函数，sc=sizeCtl=指定的容量大小。 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") //创建指定容量的Node数组(table)。 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; //计算阈值，n - (n &gt;&gt;&gt; 2) = 0.75n当ConcurrentHashMap储存的键值对数量 //大于这个阈值，就会发生扩容。 //这里的0.75相当于HashMap的默认负载因子，可以发现HashMap、Hashtable如果 //使用传入了负载因子的构造函数初始化的话，那么每次扩容，新阈值都是=新容 //量 * 负载因子，而ConcurrentHashMap不管使用的哪一种构造函数初始化， //新阈值都是=新容量 * 0.75。 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 判断table是否为null，即需不需要首次初始化，如果某个线程进到这个方法后，其他线程已经将table初始化好了，那么该线程结束该方法返回。 如果table为null，进入到while循环，如果sizeCtl小于0(其他线程正在对table初始化)，那么该线程调用Thread.yield()挂起该线程，让出CPU时间，该线程也从运行态转成就绪态，等该线程从就绪态转成运行态的时候，别的线程已经table初始化好了，那么该线程结束while循环，结束初始化方法返回。如果从就绪态转成运行态后，table仍然为null，则继续while循环。 如果table为null且sizeCtl不小于0，则调用实现CAS原子性操作的compareAndSwap()方法将sizeCtl设置成-1，告诉别的线程我正在初始化table，这样别的线程无法对table进行初始化。如果设置成功，则再次判断table是否为空，不为空则初始化table，容量大小为默认的容量大小(16)，或者为sizeCtl。其中sizeCtl的初始化是在构造函数中进行的，sizeCtl = ((传入的容量大小 + 传入的容量大小无符号右移1位 + 1)的结果向上取最近的2幂次方) 四、扩容机制4.1 transfer扩容方法transfer()方法为ConcurrentHashMap扩容操作的核心方法。由于ConcurrentHashMap支持多线程扩容，而且也没有进行加锁，所以实现会变得有点儿复杂。整个扩容操作分为两步： 构建一个nextTable，其大小为原来大小的两倍，这个步骤是在单线程环境下完成的 将原来table里面的内容复制到nextTable中，这个步骤是允许多线程操作的，所以性能得到提升，减少了扩容的时间消耗 /** * Moves and/or copies the nodes in each bin to new table. See * above for explanation. * 把数组中的节点复制到新的数组的相同位置，或者移动到扩张部分的相同位置 * 在这里首先会计算一个步长，表示一个线程处理的数组长度，用来控制对CPU的使用， * 每个CPU最少处理16个长度的数组元素,也就是说，如果一个数组的长度只有16，那只有一个线程会对其进行扩容的复制移动操作 * 扩容的时候会一直遍历，知道复制完所有节点，每处理一个节点的时候会在链表的头部设置一个fwd节点，这样其他线程就会跳过他， * 复制后在新数组中的链表不是绝对的反序的 */ private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) //MIN_TRANSFER_STRIDE 用来控制不要占用太多CPU stride = MIN_TRANSFER_STRIDE; // subdivide range //MIN_TRANSFER_STRIDE=16 /* * 如果复制的目标nextTab为null的话，则初始化一个table两倍长的nextTab * 此时nextTable被设置值了(在初始情况下是为null的) * 因为如果有一个线程开始了表的扩张的时候，其他线程也会进来帮忙扩张， * 而只是第一个开始扩张的线程需要初始化下目标数组 */ if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(\"unchecked\") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; /* * 创建一个fwd节点，这个是用来控制并发的，当一个节点为空或已经被转移之后，就设置为fwd节点 * 这是一个空的标志节点 */ ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; //是否继续向前查找的标志位 boolean finishing = false; // to ensure sweep(清扫) before committing nextTab,在完成之前重新在扫描一遍数组，看看有没完成的没 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) &#123; advance = false; &#125; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; //已经完成转移 nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //设置sizeCtl为扩容后的0.75 return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) &#123; return; &#125; finishing = advance = true; i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) //数组中把null的元素设置为ForwardingNode节点(hash值为MOVED[-1]) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; synchronized (f) &#123; //加锁操作 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; if (fh &gt;= 0) &#123; //该节点的hash值大于等于0，说明是一个Node节点 /* * 因为n的值为数组的长度，且是power(2,x)的，所以，在&amp;操作的结果只可能是0或者n * 根据这个规则 * 0--&gt; 放在新表的相同位置 * n--&gt; 放在新表的（n+原来位置） */ int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; /* * lastRun 表示的是需要复制的最后一个节点 * 每当新节点的hash&amp;n -&gt; b 发生变化的时候，就把runBit设置为这个结果b * 这样for循环之后，runBit的值就是最后不变的hash&amp;n的值 * 而lastRun的值就是最后一次导致hash&amp;n 发生变化的节点(假设为p节点) * 为什么要这么做呢？因为p节点后面的节点的hash&amp;n 值跟p节点是一样的， * 所以在复制到新的table的时候，它肯定还是跟p节点在同一个位置 * 在复制完p节点之后，p节点的next节点还是指向它原来的节点，就不需要进行复制了，自己就被带过去了 * 这也就导致了一个问题就是复制后的链表的顺序并不一定是原来的倒序 */ for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; //n的值为扩张前的数组的长度 if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; /* * 构造两个链表，顺序大部分和原来是反的 * 分别放到原来的位置和新增加的长度的相同位置(i/n+i) */ for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) /* * 假设runBit的值为0， * 则第一次进入这个设置的时候相当于把旧的序列的最后一次发生hash变化的节点(该节点后面可能还有hash计算后同为0的节点)设置到旧的table的第一个hash计算后为0的节点下一个节点 * 并且把自己返回，然后在下次进来的时候把它自己设置为后面节点的下一个节点 */ ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else /* * 假设runBit的值不为0， * 则第一次进入这个设置的时候相当于把旧的序列的最后一次发生hash变化的节点(该节点后面可能还有hash计算后同不为0的节点)设置到旧的table的第一个hash计算后不为0的节点下一个节点 * 并且把自己返回，然后在下次进来的时候把它自己设置为后面节点的下一个节点 */ hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; else if (f instanceof TreeBin) &#123; //否则的话是一个树节点 TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; /* * 在复制完树节点之后，判断该节点处构成的树还有几个节点， * 如果≤6个的话，就转回为一个链表 */ ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125; &#125; 具体流程 首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素： 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要untreefi，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍，完成扩容。 多线程遍历节点，处理了一个节点，就把对应点的值set为forward，另一个线程看到forward，就向后继续遍历，再加上给节点上锁的机制，就完成了多线程的控制。这样交叉就完成了复制工作。而且还很好的解决了线程安全的问题。 4.2 红黑树转换在putVal函数中，treeifyBin是在链表长度达到一定阈值（8）后转换成红黑树的函数。 但是并不是直接转换，而是进行一次容量判断，如果容量没有达到转换的要求，直接进行扩容操作并返回；如果满足条件才将链表的结构转换为TreeBin ，这与HashMap不同的是，它并没有把TreeNode直接放入红黑树，而是利用了TreeBin这个小容器来封装所有的TreeNode。 private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 五、面试题5.1 HashMap、Hashtable、ConcurrentHashMap三者对比 HashMap Hashtable ConcurrentHashMap 是否线程安全 否 是 是 线程安全采用的方式 采用synchronized类锁，效率低 采用CAS + synchronized，锁住的只有当前操作的bucket，不影响其他线程对其他bucket的操作，效率高 数据结构 数组+链表+红黑树(链表长度超过8则转红黑树) 数组+链表 数组+链表+红黑树(链表长度超过8则转红黑树) 是否允许null键值 是 否 否 哈希地址算法 (key的hashCode)^(key的hashCode无符号右移16位) key的hashCode ( (key的hashCode)^(key的hashCode无符号右移16位) )&amp;0x7fffffff 定位算法 哈希地址&amp;(容量大小-1)，1.8之后：扩容前的原始位置+扩容的大小值 (哈希地址&amp;0x7fffffff)%容量大小 哈希地址&amp;(容量大小-1) 扩容算法 当键值对数量大于阈值，则容量扩容到原来的2倍。 当键值对数量大于等于阈值，则容量扩容到原来的2倍+1 当键值对数量大于等于sizeCtl，单线程创建新哈希表，多线程复制bucket到新哈希表，容量扩容到原来的2倍 链表插入 将新节点插入到链表尾部 将新节点插入到链表头部 将新节点插入到链表尾部 继承的类 继承abstractMap抽象类 继承Dictionary抽象类 继承abstractMap抽象类 实现的接口 实现Map接口 实现Map接口 实现ConcurrentMap接口 默认容量大小 16 11 16 默认负载因子 0.75 0.75 0.75 统计size方式 直接返回成员变量size 直接返回成员变量count 遍历CounterCell数组的值进行累加，最后加上baseCount的值即为size","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"Map","slug":"Java/Map","permalink":"http://hofe.work/categories/Java/Map/"}],"tags":[{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"http://hofe.work/tags/ConcurrentHashMap/"}]},{"title":"HashMap实现原理及源码阅读","slug":"Java/HashMap实现原理及源码阅读","date":"2020-05-02T02:47:38.000Z","updated":"2020-06-06T15:53:59.367Z","comments":true,"path":"2020/05/02/Java/HashMap实现原理及源码阅读/","link":"","permalink":"http://hofe.work/2020/05/02/Java/HashMap%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%8F%8A%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","excerpt":"","text":"重点概念 底层数据结构 hash冲突解决 1.7和1.8区别 扩容机制（为什么是2倍） rehash过程 红黑树的左右旋 一、底层数据结构public class HashMap&lt;k,v&gt; extends AbstractMap&lt;k,v&gt; implements Map&lt;k,v&gt;, Cloneable, Serializable &#123; private static final long serialVersionUID = 362498820763181265L; static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 初始容量16 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//最大容量 static final float DEFAULT_LOAD_FACTOR = 0.75f;//填充比，占满0.75进行resize static final int TREEIFY_THRESHOLD = 8; // 链表长度达到8时将链表转换为红黑树 static final int UNTREEIFY_THRESHOLD = 6; // 树大小为6，就转回链表 static final int MIN_TREEIFY_CAPACITY = 64; transient Node&lt;k,v&gt;[] table;//存储元素的数组 transient Set&lt;map.entry&lt;k,v&gt;&gt; entrySet; transient int size;//存放元素的个数 transient int modCount;//被修改的次数fast-fail机制 int threshold;//临界值 当实际大小(容量*填充比)超过临界值时，会进行扩容 final float loadFactor;//填充比// 1.位桶数组transient Node&lt;k,v&gt;[] table;//存储（位桶）的数组&lt;/k,v&gt;// 2.数组元素Node&lt;K,V&gt;实现了Entry接口//Node是单向链表，它实现了Map.Entry接口 static class Node&lt;k,v&gt; implements Map.Entry&lt;k,v&gt; &#123; final int hash; final K key; V value; Node&lt;k,v&gt; next; //构造函数Hash值 键 值 下一个节点 Node(int hash, K key, V value, Node&lt;k,v&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + = + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; //判断两个node是否相等,若key和value都相等，返回true。可以与自身比较为true public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;!--?,?--&gt; e = (Map.Entry&lt;!--?,?--&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125;// 3.红黑树static final class TreeNode&lt;k,v&gt; extends LinkedHashMap.Entry&lt;k,v&gt; &#123; TreeNode&lt;k,v&gt; parent; // 父节点 TreeNode&lt;k,v&gt; left; //左子树 TreeNode&lt;k,v&gt; right;//右子树 TreeNode&lt;k,v&gt; prev; // needed to unlink next upon deletion boolean red; //颜色属性 TreeNode(int hash, K key, V val, Node&lt;k,v&gt; next) &#123; super(hash, key, val, next); &#125; //返回当前节点的根节点 final TreeNode&lt;k,v&gt; root() &#123; for (TreeNode&lt;k,v&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125;&#125; 总结：1.7的hashmap是由位桶数组+链表组成，1.8之后的hashmap由位桶数组+链表+红黑树组成。其中数组指bucket数组，数组中的元素是实现了map.Entry&lt;k,v&gt;接口的Node&lt;k,v&gt;，每个Node&lt;k,v&gt;包含key，value，next指针，hash值。当put元素时会调用hashcode计算hash值，相同key而value不同的元素会发生哈希碰撞，采用拉链拉解决，将该元素插入到链表中。当TREEIFY_THRESHOLD&gt;8时，会转化成红黑树。 1.1 构造函数//构造函数1 public HashMap(int initialCapacity, float loadFactor) &#123; //指定的初始容量非负 if (initialCapacity &lt; 0) throw new IllegalArgumentException(Illegal initial capacity: + initialCapacity); //如果指定的初始容量大于最大容量,置为最大容量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //填充比为正 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(Illegal load factor: + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);//新的扩容临界值 &#125; //构造函数2 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; //构造函数3 public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; //构造函数4用m的元素初始化散列映射 public HashMap(Map&lt;!--? extends K, ? extends V--&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; 二、存取机制在明白它是怎么取之前需要先明白是怎么存的 2.1 put(K key, V value)public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; /** * Implements Map.put and related methods * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don't change existing value 元素已经存在，是否改变现值 * @param evict if false, the table is in creation mode. 区别通过put添加还是创建时初始化数据的 * @return previous value, or null if none */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 空表，需要初始化 n = (tab = resize()).length; // resize()不仅用来调整大小，还用来进行初始化配置 /*如果table的在（n-1）&amp;hash的值是空，就新建一个节点插入在该位置*/ // （n-1）&amp;hash相当于hash%(n-1) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); /*表示有冲突,该位置已存值，开始处理冲突，采用拉链法或是红黑树*/ else &#123; Node&lt;K,V&gt; e; K k; /*检查第一个Node，p是不是要找的值*/ if (p.hash == hash &amp;&amp;((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //已存在就记住该位置 /*如果不是，判断第一个元素是链表元素还是红黑树头结点*/ else if (p instanceof TreeNode) //按红黑树方式插入 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 遍历链表插入链尾 for (int binCount = 0; ; ++binCount) &#123; /*指针为空就挂在后面*/ if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //如果冲突的节点数已经达到8个，看是否需要改变冲突节点的存储结构， //treeifyBin首先判断当前hashMap的长度，如果不足64，只进行 //resize，扩容table，如果达到64，那么将冲突的存储结构为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; /*找到了对应元素，就可以停止*/ if (e.hash == hash &amp;&amp;((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; // 继续向后 p = e; &#125; &#125; /*就是链表上有相同的key值，修改元素值*/ if (e != null) &#123; // existing mapping for key，就是key的Value存在 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue;//返回存在的Value值 &#125; &#125; ++modCount; // 修改次数+1 /*如果当前大小大于门限，门限原本是初始容量*0.75*/ if (++size &gt; threshold) resize();//扩容两倍 afterNodeInsertion(evict); return null; &#125; 下面简单说下添加键值对put(key,value)的过程： 判断位桶数组是否为空数组，是则通过resize初始化 通过hash(key)计算hash值判断该Node&lt;k,v&gt;应该插入的位置(不同的key可能有相同的hashcode) 如果该位置还没插入值，则直接插入；如果已存在值 判断key是否相同，是：则用e记录该结点； 否：则判断table[i]是否为树结点， 是：则以红黑树的方式插入，用e记录； 否：则遍历链表插入到链尾（如果长度&gt;8转成红黑树)；遇到已存该元素的情况下，用e记录，并退出 在上述步骤中，都有用e记录了数组中或链表或红黑树已存在该元素的信息。通过修改e来覆盖原值 判断加入结点后是否超过门限值，是否需要扩容 2.1.1 hash()方法与hashcode()方法我们通过hash方法计算索引，得到数组中保存的位置 static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 可以看到HashMap中的hash算法是通过key的hashcode值与其hashcode右移16位后得到的值进行异或运算得到的，那么为什么不直接使用key.hashCode()，而要进行异或操作？我们知道hash的目的是为了得到进行索引，而hash是有可能冲突的，也就是不同的key得到了同样的hash值，这样就很容易产业碰撞，如何减少这种情况的发生呢，就通过上述的hash(Object key)算法将hashcode 与 hashcode的低16位做异或运算，混合了高位和低位得出的最终hash值，冲突的概率就小多了 2.1.2 Fail-Fast 机制我们知道 java.util.HashMap 不是线程安全的，因此如果在使用迭代器的过程中有其他线程修改了map，那么将抛出ConcurrentModificationException，这就是所谓fail-fast策略。这一策略在源码中的实现是通过 modCount 域，modCount 顾名思义就是修改次数，对HashMap 内容的修改都将增加这个值，那么在迭代器初始化过程中会将这个值赋给迭代器的 expectedModCount。在迭代过程中，判断 modCount 跟 expectedModCount 是否相等，如果不相等就表示已经有其他线程修改了 Map：注意到 modCount 声明为 volatile，保证线程之间修改的可见性。 所以在这里和大家建议，当大家遍历那些非线程安全的数据结构时，尽量使用迭代器 2.2 get(key)通过put过程，我们已经知道Node(k,v)是怎么保存到map中的，现在来看看怎么取 public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; /** * Implements Map.get and related methods * * @param 该key的hash值和key * @param key the key * @return the node, or null if none */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab;//Entry对象数组 Node&lt;K,V&gt; first,e; //在tab数组中经过散列的第一个位置 int n; K k; /*找到插入的第一个Node，方法是hash值和n-1相与，tab[(n - 1) &amp; hash]*/ //也就是说在一条链上的hash值相同的 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp;(first = tab[(n - 1) &amp; hash]) != null) &#123; /*检查第一个Node是不是要找的Node*/ if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k))))//判断条件是hash值要相同，key值要相同 return first; /*检查first后面的node*/ if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); /*遍历后面的链表，找到key值和hash值都相同的Node*/ do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 通过hash(key)找到bucket数组中该hash值的位置，判断该位置的元素也就是first的key是否与要找的这个key相同 是：则返回该first元素 否：判断first是否是树节点 是：则通过红黑树的方式进行查找 否：遍历链表查找到key相同的Node并返回 如果没找到，则返回null 2.3 面试题2.3.1 hashcode()与equals()区别get()查找元素的过程：计算key的hashcode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。 Object的equals()是基于比较内存地址实现的，hashcode()是比较内存地址的hash值 在map中，hashcode(实际是hash方法，封装了hashcode和低16位异或运算)用来计算key应该放在数组中的哪个位置，equals是用在有多个hashcode相同的情况下查找需要的key。 2.3.2 为什么要重写equals()方法？因为object中的equals()方法比较的是对象的引用地址是否相等，如何你需要判断对象里的内容是否相等，则需要重写equals()方法。 2.3.3 为什么改写了equals()，也需要改写hashcode()如果你重载了equals，比如说是基于对象的内容实现的，而保留hashCode的实现（基于内存地址的hash值）不变，那么在添加进map中时需要比对hashcode，很可能某两个对象明明是“相等”，而hashCode却不一样。 2.3.4 为什么改写了hashcode()，也需要改写equals()Hashmap的key可以是任何类型的对象，例如User这种对象，为了保证两个具有相同属性的user的hashcode相同，我们就需要改写hashcode方法，比方把hashcode值的计算与User对象的id关联起来，那么只要user对象拥有相同id，那么他们的hashcode也能保持一致了，这样就可以找到在hashmap数组中的位置了。如果这个位置上有多个元素，还需要用key的equals方法在对应位置的链表中找到需要的元素，所以只改写了hashcode方法是不够的，equals方法也是需要改写。 在改写equals方法的时候，需要满足以下三点：(1) 自反性：就是说a.equals(a)必须为true。(2) 对称性：就是说a.equals(b)=true的话，b.equals(a)也必须为true。(3) 传递性：就是说a.equals(b)=true，并且b.equals(c)=true的话，a.equals(c)也必须为true。通过改写key对象的equals和hashcode方法，我们可以将任意的业务对象作为map的key(前提是你确实有这样的需要)。 三、扩容机制当hashmap中的元素个数超过数组大小loadFactor时，就会进行数组扩容，loadFactor的默认值为0.75，也就是说，默认情况下，数组大小为16，那么当hashmap中元素个数超过16\\0.75=12的时候，就把数组的大小扩展为2*16=32，即扩大为原来2倍，然后重新调用hash方法找到新的bucket位置。 3.1 resize()jdk1.7的源码： void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; ...... // 创建一个新的 Hash Table Entry[] newTable = new Entry[newCapacity]; // 将 Old Hash Table 上的数据迁移到 New Hash Table 上 transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;// 迁移数组void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; //下面这段代码的意思是： // 从OldTable里摘一个元素出来，然后放到NewTable中 for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 迁移过程： 单线程下的迁移：在扩容之后，重新计算hash定位到新数组中，相同hash值的元素照样连接成链表，只是链表相对位置进行了反转。 多线程下的迁移： 线程1在获取next结点之后被挂起，Thread 1 的 e 指向了 key(3)，而 next 指向了 key(7)。线程2顺利完成rehash过程，链表反转。 do &#123; Entry&lt;K,V&gt; next = e.next; // 假设线程一执行到这里就被调度挂起了 int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next;&#125; while (e != null); 线程1继续执行，仍会把线程二的新表当成原始的hash表，将原来e指向的key(3)节点当成是线程二中的key(3)，放在自己所建newTable[3]的头节点，线程1的next仍然指向key(7)，此时key(3)的next已经是null。 e.next = newTable[i]; // key(3)的 next 指向了线程1的新 Hash 表，因为新 Hash 表为空，所以e.next = nullnewTable[i] = e; // 线程1的新 Hash 表第一个元素指向了线程2新 Hash 表的 key(3)。e 处理完毕e = next; // 将 e 指向 next，所以新的 e 是 key(7) 线程1的e指向了上一次循环的next，也就是key(7)，此时key(7)的next已经是key(3)。将key(7)插入到table[0]的头节点，并且将key(7)的next设置为key(3), e 和next继续往下移。此时仍然没有问题。 继续下一次循环，e.next = newTable[i] 导致 key(3).next 指向了 key(7)，但此时的 key(7).next 已经指向了 key(3)， 环形链表就这样出现了。 jdk1.8的源码： /** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * * @return the table */final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; //以前的容量大于0，也就是hashMap中已经有元素了，或者new对象的时候设置了初始容量 if (oldCap &gt; 0) &#123; //如果以前的容量大于限制的最大容量1&lt;&lt;30,则设置临界值为int的最大值2^31-1 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; /** * 如果以前容量的2倍小于限制的最大容量，同时大于或等于默认的容量16，则设置临界值为以前临界值的2 * 倍，因为threshold = loadFactor*capacity，capacity扩大了2倍，loadFactor不变， * threshold自然也扩大2倍。 */ else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; /** * 在HashMap构造器Hash(int initialCapacity, float loadFactor)中有一句代码，this.threshold * = tableSizeFor(initialCapacity)， 表示在调用构造器时，默认是将初始容量暂时赋值给了 * threshold临界值，因此此处相当于将上一次的初始容量赋值给了新的容量。什么情况下会执行到这句？当调用 * 了HashMap(int initialCapacity)构造器，还没有添加元素时 */ else if (oldThr &gt; 0) newCap = oldThr; /** * 调用了默认构造器，初始容量没有设置，因此使用默认容量DEFAULT_INITIAL_CAPACITY（16），临界值 * 就是16*0.75 */ else &#123; newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; //对临界值做判断，确保其不为0，因为在上面第二种情况(oldThr &gt; 0)，并没有计算newThr if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) /**构造新表，初始化表中数据*/ Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; //将刚创建的新表赋值给table table = newTab; if (oldTab != null) &#123; //遍历将原来table中的数据放到扩容后的新表中来 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; //没有链表Node节点，直接放到新的table中下标为[e.hash &amp; (newCap - 1)]位置即可 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果是treeNode节点，则树上的节点放到newTab中 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果e后面还有链表节点，则遍历e所在的链表， else &#123; // 保证顺序 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; //记录下一个节点 next = e.next; /** * newTab的容量是以前旧表容量的两倍,因为数组table下标并不是根据循环逐步递增 * 的，而是通过（table.length-1）&amp; hash计算得到，因此扩容后，存放的位置就 * 可能发生变化，那么到底发生怎样的变化呢，就是由下面的算法得到. * * 通过e.hash &amp; oldCap来判断节点位置通过再次hash算法后，是否会发生改变，如 * 果为0表示不会发生改变，如果为1表示会发生改变。到底怎么理解呢，举个例子： * e.hash = 13 二进制：0000 1101 * oldCap = 32 二进制：0001 0000 * &amp;运算： 0 二进制：0000 0000 * 结论：元素位置在扩容后不会发生改变 */ if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; /** * e.hash = 18 二进制：0001 0010 * oldCap = 32 二进制：0001 0000 * &amp;运算： 32 二进制：0001 0000 * 结论：元素位置在扩容后会发生改变，那么如何改变呢？ * newCap = 64 二进制：0010 0000 * 通过(newCap-1)&amp;hash * 即0001 1111 &amp; 0001 0010 得0001 0010，32+2 = 34 */ else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; /** * 若(e.hash &amp; oldCap) == 0，下标不变，将原表某个下标的元素放到扩容表同样 * 下标的位置上 */ newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; /** * 若(e.hash &amp; oldCap) != 0，将原表某个下标的元素放到扩容表中 * [下标+增加的扩容量]的位置上 */ newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 总结： 如果table == null, 则为HashMap的初始化, 生成空table返回即可; 如果table不为空, 需要重新计算table的长度, newLength = oldLength &lt;&lt; 1(注, 如果原oldLength已经到了上限, 则newLength = oldLength); 遍历oldTable，oldTable[i]为空，遍历下一个 否：判断oldTable[i].next是否为空 是：存放到newTable中newTab[e.hash &amp; (newCap - 1)] 否：判断是否红黑树 是：走红黑树的重定位 否：JAVA7时还需要重新计算hash位, 但是JAVA8做了优化, 通过(e.hash &amp; oldCap)== 0来判断节点位置通过再次hash算法后，是否会发生改变 是：移动到当前hash槽位 + oldCap的位置 否：移动到新表中原下标的位置 注：newCap/oldCap为容量 四、面试题4.1 扩容为什么是2倍？主要与HashMap计算添加元素的位置时，使用的位运算有关，这是特别高效的运算；HashMap的初始容量是2的n次幂，扩容也是2倍的形式进行扩容，可以使得添加的元素均匀分布在HashMap中的数组上，减少hash碰撞，避免形成链表的结构，使得查询效率降低。 4.2 为什么String, Interger这样的wrapper类适合作为键？如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能，也就适合做Hashmap的键。因为获取对象的时候要用到equals()和hashCode()方法，键对象正确的重写这两个方法是非常重要的。因此，String，Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象 4.3 线程不安全的原因HashMap在并发场景下可能存在以下问题： 死循环：在jdk1.7中，resize过程中，从旧数组重新迁移至新数组的过程中，仍可能会发生hash冲突，形成链表，链表的相对位置发生了反转，那么在并发环境下，容易出现多线程同时resize的情况，那么就有可能在迁移过程中发生闭环，一旦发生闭环，进行get()操作的时候就会陷入死循环。在jdk1.8中，用 head 和 tail 来保证链表的顺序和之前一样，因此不会出现发生闭环的情况。 数据丢失： 如果多个线程同时使用 put 方法添加元素，而且假设正好存在两个 put 的 key 发生了碰撞（根据 hash 值计算的 bucket 一样），那么根据 HashMap 的实现，这两个 key 会添加到数组的同一个位置，这样最终就会发生其中一个线程 put 的数据被覆盖 如果多个线程同时检测到元素个数超过数组大小 * loadFactor，这样就会发生多个线程同时对 Node 数组进行扩容，都在重新计算元素位置以及复制数据，但是最终只有一个线程扩容后的数组会赋给 table，也就是说其他线程的都会丢失，并且各自线程 put 的数据也丢失 4.4 你了解重新调整HashMap大小存在什么问题吗？Jdk1.7 当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调 整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部， 这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。 注：尾部遍历（避免尾部遍历是为了避免在新列表插入数据时，遍历队尾的位置。因为，直接插入的效率更高。） 死循环的发生","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"Map","slug":"Java/Map","permalink":"http://hofe.work/categories/Java/Map/"}],"tags":[{"name":"HashMap","slug":"HashMap","permalink":"http://hofe.work/tags/HashMap/"}]},{"title":"一文详解排序算法","slug":"数据结构与算法/一文详解排序算法","date":"2020-05-01T14:23:49.000Z","updated":"2020-05-31T15:13:22.509Z","comments":true,"path":"2020/05/01/数据结构与算法/一文详解排序算法/","link":"","permalink":"http://hofe.work/2020/05/01/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/","excerpt":"","text":"转载至https://www.cnblogs.com/onepixel/articles/7674659.html 0、算法概述0.1 算法分类十种常见排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 0.2 算法复杂度 0.3 相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。快些选堆 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 空间复杂度：是指算法在计算机 内执行时所需存储空间的度量，它也是数据规模n的函数。 1、冒泡排序（Bubble Sort）冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 1.1 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 1.2 动图演示 1.3 代码实现/**冒泡排序(加入了判断是否已经排序了的boolean变量) */static void bubbleSort(int[] arr) &#123; for(int end = arr.length-1; end &gt; 0; end--)&#123; boolean isSort = true; for(int i = 0; i &lt; end; i++)&#123; if(arr[i] &gt; arr[i+1])&#123; swap(arr,i,i+1); isSort = false; &#125; &#125; if(isSort)break; &#125;&#125; 2、选择排序（Selection Sort）选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 2.1 算法描述n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； n-1趟结束，数组有序化了。 2.2 动图演示 2.3 代码实现//选择排序static void selectSort(int[] arr)&#123; for(int i = 0; i &lt; arr.length; i++) &#123; int minIndex = i;/**记录后面的最小值的下标*/ for (int j = i + 1; j &lt; arr.length; j++) //注意从i+1开始 minIndex = arr[j] &lt; arr[minIndex] ? j : minIndex; swap(arr,i,minIndex); &#125;&#125; 2.4 算法分析表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 3、插入排序（Insertion Sort）插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 3.1 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 3.2 动图演示 3.2 代码实现static void insertSort(int[] arr)&#123; for (int i = 1; i &lt; arr.length; i++)&#123; for(int j = i; j &gt; 0 &amp;&amp; arr[j] &lt; arr[j-1]; j--) swap(arr,j,j-1); &#125;&#125;//二分插入排序static void insertSort2(int[] arr) &#123; for(int i = 1; i &lt; arr.length; i++) &#123; int key = arr[i]; int L = 0, R = i-1; while(L &lt;= R) &#123; int mid = L + (R-L)/2; if(arr[mid] &gt; key) R = mid - 1; else L = mid + 1; &#125; //二分结束之后 L = 刚好大于key(不是等于)的那个位置 for(int j = i-1; j &gt;= L; j--) arr[j+1] = arr[j]; arr[L] = key; &#125;&#125; 3.4 算法分析插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 4、希尔排序（Shell Sort）1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。 4.1 算法描述先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 4.2 动图演示 4.3 代码实现//步长为 n*3+1static void shellSort(int[] arr) &#123; int gap = 0; for(; gap &lt;= arr.length; gap = gap*3+1); for(; gap &gt; 0; gap = (gap-1)/3) &#123; //增量序列 for(int i = gap; i &lt; arr.length; i++) &#123; //从数组第gap个元素开始 int key = arr[i],j; //每个元素与自己组内的数据进行直接插入排序 for(j = i-gap; j &gt;= 0 &amp;&amp; key &lt; arr[j]; j -= gap) arr[j+gap] = arr[j]; arr[j+gap] = key; &#125; &#125;&#125; 4.4 算法分析希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的。 5、归并排序（Merge Sort）归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 5.1 算法描述 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 5.2 动图演示 5.3 代码实现public static void main(String[] args) &#123; int[] arrays = &#123;9, 2, 5, 1, 3, 2, 9, 5, 2, 1, 8&#125;; mergeSort(arrays, 0, arrays.length - 1); System.out.println(\" \" + arrays); &#125;public static void mergeSort(int[] arrays, int L, int R) &#123; //如果只有一个元素，那就不用排序了 if (L == R) &#123; return; &#125; else &#123; //取中间的数，进行拆分 int M = (L + R) / 2; //左边的数不断进行拆分 mergeSort(arrays, L, M); //右边的数不断进行拆分 mergeSort(arrays, M + 1, R); //合并 merge(arrays, L, M + 1, R); &#125;&#125; public static void merge(int[] arrays, int L, int M, int R) &#123; //左边的数组的大小 int[] leftArray = new int[M - L]; //右边的数组大小 int[] rightArray = new int[R - M + 1]; //往这两个数组填充数据 for (int i = L; i &lt; M; i++) &#123; leftArray[i - L] = arrays[i]; &#125; for (int i = M; i &lt;= R; i++) &#123; rightArray[i - M] = arrays[i]; &#125; int i = 0, j = 0; // arrays数组的第一个元素 int k = L; //比较这两个数组的值，哪个小，就往数组上放 while (i &lt; leftArray.length &amp;&amp; j &lt; rightArray.length) &#123; //谁比较小，谁将元素放入大数组中,移动指针，继续比较下一个 if (leftArray[i] &lt; rightArray[j]) &#123; arrays[k] = leftArray[i]; i++; k++; &#125; else &#123; arrays[k] = rightArray[j]; j++; k++; &#125; &#125; //如果左边的数组还没比较完，右边的数都已经完了，那么将左边的数抄到大数组中(剩下的都是大数字) while (i &lt; leftArray.length) &#123; arrays[k] = leftArray[i]; i++; k++; &#125; //如果右边的数组还没比较完，左边的数都已经完了，那么将右边的数抄到大数组中(剩下的都是大数字) while (j &lt; rightArray.length) &#123; arrays[k] = rightArray[j]; k++; j++; &#125; &#125; 5.4 算法分析归并排序是一种稳定的排序方法。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(nlogn）的时间复杂度。代价是需要额外的内存空间。 6、快速排序（Quick Sort）快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 6.1 算法描述快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 6.2 动图演示 6.3 代码实现static void quickSort(int arr[]) &#123; if (arr == null || arr.length &lt;= 1) return; quickProcess(arr, 0, arr.length - 1);&#125;static void quickProcess(int[] arr, int L, int R) &#123; if (L &gt;= R) return; int p = partition(arr, L, R); quickProcess(arr, L, p - 1); quickProcess(arr, p + 1, R);&#125;/*** 对arr[l...r]部分进行partition操作* 返回p, 使得arr[L...p-1] &lt; arr[p] ; arr[p+1...R] &gt; arr[p]*/static int partition(int[] arr, int L, int R) &#123; //直接选取 arr[L]作为pivot(中心点) int key = arr[L]; int pivot = L; for (int i = L + 1; i &lt;= R; i++) &#123; if (arr[i] &lt; key) swap(arr, i, ++pivot); &#125; swap(arr, pivot, L); // 将arr[L]放到pivot位置(中间) --&gt; 完全了按照arr[L]划分数组的目的 return pivot;&#125; 7、堆排序（Heap Sort）堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 7.1 算法描述 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]&lt;=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 7.2 动图演示 7.3 代码实现static void heapSort(int[] arr) &#123; if (arr == null || arr.length &lt;= 1) return; for (int i = 0; i &lt; arr.length; i++) &#123; siftUp(arr, i); &#125; int size = arr.length - 1; swap(arr, 0, size); while (size &gt; 0) &#123; siftDown(arr, 0, size); swap(arr, 0, --size); &#125;&#125;//上浮的过程 --&gt; 把新插入的数调整为大根堆的过程static void siftUp(int[] arr, int i) &#123; while (arr[i] &gt; arr[(i - 1) / 2]) &#123; swap(arr, i, (i - 1) / 2); i = (i - 1) / 2; &#125;&#125;//下沉的过程 --&gt; 这个函数就是一个数变小了，往下沉的函数,改变的数为index 目前的自己指定的堆的大小为heapSizestatic void siftDown(int[] arr, int i, int heapSize) &#123; int L = 2 * i + 1; while (L &lt; heapSize) &#123; int maxIndex = L + 1 &lt; heapSize &amp;&amp; arr[L + 1] &gt; arr[L] ? L + 1 : L; maxIndex = arr[i] &gt; arr[maxIndex] ? i : maxIndex; if (maxIndex == i) break; //自己就是最大的， 不用忘下沉 //否则就要一直往下沉 swap(arr, i, maxIndex); i = maxIndex; L = 2 * i + 1; //继续往下 &#125;&#125; 8、计数排序（Counting Sort）计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 8.1 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 8.2 动图演示 8.3 代码实现/*** 计数排序 count 统计数组， tmp 目标填充数组*/static void countSort(int[] arr, int RANGE) &#123; /**数组中最大的元素不能超过 RANGE*/ int[] count = new int[RANGE + 1]; int[] tmp = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) count[arr[i]]++; /**现在的count[i]表示小于i的数有count[i]个，排序后元素i就放在第C[i]个输出位置上*/ for (int i = 1; i &lt;= RANGE; i++) count[i] += count[i - 1]; /** * 从后向前扫描保证计数排序的稳定性(重复元素相对次序不变) * 当再遇到重复元素时会被放在当前元素的前一个位置上保证计数排序的稳定性 */ for (int i = arr.length - 1; i &gt;= 0; i--) tmp[--count[arr[i]]] = arr[i]; //拷贝回原来的数组 for (int i = 0; i &lt; arr.length; i++) arr[i] = tmp[i];&#125; 8.4 算法分析计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 9、桶排序（Bucket Sort）桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 9.1 算法描述 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来。 9.2 图片演示 9.3 代码实现public static final int bucketNum = 100; //桶的个数 0 ～ 9号桶//桶排序public static int mapToBucket(int x) &#123; // 映射函数f(x) return x / bucketNum;&#125;public static void bucketSort(int[] arr) &#123; int[] count = new int[bucketNum]; // 计数数组，存放桶的边界信息 int[] tmp = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) //count[i]保存着i号桶中元素的个数 count[mapToBucket(arr[i])]++; for (int i = 1; i &lt; bucketNum; i++) // 定位桶边界初始时，count[i]-1(下标从0开始)为i号桶最后一个元素的位置 count[i] += count[i - 1]; //count[0]~count[9] for (int i = arr.length - 1; i &gt;= 0; i--) tmp[--count[mapToBucket(arr[i])]] = arr[i]; for (int i = 0; i &lt; arr.length; i++) arr[i] = tmp[i]; //利用计数排序确定各个桶的边界（分桶） for (int i = 0; i &lt; bucketNum; i++) &#123; int L = count[i]; //count[i]为i号桶第一个元素的位置 int R = (i == bucketNum - 1 ? arr.length - 1 : count[i + 1] - 1); //count[i+1]-1为i号桶最后一个元素的位置 //for(int j = L; j &lt;= R; j++)System.out.print( arr[j] + \" \"); System.out.println(); //print if (L &lt; R) //对每个桶里面进行排序 Arrays.sort(arr, L, R + 1);//注意这里是R+1，系统库的sort的右边界是开区间 &#125;&#125; 9.4 算法分析桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 10、基数排序（Radix Sort）基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 10.1 算法描述 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 10.2 动图演示 10.3 代码实现/** * 将整形数按照每位(从低到高)拆分，然后从低位(个位)到高位依次比较各个位，得到所在的位置 *///dn表示最大的数的位数 3位的话只能表示到999static final int dn = 3;// 获得元素x的第d位数字static int getDigit(int num, int d) &#123; int[] radix = &#123;1, 10, 100&#125;; //这里只排序总共有三位数,分别代表 个位，十位，百位 return (num / radix[d]) % 10;&#125;//根据数组arr中每个元素 的第d位数,来对整个arr数组排序static void lsdRadixSortInfo(int[] arr, int d) &#123; int[] count = new int[10]; // 单独考虑每一个位的时候， 数字都是从[0~9] int[] tmp = new int[arr.length]; for (int i = 0; i &lt; arr.length; i++) count[getDigit(arr[i], d)]++; for (int i = 1; i &lt; 10; i++) count[i] += count[i - 1]; for (int i = arr.length - 1; i &gt;= 0; i--) &#123; int digit = getDigit(arr[i], d); //元素arr[i]当前 的d位的数字为dight tmp[--count[digit]] = arr[i]; //根据当前位数字digit来排序，把每个元素A[i]放到它在输出数组B中的正确位置上 &#125; for (int i = 0; i &lt; arr.length; i++) arr[i] = tmp[i];&#125;static void lsdRadixSort(int[] arr) &#123; for (int d = 0; d &lt; dn; d++) //从低位(个位)到高位 按照每一位排序 lsdRadixSortInfo(arr, d);&#125; 10.4 算法分析基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n&gt;&gt;k，因此额外空间需要大概n个左右。","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"排序","slug":"排序","permalink":"http://hofe.work/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"Markdown中使用flow绘制流程图","slug":"工具/Markdown中使用flow绘制流程图","date":"2020-05-01T06:25:23.000Z","updated":"2020-05-05T14:21:40.628Z","comments":true,"path":"2020/05/01/工具/Markdown中使用flow绘制流程图/","link":"","permalink":"http://hofe.work/2020/05/01/%E5%B7%A5%E5%85%B7/Markdown%E4%B8%AD%E4%BD%BF%E7%94%A8flow%E7%BB%98%E5%88%B6%E6%B5%81%E7%A8%8B%E5%9B%BE/","excerpt":"","text":"栗子1： st=&gt;start: 开始e=&gt;end: 结束out1=&gt;inputoutput: 显示系统主菜单op3=&gt;operation: 选择模块out4=&gt;inputoutput: 显示模块子菜单op5=&gt;operation: 选择功能out6=&gt;inputoutput: 输出结果cond7=&gt;condition: 是否继续操作op8=&gt;operation: 退出系统st-&gt;out1out1-&gt;op3-&gt;out4-&gt;op5-&gt;out6-&gt;cond7cond7(yes)-&gt;out4cond7(no)-&gt;op8op8-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束out1&#x3D;&gt;inputoutput: 显示系统主菜单op3&#x3D;&gt;operation: 选择模块out4&#x3D;&gt;inputoutput: 显示模块子菜单op5&#x3D;&gt;operation: 选择功能out6&#x3D;&gt;inputoutput: 输出结果cond7&#x3D;&gt;condition: 是否继续操作op8&#x3D;&gt;operation: 退出系统st-&gt;out1out1-&gt;op3-&gt;out4-&gt;op5-&gt;out6-&gt;cond7cond7(yes)-&gt;out4cond7(no)-&gt;op8op8-&gt;e 栗子2：增加读者 st=&gt;start: 开始e=&gt;end: 结束op1=&gt;operation: 选择增加读者in2=&gt;inputoutput: 录入读者信息op3=&gt;operation: 建立链表结点out4=&gt;inputoutput: 录入完毕cond5=&gt;condition: 是否继续录入op6=&gt;operation: 保存链表st-&gt;op1-&gt;in2-&gt;op3-&gt;out4-&gt;cond5cond5(yes)-&gt;in2cond5(no)-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 选择增加读者in2&#x3D;&gt;inputoutput: 录入读者信息op3&#x3D;&gt;operation: 建立链表结点out4&#x3D;&gt;inputoutput: 录入完毕cond5&#x3D;&gt;condition: 是否继续录入op6&#x3D;&gt;operation: 保存链表st-&gt;op1-&gt;in2-&gt;op3-&gt;out4-&gt;cond5cond5(yes)-&gt;in2cond5(no)-&gt;e 栗子3：查询读者 st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 选择查询读者in2&#x3D;&gt;inputoutput: 录入读者证件号&#x2F;姓名op3&#x3D;&gt;operation: 打开文件op4&#x3D;&gt;operation: fscanf读数据cond5&#x3D;&gt;condition: 是否符合查找条件out6&#x3D;&gt;inputoutput: 输出结果op7&#x3D;&gt;operation: 关闭文件st-&gt;op1-&gt;in2-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;op4cond5(yes)-&gt;out6-&gt;op7-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 选择查询读者in2&#x3D;&gt;inputoutput: 录入读者证件号&#x2F;姓名op3&#x3D;&gt;operation: 打开文件op4&#x3D;&gt;operation: fscanf读数据cond5&#x3D;&gt;condition: 是否符合查找条件out6&#x3D;&gt;inputoutput: 输出结果op7&#x3D;&gt;operation: 关闭文件st-&gt;op1-&gt;in2-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;op4cond5(yes)-&gt;out6-&gt;op7-&gt;e 栗子4：修改读者 st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 读取文件op2&#x3D;&gt;operation: 建立单链表op3&#x3D;&gt;inputoutput: 输入证件号op4&#x3D;&gt;operation: 查找链表cond5&#x3D;&gt;condition: 是否找到op6&#x3D;&gt;operation: 修改记录op7&#x3D;&gt;operation: 保存链表st-&gt;op1-&gt;op2-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;econd5(yes)-&gt;op6-&gt;op7-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 读取文件op2&#x3D;&gt;operation: 建立单链表op3&#x3D;&gt;inputoutput: 输入证件号op4&#x3D;&gt;operation: 查找链表cond5&#x3D;&gt;condition: 是否找到op6&#x3D;&gt;operation: 修改记录op7&#x3D;&gt;operation: 保存链表st-&gt;op1-&gt;op2-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;econd5(yes)-&gt;op6-&gt;op7-&gt;e 栗子5：浏览所有读者 st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 读取文件op2&#x3D;&gt;operation: 遍历op3&#x3D;&gt;inputoutput: 输出读者记录cond4&#x3D;&gt;condition: 是否到文件尾op5&#x3D;&gt;operation: 退出st-&gt;op1-&gt;op2-&gt;op3-&gt;cond4-&gt;op5cond4(no)-&gt;op2cond5(yes)-&gt;op5-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 读取文件op2&#x3D;&gt;operation: 遍历op3&#x3D;&gt;inputoutput: 输出读者记录cond4&#x3D;&gt;condition: 是否到文件尾op5&#x3D;&gt;operation: 退出st-&gt;op1-&gt;op2-&gt;op3-&gt;cond4-&gt;op5cond4(no)-&gt;op2cond4(yes)-&gt;op5-&gt;e 栗子6：证件挂失 st=&gt;start: 开始e=&gt;end: 结束op1=&gt;operation: 读取文件op2=&gt;operation: 建立单链表op3=&gt;inputoutput: 输入证件号op4=&gt;operation: 查找链表cond5=&gt;condition: 是否找到op6=&gt;operation: 将该记录标记为挂失op7=&gt;operation: 保存链表st-&gt;op1-&gt;op2-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;econd5(yes)-&gt;op6-&gt;op7-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 读取文件op2&#x3D;&gt;operation: 建立单链表op3&#x3D;&gt;inputoutput: 输入证件号op4&#x3D;&gt;operation: 查找链表cond5&#x3D;&gt;condition: 是否找到op6&#x3D;&gt;operation: 将该记录标记为挂失op7&#x3D;&gt;operation: 保存链表st-&gt;op1-&gt;op2-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;econd5(yes)-&gt;op6-&gt;op7-&gt;e 栗子7：修改分类 st=&gt;start: 开始e=&gt;end: 结束op1=&gt;operation: 输入分类编号op3=&gt;operation: 打开文件op4=&gt;operation: fread读数据cond5=&gt;condition: 是否符合查找条件out6=&gt;inputoutput: fseek定位op7=&gt;operation: fwrite修改记录st-&gt;op1-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;op4cond5(yes)-&gt;out6-&gt;op7-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 输入分类编号op3&#x3D;&gt;operation: 打开文件op4&#x3D;&gt;operation: fread读数据cond5&#x3D;&gt;condition: 是否符合查找条件out6&#x3D;&gt;inputoutput: fseek定位op7&#x3D;&gt;operation: fwrite修改记录st-&gt;op1-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;op4cond5(yes)-&gt;out6-&gt;op7-&gt;e 栗子8：输出分类目录树 st=&gt;start: 开始e=&gt;end: 结束op1=&gt;operation: 打开文件op2=&gt;operation: 建立双亲存储结构out3=&gt;inputoutput: 输出根节点op4=&gt;operation: 检索孩子结点cond5=&gt;condition: 是否有孩子cond6=&gt;condition: 是否第一个op7=&gt;operation: level++st-&gt;op1-&gt;op2-&gt;out3-&gt;op4-&gt;cond5cond5(no)-&gt;econd5(yes)-&gt;cond6cond6(no)-&gt;out3cond6(yes)-&gt;op7-&gt;out3 st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 打开文件op2&#x3D;&gt;operation: 建立双亲存储结构out3&#x3D;&gt;inputoutput: 输出根节点op4&#x3D;&gt;operation: 检索孩子结点cond5&#x3D;&gt;condition: 是否有孩子cond6&#x3D;&gt;condition: 是否第一个op7&#x3D;&gt;operation: level++st-&gt;op1-&gt;op2-&gt;out3-&gt;op4-&gt;cond5cond5(no)-&gt;econd5(yes)-&gt;cond6cond6(no)-&gt;out3cond6(yes)-&gt;op7-&gt;out3 栗子9：增加分类 st=&gt;start: 开始e=&gt;end: 结束op1=&gt;operation: 选择增加分类in2=&gt;inputoutput: 录入分类信息op3=&gt;operation: 以二进制录入文件out4=&gt;inputoutput: 录入完毕cond5=&gt;condition: 是否读入\"#\"op6=&gt;operation: 保存链表st-&gt;op1-&gt;in2-&gt;op3-&gt;out4-&gt;cond5cond5(no)-&gt;in2cond5(yes)-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 选择增加分类in2&#x3D;&gt;inputoutput: 录入分类信息op3&#x3D;&gt;operation: 以二进制录入文件out4&#x3D;&gt;inputoutput: 录入完毕cond5&#x3D;&gt;condition: 是否读入&quot;#&quot;op6&#x3D;&gt;operation: 保存链表st-&gt;op1-&gt;in2-&gt;op3-&gt;out4-&gt;cond5cond5(no)-&gt;in2cond5(yes)-&gt;e 栗子9：删除分类 st=&gt;start: 开始e=&gt;end: 结束op1=&gt;operation: 输入分类编号op3=&gt;operation: 打开文件op4=&gt;operation: fread读数据cond5=&gt;condition: 是否符合查找条件out6=&gt;inputoutput: fseek定位op7=&gt;operation: fwrite删除记录st-&gt;op1-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;op4cond5(yes)-&gt;out6-&gt;op7-&gt;e st&#x3D;&gt;start: 开始e&#x3D;&gt;end: 结束op1&#x3D;&gt;operation: 输入分类编号op3&#x3D;&gt;operation: 打开文件op4&#x3D;&gt;operation: fread读数据cond5&#x3D;&gt;condition: 是否符合查找条件out6&#x3D;&gt;inputoutput: fseek定位op7&#x3D;&gt;operation: fwrite删除记录st-&gt;op1-&gt;op3-&gt;op4-&gt;cond5cond5(no)-&gt;op4cond5(yes)-&gt;out6-&gt;op7-&gt;e","categories":[{"name":"工具","slug":"工具","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/"},{"name":"markdown","slug":"工具/markdown","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/markdown/"}],"tags":[{"name":"markdown","slug":"markdown","permalink":"http://hofe.work/tags/markdown/"}]},{"title":"数据库学习笔记","slug":"数据库/数据库学习笔记","date":"2020-04-28T08:37:45.000Z","updated":"2020-05-29T13:21:00.996Z","comments":true,"path":"2020/04/28/数据库/数据库学习笔记/","link":"","permalink":"http://hofe.work/2020/04/28/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"三大范式第一范式：属性不可分割。也就是每个字段都是不可再拆分的 第二范式：非主属性完全依赖于主属性。也就是说每一行数据都可以通过主键的唯一标识获取自身其它的非主属性 第三范式：属性间不能存在传递依赖，会造成数据的冗余，这种情况需要将表拆分，并做外键关联","categories":[{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"MySQL学习笔记基础篇","slug":"数据库/通过面试题学MySQL基础篇","date":"2020-04-28T08:37:45.000Z","updated":"2020-05-31T08:25:02.338Z","comments":true,"path":"2020/04/28/数据库/通过面试题学MySQL基础篇/","link":"","permalink":"http://hofe.work/2020/04/28/%E6%95%B0%E6%8D%AE%E5%BA%93/%E9%80%9A%E8%BF%87%E9%9D%A2%E8%AF%95%E9%A2%98%E5%AD%A6MySQL%E5%9F%BA%E7%A1%80%E7%AF%87/","excerpt":"","text":"欢迎来我的个人网站，里面有最新的版本 这篇介绍了下Mysql面试题常考内容，并做了延伸；个人觉得MVCC部分讲的应该是网上较为清楚的了。 参考： https://www.codercto.com/a/88775.html https://baijiahao.baidu.com/s?id=1629409989970483292&amp;wfr=spider&amp;for=pc 如果有图片失效的情况，可能是因为我用的github图库，网站无法爬到，大家可以进入个人网站进行阅读。 一、概念mysql 有那些存储引擎，有哪些区别（innodb 与myisam 的区别？ ） mysql 索引在什么情况下会失效 mysql 的索引模型 数据库为什么用B+树。联合索引特点 可重复读是什么； 有一段代码，两个session，判断结果是什么（和重复读有关）； 数据库用什么实现可重复读。 二、存储引擎2.1 什么是存储引擎 MySQL中的数据用各种不同的技术存储在文件(或者内存)中。每一种技术都使用不同的存储机制、索引技巧、锁定水平并且最终提供广泛的不同的功能和能力。通过选择不同的技术，你能够获得额外的速度或者功能，从而改善你的应用的整体功能。 2.2 存储引擎分类使用show engines;可查看引擎种类，有MEMORY、ARCHIVE、MERGE等等，主要的是MyISAM和InnoDB。 2.3 MyISAM和InnoDB的区别 存储结构 MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。 .frm文件存储表定义，数据文件的扩展名为.MYD(MYD)，索引文件的扩展名是.MYI(MYIndex)。 InnoDB:所在的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 存储空间 MyISAM:可被压缩，存储空间较小。支持三种不同的存储格式：静态表、动态表、压缩表。 InnoDB:需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 事务支持 MyISAM:强调的是性能，每次查询具有原子性，其执行速度比Innodb类型更快，但是不提供事务支持。 InnoDB:提供事务支持。具有提交（commit）、回滚（rollback）和崩溃修复能力（crach recovery capabilities）的事务安全（transaction-safe ACID compliant）型表。 CURD操作 MyISAM: 如果执行大量的select, MyISAM是更好的选择。（因为没有支持行级锁），在增删的时候需要锁定整个表格，效率会低一些。 InnoDB:如果你的数据执行大量的insert或update，出于性能方面的考虑，应该使用InnoDB表。innoDB支持行级锁，删除插入的时候只需要锁定该行就行，效率较高。但delete from table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。 外键 MyISAM: 不支持。 InoDB:支持。 索引 MyISAM:采用非聚集索引，索引文件的数据域存储指向数据文件的指针。辅索引与主索引基本一致，但是辅索引不用保证唯一性。支持全文索引和空间索引。 InnoDB:主键索引采用聚集索引（索引的数据域存储数据文件本身），辅索引的数据域存储主键的值；因此从辅索引查找数据，需要先通过辅索引找到主键值，再访问主键索引。 三、索引3.1 索引是什么索引(Index)是帮助MySQL高效获取数据的数据结构。可以简单理解为：快速查找排好序的一种数据结构，可以提高数据检索的效率，降低数据库的IO成本 3.1.1为什么要使用索引查询数据时需要从磁盘中全表扫描读取数据，会遇上两个问题 数据量过大的时候，从磁盘中读数据到内存，内存是否有足够容量存放 如果表非常大，那么我们不可能一次将所有的数据读到内存，需要分多次读取磁盘，而操作磁盘相对于内存来说是一个非常耗时的操作 优点： 通过创建唯一索引，可以保证数据库表中每一行数据的唯一性 大大加快数据的查询速度 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能 缺点： 创建索引和维护索引要耗费时间，并且随着数据量的增加所耗费的时间也会增加 索引也需要占空间，我们知道数据表中的数据也会有最大上线设置的，如果我们有大量的索引，索引文件可能会比数据文件更快达到上线值 当对表中的数据进行增加、删除、修改时，索引也需要动态的维护，降低了数据的维护速度。 使用原则： 通过上面说的优点和缺点，我们应该可以知道，并不是每个字段度设置索引就好，也不是索引越多越好，而是需要自己合理的使用。 加在哪些列上？ 在经常需要搜索的列上，可以加快搜索的速度； 在作为主键的列上，强制该列的唯一性和组织表中数据的排列结构； 在经常用在连接的列上，这些列主要是一些外键，可以加快连接的速度； 在经常需要根据范围进行搜索的列上创建索引，因为索引已经排序，其指定的范围是连续的； 在经常需要排序的列上创建索引，因为索引已经排序，这样查询可以利用索引的排序，加快排序查询时间； 在经常使用在WHERE子句中的列上面创建索引，加快条件的判断速度 不该加在哪些列上？ 对于那些在查询中很少使用或者参考的列不应该创建索引。这是因为，既然这些列很少使用到，因此有索引或者无索引，并不能提高查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 对于那些只有很少数据值的列也不应该增加索引。这是因为，由于这些列的取值很少，例如人事表的性别列，在查询的结果中，结果集的数据行占了表中数据行的很大比例，即需要在表中搜索的数据行的比例很大。增加索引，并不能明显加快检索速度。 第三，对于那些定义为text, image和bit数据类型的列不应该增加索引。这是因为，这些列的数据量要么相当大，要么取值很少。 第四，当修改性能远远大于检索性能时，不应该创建索引。这是因为，修改性能和检索性能是互相矛盾的。当增加索引时，会提高检索性能，但是会降低修改性能。当减少索引时，会提高修改性能，降低检索性能。因此，当修改性能远远大于检索性能时，不应该创建索引。 3.2 索引分类按照索引形式不同可以分为： 普通索引：仅加速查询 唯一索引：加速查询 + 列值唯一（可以有null） 主键索引：加速查询 + 列值唯一（不可以有null）+ 表中只有一个 全文索引：对文本的内容进行分词，进行搜索 联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并 按照索引实现方式不同可以分为： 描述 特点 使用场景 B+ Tree索引 使用B+ Tree作为底层实现 对树进行搜索，查找速度快分为聚簇索引和非聚簇索引 查找、排序、分组 哈希索引 使用哈希作为底层实现 无法用于排序与分组只支持精确查找，时间复杂度为O(1) 当索引值使用的频繁时，会在B+ Tree索引之上再创建一个哈希索引 全文索引 全文索引使用倒排索引实现，记录着关键词到其所在文档的映射 查找文本中的关键词 空间索引 从所有维度来索引数据 用于地理数据存储 3.3 索引数据结构数据库索引是存储在磁盘上的，当数据量大时，就不能把整个索引全部加载到内存了，只能逐一加载每一个磁盘页（对应索引树的节点）。所以我们要减少IO次数，对于树来说，IO次数就是树的高度，而“矮胖”就是b树的特征之一，它的每个节点最多包含m个孩子，m称为b树的阶，m的大小取决于磁盘页的大小。 通过对比各种数据结构的特点，可以回答为什么要用B+Tree作为数据库的索引的问题 和其他数据结构对比来看： 二叉搜索树：如果数据是单边增长的情况，那么出现的就是和链表一样的数据结构了，树高度大。 红黑树：在二叉树的基础上多了树平衡，也叫二叉平衡树，不像二叉树那样极端的情况会往一个方向发展，但数据量大的话，红黑树的深度会很深，也就是说深度不可控，这样一来查找数据还是会很耗时。 Hash表：通过hash函数计算出数据映射位置，相比较于红黑树，hash可以固定“深度”，且映射到磁盘存储引用，但是 hash 还是有些不足：只能用于精确查找的场景，无法进行范围查询。 BTree：每个节点是一个二元数组，存放着key，value， 从左到右递增排列。节点之间存放着指向这相应区间范围内的节点。然而也存在着一些缺陷： 在插入删除时，需要对树进行一个分裂、合并、转移等操作以保持B-Tree性质。造成IO操作频繁。 区间查找可能需要返回上层节点重复遍历，IO操作繁琐。 B+Tree：非叶子结点不存储data，只存储索引（减小内存占用，这样可以多读入一些节点） 叶子结点包含所有索引字段，存放所有的data 叶子结点用指针连接，提高区间访问的性能（不用回到上层节点再往下找） b+树相比于b树的查询优势： 一次IO只能加载一个磁盘页（对应一个节点），b树的节点是由key,data组成的，而b+树的中间节点不保存数据，所以磁盘页能容纳更多节点元素，可以使得树更加矮胖； b+树查询必须查找到叶子节点，b树只要匹配到即可不用管元素位置，因此b+树查找更稳定（并不慢）； 对于范围查找来说，b+树只需遍历叶子节点链表即可，b树却需要重复地中序遍历，如下两图： 总结：B+Tree 既减少查询次数又提供了很好的范围查询 3.4 MyISAM和InnoDB索引实现索引属于存储引擎级别的概念，不同存储引擎对索引的实现方式是不同的。它们实现的都是B+Tree索引模型 3.4.1 MyISAM实现在MyISAM中，主索引和辅助索引（Secondary key）在结构上没有任何区别，只是主索引要求key是唯一的，而辅助索引的key可以重复。 MyISAM的主索引图：索引文件的每个数据域存储指向数据文件的指针(每个索引指向了数据地址) MyISAM的辅索引：索引文件的每个数据域存储指向数据文件的指针(每个索引指向了数据地址)，辐索引不用保证唯一性。因此，MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。 MyISAM的索引方式也叫做“非聚集”的，之所以这么称呼是为了与InnoDB的聚集索引区分 3.4.2 InnoDB实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。 第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶结点data域保存了完整的数据记录。这个索引的key是数据表的主键，因此InnoDB表数据文件本身就是主索引。 Innodb的主索引图: 叶子节点存储数据本身 Innodb的辐索引图: 叶子结点存储主键的值。换句话说，InnoDB的所有辅助索引都引用主键作为data域。 3.5 其他关于索引的问题主键索引三问： 为什么非主键索引结构叶子节点存储的是主键值？ 一是保证一致性，更新数据的时候只需要更新主键索引树；二是节省存储空间。 为什么推荐InnoDB表必须有主键？ 保证会有主键索引树的存在（因为数据存放在主键索引树上面），如果没有mysql会自己生成一个rowid作为自增的主键主键索引 为什么推荐使用整型的自增主键？ 一是方便查找比较，二是新增数据的时候只需要在最后加入，不会大规模调整树结构，如果是UUID的话，大小不好比较，新增的时候也极有可能在中间插入数据，会导致树结构大规调整，造成插入数据变慢。 索引合并：使用多个单列索引组合搜索 索引覆盖：select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的索引覆盖 索引下推：如果没有索引下推优化（或称ICP优化），当进行索引查询时，首先根据索引来查找记录，然后再根据where条件来过滤记录；在支持ICP优化后，MySQL会在取出索引的同时，判断是否可以进行where条件过滤，也就是说提前执行where的部分过滤操作，在某些场景下，可以大大减少回表次数，从而提升整体性能。 对于user_table表，我们现在有（username,age）联合索引 如果现在有一个需求，查出名称中以“张”开头且年龄小于等于10的用户信息，语句C如下： select * from user_table where username like '张%' and age &gt; 10 语句C有两种执行可能： 根据（username,age）联合索引查询所有满足名称以“张”开头的索引，然后回表查询出相应的全行数据，然后再筛选出满足年龄小于等于10的用户数据。 根据（username,age）联合索引查询所有满足名称以“张”开头的索引，然后直接再筛选出年龄小于等于10的索引，之后再回表查询全行数据。 明显的，第二种方式需要回表查询的全行数据比较少，这就是mysql的索引下推 注意点： 1、innodb引擎的表，索引下推只能用于二级索引。 就像之前提到的，innodb的主键索引树叶子结点上保存的是全行数据，所以这个时候索引下推并不会起到减少查询全行数据的效果。 2、索引下推一般可用于所求查询字段（select列）不是/不全是联合索引的字段，查询条件为多条件查询且查询条件子句（where/order by）字段全是联合索引。 假设表t有联合索引（a,b）,下面语句可以使用索引下推提高效率 select * from t where a &gt; 2 and b &gt; 10; 索引失效 如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)。要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 使用查询的时候遵循mysql组合索引的”最左前缀“规则，假设现在有组合索引（a，b，c），查询语句就只能是a=1或a=1and b=1或a=1 and b=1 and c=1。这里有两点需要注意 a=1 and b=1和b=1 and a=1一样，没有区别，都会使用索引。b = 1和b = 1 and c=1无法使用索引 组合索引（a，b，c）的最左前缀是a；组合索引（c，b，a）的最左前缀是c，最左前缀和表字段顺序无关。在组合索引中，如果where查询条件中某个列使用了范围查询（不管%在哪），则其右边的所有列都无法使用索引优化查询 like查询以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 需要将打算加索引的列设置为 not NULL，否则将导致引擎放弃使用索引而进行全表扫描。 索引列不能是表达式的一部分，也不能作为函数的参数，否则无法使用索引查询 SELECT * FROM user_test WHERE user_name = concat(user_name, 'fei'); 四、事务4.1 特性ACID原子性(Atomicity)：指一个事务是一个不可分割的工作单位，其中的操作要么都做，要么都不做 一致性(Consistency)：事务使得系统从一个一致的状态转换到另一个一致状态。事务的一致性决定了一个系统设计和实现的复杂度，也导致了事务的不同隔离级别。 隔离性(Isolation)：事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性(Durability)：事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 4.2 实现原理首先思考事务想要做到什么效果？ 按我理解，无非是要做到可靠性以及并发处理 可靠性：数据库要保证当insert或update操作时抛异常或者数据库crash的时候需要保障数据的操作前后的一致，想要做到这个，我需要知道我修改之前和修改之后的状态，所以就有了undo log和redo log。 并发处理：也就是说当多个并发请求过来，并且其中有一个请求是对数据修改操作的时候会有影响，为了避免读到脏数据，所以需要对事务之间的读写进行隔离，至于隔离到啥程度得看业务系统的场景了，实现这个就得用MySQL 的隔离级别。 4.2.1 原子性要实现原子性，即需要保证操作要么全做，要么全不做，对于做到一半无法完成的操作就应该进行回滚。那么就需要有日志文件记录数据被修改前的信息，这样才能达到回到修改前的状态，undo log 回滚日志就实现了这一需求。 4.2.2 持久性事务一旦提交，对数据库的改变就该是永久性的。那么由于数据是存放在磁盘中的，如果每次读写数据都需要磁盘IO，效率会很低。为此，InnoDB提供了缓存(Buffer Pool)，当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。但如果MySQL宕机了，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。 于是引入redo log，当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。 4.2.3 隔离性事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。主要考虑读写操作下的隔离，可以分为两个方面： (一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性 (一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性 4.2.4 一致性事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。 实现一致性的措施包括： 保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证 数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等 应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致 4.2.5 并发一致性问题 丢失修改：一个事务对数据进行了修改，在事务提交之前，另一个事务对同一个数据进行了修改，覆盖了之前的修改； 脏读（Dirty Read）：读到了其他事务未提交的脏数据。 不可重复读（Nonrepeatable Read）：由于其他事务对某个记录进行了 修改(更新或删除) 而导致当前事务对某个数据的两次读取结果不同。 幻读（Phantom Read）：由于其他事务 插入了一条记录 而导致当前事务对某个数据的两次读取结果不同。对于幻读必须加表级锁，防止在这个表中新增一条数据。 4.2.6 隔离级别MySQL数据的四种隔离级别： Read uncommitted (读未提交)：最低级别，任何情况都无法保证。 Read committed (读已提交)：可避免脏读的发生。 Repeatable read (可重复读)：可避免脏读、不可重复读的发生。 Serializable (串行化)：可避免脏读、不可重复读、幻读的发生。 隔离级别 可以处理的并发问题 描述 实现 性能 SERIALIZABLE (串行化) 全部 事务一个接着一个执行 三级 + GAP 锁 1星 REPEATABLE READ (可重复读) 脏读，不可重复读 所有被 SELECT 的数据不能被修改 三级 2星 READ COMMITTED (读已提交) 脏读 不可重复读，幻读 二级 3星 READ UNCOMMITTED (读未提交) 无 允许其他事务读取未提交的数据 以上四种隔离级别最高的是Serializable级别，最低的是Read uncommitted级别，当然级别越高，执行效率就越低。像Serializable这样的级别，就是以锁表的方式(类似于Java多线程中的锁)使得其他的线程只能在锁外等待，所以平时选用何种隔离级别应该根据实际情况。在MySQL数据库中默认的隔离级别为Repeatable read (可重复读)。 可以看到，隔离级别只解决了脏读、不可重复读、幻读问题，却没解决丢失修改的问题。至于为什么隔离性级别不解决丢失修改，我猜是有更好的解决方案吧。这也就解释了为什么“防止更新丢失，并不能单靠数据库事务控制器来解决，需要应用程序对要更新的数据加必要的锁来解决” repeatable read能解决脏读和不可重复读，但不能解决丢失修改。 五、锁机制按锁的粒度划分：表级锁、行级锁、页级锁 按锁级别划分：共享锁、排它锁、意向锁 按加锁方式划分：自动锁、显示锁 按使用方式划分：乐观锁、悲观锁 行锁、页锁、表锁 锁机制的基本原理可以概括为：事务在修改数据之前，需要先获得相应的锁；获得锁之后，事务便可以修改数据；该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁。 按照粒度，锁可以分为表锁、行锁以及其他位于二者之间的锁。表锁在操作数据时会锁定整张表，并发性能较差；行锁则只锁定需要操作的数据，并发性能好。但是由于加锁本身需要消耗资源，因此在锁定数据较多情况下使用表锁可以节省大量资源。MySQL中不同的存储引擎支持的锁是不一样的，例如MyIsam只支持表锁，而InnoDB同时支持表锁和行锁，且出于性能考虑，绝大多数情况下使用的都是行锁。 行级锁：行级锁分为共享锁和排他锁。行级锁是MySQL中锁定粒度最细的锁。InnoDB引擎支持行级锁和表级锁，只有在通过索引检索数据的时候，才使用行级锁，否就使用表级锁。行级锁开销大，加锁慢，锁定粒度最小，发生锁冲突的概率最低，并发度最高。 表级锁：表级锁分为表共享锁和表独占锁。表级锁开销小，加锁快，锁定粒度大，发生锁冲突最高，并发度最低 页级锁：页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折中的页级，一次锁定相邻的一组记录。BDB支持页级锁。开销和加锁时间界于表锁和行锁之间；会出现死锁。锁定粒度界于表锁和行锁之间，并发度一般。 触发行锁、表锁时机 如果where条件中只用到索引项，则加的是行锁；否则加的是表锁。比如说主键索引，唯一索引和聚簇索引等。如果sql的where是全表扫描的，想加行锁也爱莫能助。 行锁和表锁对我们编程有什么影响，要在where中尽量只用索引项，否则就会触发表锁。 innodb一般情况下走索引或者主键更新都是锁行，其余都是锁表，在并发的时候可以加select for update手工锁 5.1 什么是乐观锁和悲观锁 悲观锁：认为数据随时会被修改，因此每次读取数据之前都会上锁，防止其它事务读取或修改数据；应用于数据更新比较频繁的场景； 乐观锁：操作数据时不会上锁，但是更新时会判断在此期间有没有别的事务更新这个数据，若被更新过，则失败重试；适用于读多写少的场景。 乐观锁的实现方式有： 加一个版本号或者时间戳字段，每次数据更新时同时更新这个字段； 先读取想要更新的字段或者所有字段，更新的时候比较一下，只有字段没有变化才进行更新 5.2 事务隔离是怎么实现的？有哪些锁？事务隔离通过排他锁、共享锁实现 排他写锁（X 锁）：若事务 T对数据对象 A 加上 X 锁，则只允许 T 读取和修改 A，其他任何事物都不能再对 A 加任何类型的锁，直到 T 释放 A 上的锁为止。 共享读锁（S 锁）：若事务 T 对数据 A 加上 S 锁，则事务 T 可以读 A 但是不能修改 A，其他事务只能对 A 加 S 锁而不能加 X 锁，直到 T 释放 A 上的 S 锁为止。所以它的最主要作用是阻塞 X 锁。 意向锁：事务A锁住了表中的一行，让这一行只能读，不能写。之后，事务B申请整个表的写锁。如果事务B申请成功，那么理论上它就能修改表中的任意一行，这与A持有的行锁是冲突的。数据库需要避免这种冲突，就是说要让B的申请被阻塞，直到A释放了行锁。所以需要意向锁，当事务A申请一行的行锁的时候，数据库会自动先开始申请表的意向锁，其他事务B要加锁的时候先判断表是否已被其他事务用表锁锁表，发现表上有意向共享锁，说明表中有些行被共享行锁锁住了，因此，事务B申请表的写锁会被阻塞。 5.3 三级加锁协议（可解决脏读和不可重复读）一级加锁协议（无法避免脏读、不可重复读）–读不加锁，写加X锁 如果只对数据进行读操作，不需要加锁。 如果对数据进行写操作，需要加 X 锁直至事务结束。事务结束包括正常结束（COMMIT）和非正常结束（ROLLBACK）。 二级加锁协议（可避免脏读，无法避免不可重复读）–读加S锁（读完释放），写加X锁 在一级加锁协议的基础上，添加操作：事务 T 在读取数据 R 前要对 R 加上 S 锁，读完之后即可释放 S 锁，无需等待事务结束。 三级加锁协议（可避免脏读、不可重复读）–读加S锁（事务结束释放），写加X锁 基本与二级加锁协议相同，只是 S 锁加上之后要等到事务结束才能释放。 3 种锁： Record Locks（记录锁）：在索引记录上加锁。 Gap Locks（间隙锁）：在索引记录之间加锁，或者在第一个索引记录之前加锁，或者在最后一个索引记录之后加锁，可避免幻读。 Next-Key Locks：在索引记录上加锁，并且在索引记录之前的间隙加锁。它相当于是 Record Locks 与 Gap Locks 的一个结合。 六、MVCC6.1 什么是MVCC多版本并发控制 ，是现代数据库（包括 MySQL 、 Oracle 、 PostgreSQL 等）引擎实现中常用的处理读写冲突的手段， 目的在于提高数据库高并发场景下的吞吐性能 。 作用： 如此一来不同的事务在并发过程中， SELECT 操作可以不加锁而是通过 MVCC 机制读取指定的版本历史记录，并通过一些手段保证保证读取的记录值符合事务所处的隔离级别，从而解决并发场景下的读写冲突。 6.2 什么是读写冲突先来看下什么是版本冲突，也就知道为什么要引入MVCC解决版本冲突问题。 设想一种常见，事务A begin之后修改了数据但还没commit或者rollback这段时间内，事务B select查找数据，那事务B读到的值应该是什么？很明显，理论上来说，既然还没提交，那肯定B读到的是旧数据，但在不同隔离级别下，B读到的值也会不一样！ 如果事务 B 的隔离级别是读未提交（RU），那么两次读取均读取到 x 的最新值，即 20 。 如果事务 B 的隔离级别是读已提交（RC），那么第一次读取到旧值 10 ，第二次因为事务 A 已经提交，则读取到新值 20。 如果事务 B 的隔离级别是可重复读或者串行（RR，S），则两次均读到旧值 10 ，不论事务 A 是否已经提交。 可见在不同的隔离级别下，数据库通过 MVCC 和隔离级别，让事务之间并行操作遵循了某种规则，来保证单个事务内前后数据的一致性。 注意：这里大家会发现RR和S隔离级别读到的数据是一样的，但实现原理不同，RR是由MVCC实现，S由加锁来实现。因此，MVCC用于RC和RR级别下。 6.3 为什么需要MVCC6.3.1 事务所带来的问题InnoDB 相比 MyISAM 有两大特点，一是支持事务二是支持行级锁，事务的引入带来了一些新的挑战。相对于串行处理来说，并发事务处理能大大增加数据库资源的利用率，提高数据库系统的事务吞吐量，从而可以支持更多的用户。但并发事务处理也会带来一些问题，主要包括以下几种情况： 更新丢失（ Lost Update ）：当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题 —— 最后的更新覆盖了其他事务所做的更新。如何避免这个问题呢，最好在一个事务对数据进行更改但还未提交时，其他事务不能访问修改同一个数据。 脏读（ Dirty Reads ）：一个事务正在对一条记录做修改，在这个事务并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些尚未提交的脏数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做 “脏读” 。 不可重复读（ Non-Repeatable Reads ）：一个事务在读取某些数据已经发生了改变、或某些记录已经被删除了！这种现象叫做“不可重复读”。 幻读（ Phantom Reads ）：一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为 “幻读” 。 以上是并发事务过程中会存在的问题，解决更新丢失可以交给应用，但是后三者需要数据库提供事务间的隔离机制来解决。 实现隔离机制的方法主要有两种 ： 加读写锁 一致性快照读，即 MVCC 但本质上，隔离级别是一种在并发性能和并发产生的副作用间的妥协，通常数据库均倾向于采用 Weak Isolation 总结来说，使用MVCC的开销比使用加读写锁的开销来的小，所以需要MVCC。 6.4 MVCC原理6.4.1 行记录结构InnoDB 中 MVCC 的实现方式为：每一行记录都有两个隐藏列： DATA_TRX_ID 、 DATA_ROLL_PTR column1 column1 …… DATA_TRX_ID DATA_ROLL_PTR DB_ROW_ID id name 事务版本号 上一版本地址 没有主键的时候才出现 实现MVCC，最主要的就是 DATA_TRX_ID 、 DATA_ROLL_PTR这两个隐藏列 DATA_TRX_ID：记录最近更新这条行记录的 事务 ID ，大小为 6 个字节。也就是标记是哪个事务修改的。 DATA_ROLL_PTR：表示指向该行回滚段 （rollback segment） 的指针，大小为 7 个字节， InnoDB 便是通过这个指针找到之前版本的数据。该行记录上所有旧版本，在 undo 中都通过链表的形式组织。 这里需要大家了解了上文讲到的事务原子性由undo log记录更改前的版本来实现这部分知识。 6.4.2 版本链在多个事务并行操作某行数据的情况下，不同事务对该行数据的 UPDATE 会产生多个版本，然后通过回滚指针组织成一条 Undo Log 链，这节我们通过一个简单的例子来看一下 Undo Log 链是如何组织的， DATA_TRX_ID 和 DATA_ROLL_PTR 两个参数在其中又起到什么样的作用。 事务200的操作过程 对 column1 = 20 的这行记录加排他锁 将column1 = 20 的这一行原本的值原样拷贝到 undo log 中 修改该行的值这时产生一个新版本，更新 DATA_TRX_ID 为修改记录的事务 ID ，将 DATA_ROLL_PTR 指向刚刚拷贝到 undo log 链中的旧版本记录。如果对同一行记录执行连续的 UPDATE ， Undo Log 会组成一个链表，遍历这个链表可以看到这条记录的变迁。 记录 redo log ，包括 undo log 中的修改 那么 INSERT 和 DELETE 会怎么做呢？其实相比 UPDATE 这二者很简单， INSERT 会产生一条新纪录，它的 DATA_TRX_ID 为当前插入记录的事务 ID ； DELETE 某条记录时可看成是一种特殊的 UPDATE ，其实是软删，真正执行删除操作会在 commit 时， DATA_TRX_ID 则记录下删除该记录的事务 ID 。 6.4.3 ReadView上面说了，通过版本链的方式控制不同隔离级别下，并发事务读到的版本不一样。那事务怎么知道自己能读哪些版本呢？这就由ReadView来解决，读已提交（RC）和可重复读（RR）的生成ReadView策略是不一样的。 ReadView中主要就是有个列表来存储我们系统中当前活跃着的读写事务，也就是begin了还未提交的事务。通过这个列表来判断记录的某个版本是否对当前事务可见。 总结：ReadView记录了当前活跃着的读写事务id的列表，称之为 m_ids，它有个[ up_limit_id，low_limit_id]范围（是的，没写错，up_limit_id就是左界限），这个左右界限就决定了哪些版本的数据是事务可以访问的。 如果被访问版本的 trx_id 小于 m_ids 中的最小值 up_limit_id ，说明生成该版本的事务在 ReadView 生成前就已经提交了，所以该版本可以被当前事务访问。 如果被访问版本的 trx_id 大于 m_ids 列表中的最大值 low_limit_id ，说明生成该版本的事务在生成 ReadView 后才生成，所以该版本不可以被当前事务访问。需要根据 Undo Log 链找到前一个版本，然后根据该版本的 DB_TRX_ID 重新判断可见性。 如果被访问版本的 trx_id 属性值在 m_ids 列表中最大值和最小值之间（包含），那就需要判断一下 trx_id 的值是不是在 m_ids 列表中。如果在，说明创建 ReadView 时生成该版本所属事务还是活跃的，因此该版本不可以被访问，需要查找 Undo Log 链得到上一个版本，然后根据该版本的 DB_TRX_ID 再从头计算一次可见性；如果不在，说明创建 ReadView 时生成该版本的事务已经被提交，该版本可以被访问。 此时经过一系列判断我们已经得到了这条记录相对 ReadView 来说的可见结果。此时，如果这条记录的 delete_flag 为 true ，说明这条记录已被删除，不返回。否则说明此记录可以安全返回给客户端。 举个例子吧 假设当前列表里的事务id为[80,100]。 如果你要访问的记录版本的事务id为50，比当前列表最小的id80小，那说明这个事务在之前就提交了，所以对当前活动的事务来说是可访问的。 如果你要访问的记录版本的事务id为90, 发现此事务在列表id最大值和最小值之间，那就再判断一下是否在列表内，如果在那就说明此事务还未提交，所以版本不能被访问。如果不在那说明事务已经提交，所以版本可以被访问。 如果你要访问的记录版本的事务id为110，那比事务列表最大id100都大，那说明这个版本是在ReadView生成之后才发生的，所以不能被访问。 这些记录都是去版本链里面找的，先找最近记录，如果最近这一条记录事务id不符合条件，不可见的话，再去找上一个版本再比较当前事务的id和这个版本事务id看能不能访问，以此类推直到返回可见的版本或者结束。 6.4.4 不同隔离级别下的ReadView大致的流程就像上面说讲的那样，但是根据不同的ReadView也会有不一样的结果。这是因为已提交读隔离级别下的事务在每次查询的开始都会生成一个独立的ReadView,而可重复读隔离级别则在第一次读的时候生成一个ReadView，之后的读都复用之前的ReadView RC下的MVCC判断流程 当事务 A 未提交时，事务 B 进行查询，假设事务 B 的事务 ID 为 300 ，此时生成 ReadView 的 m_ids 为 [200，300]，而最新版本的 trx_id 为 200 ，处于 m_ids 中，则该版本记录不可被访问，查询版本链得到上一条记录的 trx_id 为 100 ，小于 m_ids 的最小值 200 ，因此可以被访问，此时事务 B 就查询到值 10 而非 20 。 待事务 A 提交之后，事务 B 进行查询，此时生成的 ReadView 的 m_ids 为 [300]，而最新的版本记录中 trx_id 为 200 ，小于 m_ids 的最小值 300 ，因此可以被访问到，此时事务 B 就查询到 20 。 RR下的MVCC判断流程 如果在 RR 隔离级别下，为什么事务 B 前后两次均查询到 10 呢？ RR 下生成 ReadView 是在事务开始时，m_ids 为 [200,300]，后面不发生变化，因此即使事务 A 提交了， trx_id 为 200 的记录依旧处于 m_ids 中，不能被访问，只能访问版本链中的记录 10 。","categories":[{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://hofe.work/tags/MySQL/"}]},{"title":"HTTPS协议详解","slug":"网络/Https协议详解","date":"2020-04-28T06:00:25.000Z","updated":"2020-05-29T02:30:11.075Z","comments":true,"path":"2020/04/28/网络/Https协议详解/","link":"","permalink":"http://hofe.work/2020/04/28/%E7%BD%91%E7%BB%9C/Https%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"欢迎来我的个人网站，里面有最新的版本 这篇介绍了下HTTPS面试题常考内容，并做了延伸，解释了原理。 一、常见面试题先来看看一些常见的面试题 Https的过程（对称加密和非对称加密，CA，还有随机数生成秘钥的方式） 二、Http和Https的区别Http协议运行在TCP之上，明文传输，客户端与服务器端都无法验证对方的身份；Https是身披SSL(Secure Socket Layer)外壳的Http，运行于SSL上，SSL运行于TCP之上，是添加了加密和认证机制的HTTP。二者之间存在如下不同： 端口不同：Http与Http使用不同的连接方式，用的端口也不一样，前者是80，后者是443； 资源消耗：和HTTP通信相比，Https通信会由于加减密处理消耗更多的CPU和内存资源； 开销：Https通信需要证书，而证书一般需要向认证机构购买； Https的加密机制是一种共享密钥加密（SSL）和公开密钥加密（TLS）并用的混合加密机制。SSL/TLS分别为对称加密和非对称加密两种方式。 三、对称加密与非对称加密3.1 对称加密对称加密是指加密和解密都用同一份密钥。A和B通过协商确定加密算法（不同客户端与服务器之间的加密算法需要不同）以及密钥。 但是存在着一个问题，A通过明文传输和server协商采用了加密算法A，但这条信息本身是没有加密的，密钥有可能泄漏，因此还是不安全。于是引入非对称加密对协商过程信息加密。 对称加密算法(加解密密钥相同) 名称 密钥长度 运算速度 安全性 资源消耗 DES 56位 较快 低 中 3DES 112位或168位 慢 中 高 AES 128、192、256位 快 高 低 3.2 非对称加密在密码学跟对称加密一起出现的，应用最广的加密机制“非对称加密”，如图，特点是私钥加密后的密文，只要是公钥，都可以解密，但是反过来公钥加密后的密文，只有私钥可以解密。私钥只有一个人有，而公钥可以发给所有的人。 基于上述的特点，我们可以得出如下结论： （1）公钥是开放给所有人的，但私钥是需要保密的，存在于服务端 （2）服务器端server向client端（A、B…..）的信息传输是不安全的：因为所有人都可以获取公钥 （3）但client端（A、B…..）向server端的信息传输确实安全的：因为私钥只有server端存在 因此，如何协商加密算法的问题，我们解决了，非对称加密算法进行对称加密算法协商过程。客户端用公钥对采用何种对称加密算法以及密钥进行加密，服务器接收到之后用私钥进行解密，只有服务器才知道和该客户端之间用什么加密算法以及密钥是什么。 非对称算法(加密密钥和解密密钥不同) 名称 成熟度 安全性(取决于密钥长度) 运算速度 资源消耗 RSA 高 高 慢 高 DSA 高 高 慢 只能用于数字签名 ECC 低 高 快 低(计算量小,存储空间占用小,带宽要求低) 四、HTTPS通信过程回顾一下Http通信过程 客户端连接到Web服务器。与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。 发送HTTP请求通过TCP套接字，客户端向Web服务器发送请求报文由请求行、请求头部、空行和请求数据4部分组成。 服务器接受请求并返回HTTP响应由状态行、响应头部、空行和响应数据4部分组成。 释放连接TCP连接 客户端浏览器解析HTML内容 Https通信过程 客户端浏览器发起连接（Http通信的第一步），端口是443。 WEB服务器将公钥发给客户端。 客户端生成一个session key，并且将session key用公钥加密后发送给服务器。 服务器用私钥将session key解密出来。 客户端和服务器用session key做对称加密通信。（Http通信的第二步） 实际上session key的生成是需要多次协商的结果。整个流程会有一个问题，第2步中WEB服务器发给客户端的公钥，万一被中间人修改了呢，换句话说，客户端怎么验证公钥的正确性呢？那就需要数字证书签发机构（CA）颁发的证书（SSL） 五、证书、数字签名在第 ② 步时服务器发送了一个SSL证书给客户端，SSL 证书中包含的具体内容有证书的颁发机构、有效期、公钥、证书持有者、签名，通过第三方的校验保证了身份的合法，解决了公钥获取的安全性。 （数字签名、摘要签名、摘要信息意义一样；摘要算法、哈希算法意义一样） 以浏览器为例说明如下整个的校验过程： （1）首先浏览器读取证书中的证书所有者、有效期等信息进行一一校验 （2）浏览器开始查找操作系统中已内置的受信任的证书发布机构CA，与服务器发来的证书中的颁发者CA比对，用于校验证书是否为合法机构颁发 （3）如果找不到，浏览器就会报错，说明服务器发来的证书是不可信任的。 （4）如果找到，那么浏览器就会从操作系统中取出 颁发者CA 的公钥，然后对服务器发来的证书里面的数字签名进行解密得到摘要签名和摘要算法 （5）浏览器使用得到的摘要算法计算出服务器发来的证书的公钥，将这个计算的值与证书中摘要签名做对比 （6）对比结果一致，则证明服务器发来的证书合法，没有被冒充 （7）此时浏览器就可以读取证书中的公钥(这才是非对称加密的公钥)，用于后续加密了 总结： 申请者通过非对称加密算法（RSA） 生成一对公钥和密钥，然后把需要的申请信息（国家，域名等）连同公钥发送给 证书认证机构（CA） CA机构确认无误后通过消息摘要算法（MD5，SHA) 生成整个申请信息的摘要签名M， 然后 把 签名M和使用的摘要算法 用 CA自己的私钥 进行加密(数字签名) 浏览器从操作系统中获取CA的公钥对数字签名进行解密，得到签名和摘要算法，用摘要算法对证书包含的公钥信息进行计算。如果计算得到的值和摘要签名m一样，说明证书中包含的公钥是正确的。 散列算法比较 名称 安全性 速度 SHA-1 高 慢 MD5 中 快 六、总结HTTPS要使客户端与服务器端的通信过程得到安全保证，必须使用对称加密算法，但是协商对称加密算法的过程，需要使用非对称加密算法来保证安全，然而直接使用非对称加密的过程本身也不安全，会有中间人篡改公钥的可能性，所以客户端与服务器不直接使用公钥，而是使用数字证书签发机构颁发的证书来保证非对称加密过程本身的安全。这样通过这些机制协商出一个对称加密算法，就此双方使用该算法进行加密解密。从而解决了客户端与服务器端之间的通信安全问题。","categories":[{"name":"网络","slug":"网络","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/"},{"name":"HTTPS","slug":"网络/HTTPS","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/HTTPS/"}],"tags":[{"name":"HTTPS","slug":"HTTPS","permalink":"http://hofe.work/tags/HTTPS/"}]},{"title":"递归题目合集","slug":"数据结构与算法/递归题目合集","date":"2020-04-28T03:25:11.000Z","updated":"2020-06-06T15:35:19.405Z","comments":true,"path":"2020/04/28/数据结构与算法/递归题目合集/","link":"","permalink":"http://hofe.work/2020/04/28/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E9%80%92%E5%BD%92%E9%A2%98%E7%9B%AE%E5%90%88%E9%9B%86/","excerpt":"","text":"剑指offer中所有关于树的题目，一般靠递归解决 可访问个人网站进行阅读最新版本 呼，在做了很多很多树的题目之后，我似乎掌握了递归的精髓了，不容易不容易。 二叉树的构建题目根据二叉树的前序遍历和中序遍历的结果，重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。 前序的特点是从左到右都是每颗子树的父节点，通过父节点，在中序中找到等于父节点的位置，在这个位置左边的就是左子树，在右边就是右子树 链接：https://www.nowcoder.com/questionTerminal/8a19cbe657394eeaac2f6ea9b0f6fcf6?f=discussion来源：牛客网import java.util.*;public class Solution &#123; public TreeNode reConstructBinaryTree(int [] pre,int [] in) &#123; int plen = pre.length; int inlen = in.length; if(plen == 0 || inlen == 0)&#123; return null; &#125; TreeNode root = new TreeNode(pre[0]); for(int i = 0; i &lt; inlen; i++)&#123; if(pre[0] == in[i])&#123; root.left = reConstructBinaryTree(Arrays.copyOfRange(pre, 1, i+1), //这里1~i+1的原因是0的位置是当前树的父节点，所以不用传入子树，下一颗子树的父节点从1开始；i+1是因为在前序数组中从1到i+1的位置都是子树包含的节点，i+1取不到 Arrays.copyOfRange(in, 0, i)); // 中序数组中0~i都是子树的节点 root.right = reConstructBinaryTree(Arrays.copyOfRange(pre, i+1,plen), Arrays.copyOfRange(in, i+1, inlen)); &#125; &#125; return root; &#125;&#125; 树的子结构题目描述输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 链接：https://www.nowcoder.com/questionTerminal/6e196c44c7004d15b1610b9afca8bd88?f=discussion来源：牛客网public class Solution &#123; public static boolean HasSubtree(TreeNode root1, TreeNode root2) &#123; boolean result = false; //当Tree1和Tree2都不为零的时候，才进行比较。否则直接返回false if (root2 != null &amp;&amp; root1 != null) &#123; //如果找到了对应Tree2的根节点的点 if(root1.val == root2.val)&#123; //以这个根节点为为起点判断是否包含Tree2 result = doesTree1HaveTree2(root1,root2); &#125; //如果找不到，那么就再去root的左儿子当作起点，去判断时候包含Tree2 if (!result) &#123; result = HasSubtree(root1.left,root2); &#125; //如果还找不到，那么就再去root的右儿子当作起点，去判断时候包含Tree2 if (!result) &#123; result = HasSubtree(root1.right,root2); &#125; &#125; //返回结果 return result; &#125; public static boolean doesTree1HaveTree2(TreeNode node1, TreeNode node2) &#123; //如果Tree2已经遍历完了都能对应的上，返回true if (node2 == null) &#123; return true; &#125; //如果Tree2还没有遍历完，Tree1却遍历完了。返回false if (node1 == null) &#123; return false; &#125; //如果其中有一个点没有对应上，返回false if (node1.val != node2.val) &#123; return false; &#125; //如果根节点对应的上，那么就分别去子节点里面匹配 return doesTree1HaveTree2(node1.left,node2.left) &amp;&amp; doesTree1HaveTree2(node1.right,node2.right); &#125; 二叉树的镜像题目描述操作给定的二叉树，将其变换为源二叉树的镜像。输入描述:二叉树的镜像定义：源二叉树 8 / \\ 6 10 / \\ / \\ 5 7 9 11 镜像二叉树 8 / \\ 10 6 / \\ / \\ 11 9 7 5 public void Mirror(TreeNode root) &#123; if (root == null) return; swap(root); Mirror(root.left); Mirror(root.right);&#125;private void swap(TreeNode root) &#123; TreeNode t = root.left; root.left = root.right; root.right = t;&#125; 对称的二叉树题目描述请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 链接：https://www.nowcoder.com/questionTerminal/ff05d44dfdb04e1d83bdbdab320efbcb?f=discussion来源：牛客网/*思路：首先根节点以及其左右子树，左子树的左子树和右子树的右子树相同* 左子树的右子树和右子树的左子树相同即可，采用递归* 非递归也可，采用栈或队列存取各级子树根节点*/public class Solution &#123; boolean isSymmetrical(TreeNode pRoot) &#123; if(pRoot == null)&#123; return true; &#125; return isSymTree(pRoot.left, pRoot.right); &#125; boolean isSymTree(TreeNode LTree, TreeNode RTree)&#123; if(LTree == null &amp;&amp; RTree == null)&#123; return true; &#125; if(LTree == null || RTree == null)&#123; return false; &#125; if(LTree.val != RTree.val)&#123; return false; &#125; return isSymTree(LTree.left, RTree.right) &amp;&amp; isSymTree(LTree.right, RTree.left); &#125;&#125; 二叉树打印题目描述从上往下打印出二叉树的每个节点，同层节点从左至右打印。 链接：https://www.nowcoder.com/questionTerminal/7fe2212963db4790b57431d9ed259701?f=discussion来源：牛客网public class Solution &#123; public ArrayList&lt;Integer&gt; PrintFromTopToBottom(TreeNode root) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); if(root==null)&#123; return list; &#125; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;TreeNode&gt;(); queue.offer(root); while (!queue.isEmpty()) &#123; TreeNode treeNode = queue.poll(); if (treeNode.left != null) &#123; queue.offer(treeNode.left); &#125; if (treeNode.right != null) &#123; queue.offer(treeNode.right); &#125; list.add(treeNode.val); &#125; return list; &#125;&#125; 题目描述从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 public class Solution &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; Print(TreeNode pRoot) &#123; ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;&gt;(); Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(pRoot); while (!queue.isEmpty()) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); int cnt = queue.size(); while (cnt-- &gt; 0) &#123; TreeNode node = queue.poll(); if (node == null) continue; list.add(node.val); queue.add(node.left); queue.add(node.right); &#125; if (list.size() != 0) ret.add(list); &#125; return ret;&#125; 二叉搜索树的后序遍历题目描述输入一个非空整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同 解法：由于后序遍历的最后一个结点是根，又满足左子树都&lt;根&lt;右子树这个条件，所以 链接：https://www.nowcoder.com/questionTerminal/a861533d45854474ac791d90e447bafd?f=discussion来源：牛客网思路：找住二叉查找树的特点：左子树&lt;根&lt;=右子树 使用分治思想public class Solution &#123; // 后序遍历特点：最后一个是根；从0开始到中间某一节点都小于根，右边都大于根 // 每一段的最右边又是这一段的根 public boolean VerifySquenceOfBST(int [] sequence) &#123; int len = sequence.length; if(len == 0)&#123; return false; &#125; if(len == 1)&#123; return true; &#125; //int root = sequence[len-1]; return judge(sequence, 0, len-1); &#125; public boolean judge(int sequence[], int start, int end)&#123; if(start &gt;= end)&#123; // 这个必须有，不然会超出数组 return true; &#125; int mid = start; while(sequence[mid] &lt; sequence[end])&#123; mid++; // start~i都是小于end的，i~end-1都是大于end的 &#125; // 由于上面的就可以判断出左边都小于父了，所以下面判断右边是否都大于根 for(int i = mid; i &lt; end; i++)&#123; if(sequence[i] &lt; sequence[end])&#123; return false; &#125; &#125; return judge(sequence, start, mid-1) &amp;&amp; judge(sequence, mid, end-1); &#125;&#125; 二叉树结点路径和题目描述输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 链接：https://www.nowcoder.com/questionTerminal/b736e784e3e34731af99065031301bca?f=discussion来源：牛客网import java.util.ArrayList;/**public class TreeNode &#123; int val = 0; TreeNode left = null; TreeNode right = null; public TreeNode(int val) &#123; this.val = val; &#125;&#125;*/private ArrayList&lt;ArrayList&lt;Integer&gt;&gt; ret = new ArrayList&lt;&gt;();public ArrayList&lt;ArrayList&lt;Integer&gt;&gt; FindPath(TreeNode root, int target) &#123; backtracking(root, target, new ArrayList&lt;&gt;()); return ret;&#125;private void backtracking(TreeNode node, int target, ArrayList&lt;Integer&gt; path) &#123; if (node == null) return; path.add(node.val); target -= node.val; if (target == 0 &amp;&amp; node.left == null &amp;&amp; node.right == null) &#123; ret.add(new ArrayList&lt;&gt;(path)); &#125; else &#123; backtracking(node.left, target, path); backtracking(node.right, target, path); &#125; path.remove(path.size() - 1);&#125; 二叉搜索树转成链表题目描述输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 明确Convert函数的功能。输入：输入一个二叉搜索树的根节点。过程：将其转化为一个有序的双向链表。输出：返回该链表的头节点。明确成员变量pLast的功能。pLast用于记录当前链表的末尾节点。明确递归过程。递归的过程就相当于按照中序遍历，将整个树分解成了无数的小树，然后将他们分别转化成了一小段一小段的双向链表。再利用pLast记录总的链表的末尾，然后将这些小段链表一个接一个地加到末尾。private TreeNode pLast = null;public TreeNode Convert(TreeNode root) &#123; if (root == null) return null; // 如果左子树为空，那么根节点root为双向链表的头节点 TreeNode head = Convert(root.left); if (head == null) head = root; // 连接当前节点root和当前链表的尾节点pLast root.left = pLast; if (pLast != null) pLast.right = root; pLast = root; Convert(root.right); return head;&#125; 二叉查找树的第k个结点题目描述给定一棵二叉搜索树，请找出其中的第k小的结点。例如， （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4 private TreeNode ret;private int cnt = 0;public TreeNode KthNode(TreeNode pRoot, int k) &#123; inOrder(pRoot, k); return ret;&#125;private void inOrder(TreeNode root, int k) &#123; if (root == null || cnt &gt;= k) return; inOrder(root.left, k); cnt++; if (cnt == k) ret = root; inOrder(root.right, k);&#125; 求树的深度题目描述从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 public int TreeDepth(TreeNode root) &#123; return root == null ? 0 : 1 + Math.max(TreeDepth(root.left), TreeDepth(root.right));&#125; 是否平衡二叉树题目描述输入一棵二叉树，判断该二叉树是否是平衡二叉树。 在这里，我们只需要考虑其平衡性，不需要考虑其是不是排序二叉树 //从下往上遍历，如果子树是平衡二叉树，则返回子树的高度；//如果发现子树不是平衡二叉树，则直接停止遍历，这样至多只对每个结点访问一次。private boolean isBalanced = true;public boolean IsBalanced_Solution(TreeNode root) &#123; height(root); return isBalanced;&#125;private int height(TreeNode root) &#123; if (root == null || !isBalanced) return 0; int left = height(root.left); int right = height(root.right); if (Math.abs(left - right) &gt; 1) isBalanced = false; return 1 + Math.max(left, right);&#125; 最低公共祖先解题思路在左右子树中查找是否存在 p 或者 q，如果 p 和 q 分别在两个子树中，那么就说明根节点就是最低公共祖先。 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); return left == null ? right : right == null ? left : root;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"递归","slug":"递归","permalink":"http://hofe.work/tags/%E9%80%92%E5%BD%92/"},{"name":"树","slug":"树","permalink":"http://hofe.work/tags/%E6%A0%91/"}]},{"title":"HTTP协议详解","slug":"网络/Http协议详解","date":"2020-04-27T16:00:00.000Z","updated":"2020-06-05T02:34:11.397Z","comments":true,"path":"2020/04/28/网络/Http协议详解/","link":"","permalink":"http://hofe.work/2020/04/28/%E7%BD%91%E7%BB%9C/Http%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/","excerpt":"","text":"欢迎来我的个人网站，里面有最新的版本 这篇介绍了下HTTP面试题常考内容，并做了延伸，解释了原理。 一、常见面试题先来看看一些常见的面试题 TCP/IP五层协议？ http协议以及一些延伸？ get和post区别？ 状态码？ url回车之后发生什么？ Https的过程（详细地说了对称加密和非对称加密，CA，还有随机数生成秘钥的方式）； http和tcp的关系 二、HTTP简介 HTTP协议是Hyper Text Transfer Protocol（超文本传输协议）的缩写,是用于从万维网（WWW:World Wide Web ）服务器传输超文本到本地浏览器的传送协议。 HTTP是一个基于TCP/IP通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。 HTTP是一个属于应用层的面向对象的协议 HTTP协议工作于客户端-服务端架构为上。浏览器作为HTTP客户端通过URL向HTTP服务端即WEB服务器发送所有请求。Web服务器根据接收到的请求后，向客户端发送响应信息。 三、特点 1、简单快速：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的类型不同。由于HTTP协议简单，使得HTTP服务器的程序规模小，因而通信速度很快。 2、灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 3、无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。 4、无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。 5、支持B/S及C/S模式。 四、工作原理HTTP协议定义Web客户端如何从Web服务器请求Web页面，以及服务器如何把Web页面传送给客户端。HTTP协议采用了请求/响应模型。客户端向服务器发送一个请求报文，请求报文包含请求的方法、URL、协议版本、请求头部和请求数据。服务器以一个状态行作为响应，响应的内容包括协议的版本、成功或者错误代码、服务器信息、响应头部和响应数据。 以下是 HTTP 请求/响应的步骤： 客户端连接到Web服务器一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80）建立一个TCP套接字连接。 发送HTTP请求通过TCP套接字，客户端向Web服务器发送一个文本的请求报文，一个请求报文由请求行、请求头部、空行和请求数据4部分组成。 服务器接受请求并返回HTTP响应Web服务器解析请求，定位请求资源。服务器将资源复本写到TCP套接字，由客户端读取。一个响应由状态行、响应头部、空行和响应数据4部分组成。 ​ 释放连接TCP连接若connection 模式为close，则服务器主动关闭TCP连接，客户端被动关闭连接，释放TCP连接;若connection 模式为keepalive，则该连接会保持一段时间，在该时间内可以继续接收请求; 客户端浏览器解析HTML内容客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取响应数据HTML，根据HTML的语法对其进行格式化，并在浏览器窗口中显示。 4.1 在浏览器地址栏键入URL，按下回车之后会经历以下流程： 浏览器向 DNS 服务器请求解析该 URL 中的域名所对应的 IP 地址; 解析出 IP 地址后，根据该 IP 地址和默认端口 80，和服务器建立TCP连接; 浏览器发出读取文件(URL 中域名后面部分对应的文件)的HTTP 请求，该请求报文作为 TCP 三次握手的第三个报文的数据发送给服务器; 服务器对浏览器请求作出响应，并把对应的 html 文本发送给浏览器; 释放 TCP连接; 浏览器将该 html 文本解析并显示内容; 五、HTTP请求方法HTTP/1.1协议中共定义了八种方法（也叫“动作”）来以不同方式操作指定的资源： GET向指定的资源发出“显示”请求。使用GET方法应该只用在读取数据，而不应当被用于产生“副作用”的操作中，例如在Web Application中。其中一个原因是GET可能会被网络蜘蛛等随意访问。 HEAD与GET方法一样，都是向服务器发出指定资源的请求。只不过服务器将不传回资源的本文部分。它的好处在于，使用这个方法可以在不必传输全部内容的情况下，就可以获取其中“关于该资源的信息”（元信息或称元数据）。 POST向指定资源提交数据，请求服务器进行处理（例如提交表单或者上传文件）。数据被包含在请求本文中。这个请求可能会创建新的资源或修改现有资源，或二者皆有。 PUT向指定资源位置上传其最新内容。 DELETE请求服务器删除Request-URI所标识的资源。 TRACE回显服务器收到的请求，主要用于测试或诊断。 OPTIONS这个方法可使服务器传回该资源所支持的所有HTTP请求方法。用’/‘来代替资源名称，向Web服务器发送OPTIONS请求，可以*测试服务器功能是否正常运作**。 CONNECTHTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。通常用于SSL加密服务器的链接（经由非加密的HTTP代理服务器）。 注意事项： 方法名称是区分大小写的。当某个请求所针对的资源不支持对应的请求方法的时候，服务器应当返回状态码405（Method Not Allowed），当服务器不认识或者不支持对应的请求方法的时候，应当返回状态码501（Not Implemented）。 HTTP服务器至少应该实现GET和HEAD方法，其他方法都是可选的。当然，所有的方法支持的实现都应当匹配下述的方法各自的语义定义。此外，除了上述方法，特定的HTTP服务器还能够扩展自定义的方法。例如PATCH（由 RFC 5789 指定的方法）用于将局部修改应用到资源。 5.1 状态码所有HTTP响应的第一行都是状态行，依次是当前HTTP版本号，3位数字组成的状态代码，以及描述状态的短语，彼此由空格分隔。 状态代码的第一个数字代表当前响应的类型： 1xx消息——请求已被服务器接收，继续处理 2xx成功——请求已成功被服务器接收、理解、并接受 3xx重定向——需要后续操作才能完成这一请求 4xx请求错误——请求含有词法错误或者无法被执行 5xx服务器错误——服务器在处理某个正确请求时发生错误 常见状态码： 200 OK &#x2F;&#x2F;客户端请求成功202 &#x2F;&#x2F;表示服务器已经接受了请求，但是还没有处理，而且这个请求最终会不会处理还不确定204 &#x2F;&#x2F;服务器成功处理了请求，但没有返回任何实体内容 ，可能会返回新的头部元信息301 &#x2F;&#x2F;客户端请求的网页已经永久移动到新的位置，当链接发生变化时，返回301代码告诉客户端链接的变化，客户端保存新的链接，并向新的链接发出请求，已返回请求结果400 Bad Request &#x2F;&#x2F;客户端请求有语法错误，不能被服务器所理解401 Unauthorized &#x2F;&#x2F;请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden &#x2F;&#x2F;服务器收到请求，但是拒绝提供服务404 Not Found &#x2F;&#x2F;请求资源不存在，eg：输入了错误的URL500 Internal Server Error &#x2F;&#x2F;服务器发生不可预期的错误503 Server Unavailable &#x2F;&#x2F;服务器当前不能处理客户端的请求，一段时间后可能恢复正常 六、面试题从输入网址到获得页面的过程 (越详细越好)？ 浏览器查询 DNS，获取域名对应的IP地址:具体过程包括浏览器搜索自身的DNS缓存、搜索操作系统的DNS缓存、读取本地的Host文件和向本地DNS服务器进行查询等。对于向本地DNS服务器进行查询，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析(此解析具有权威性)；如果要查询的域名不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析（此解析不具有权威性）。如果本地域名服务器并未缓存该网址映射关系，那么将根据其设置发起递归查询或者迭代查询； 浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立链接，发起三次握手； TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求； 服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器； 浏览器解析并渲染视图，若遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源； 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。 HTTP和HTTPS有什么区别？ 端口不同：HTTP使用的是80端口，HTTPS使用443端口； HTTP（超文本传输协议）信息是明文传输，HTTPS运行在SSL(Secure Socket Layer)之上，添加了加密和认证机制，更加安全； HTTPS由于加密解密会带来更大的CPU和内存开销； HTTPS通信需要证书，一般需要向证书颁发机构（CA）购买 HTTP1.0 与 HTTP1.1 的区别HTTP/1.1支持持续连接和流水线方式 持续连接就是万维网服务器在发送响应后仍然在一段时间内保持这条连接，使同一个客户（浏览器）和该服务器可以继续在这条连接上传送后续的HTTP请求报文和响应报文。这条持续的连接并不局限于传输同一个页面上链接的文档，而是只要文档在同一个服务器上就可以通过这条持续的连接传送。 流水线方式是客户在收到HTTP的响应报文之前就能接着发送新的请求报文。与之相对应的非流水线方式是客户在收到前一个响应后才能发送下一个请求。 GET与POST的区别？ GET是幂等的，即读取同一个资源，总是得到相同的数据，POST不是幂等的； GET一般用于从服务器获取资源，而POST有可能改变服务器上的资源； 请求形式上：GET请求的数据附在URL之后，在HTTP请求头中；POST请求的数据在请求体中； 安全性：GET请求可被缓存、收藏、保留到历史记录，且其请求数据明文出现在URL中。POST的参数不会被保存，安全性相对较高； GET只允许ASCII字符，POST对数据类型没有要求，也允许二进制数据； GET的长度有限制（操作系统或者浏览器），而POST数据大小无限制 GET产生一个TCP数据包；POST产生两个TCP数据包（跨域，会先发header包探测是否会接收）","categories":[{"name":"网络","slug":"网络","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/"},{"name":"HTTP","slug":"网络/HTTP","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/HTTP/"}],"tags":[{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"HTTP","slug":"HTTP","permalink":"http://hofe.work/tags/HTTP/"}]},{"title":"Java多线程学习笔记","slug":"Java/通过面试题学Java多线程","date":"2020-04-27T06:36:27.000Z","updated":"2020-06-05T06:21:33.180Z","comments":true,"path":"2020/04/27/Java/通过面试题学Java多线程/","link":"","permalink":"http://hofe.work/2020/04/27/Java/%E9%80%9A%E8%BF%87%E9%9D%A2%E8%AF%95%E9%A2%98%E5%AD%A6Java%E5%A4%9A%E7%BA%BF%E7%A8%8B/","excerpt":"","text":"复习多线程时候做的一些笔记，算是比较全的，考察重点比如Synchronized、RetreenLock、CAS都做了一些介绍，不过这些源码更重要，这篇没给出。也尝试过只看面试题，但如果不会原理的话，其实还挺难记住的，需要面试题可以看我的另一篇《Java面试题锦集》那篇博客。这篇博客借鉴了Java3y和JavaGuide的文章，特此感谢，欢迎大家关注他们的公众号，干货多多。 总结原子操作实现原理使用循环CAS实现原子操作JVM中的CAS操作正是利用了处理器提供的cmpxchg指令实现的，CPU指令是不会被打断的。自旋CAS实现的基本 思路就是循环进行CAS操作直到成功为止。 Atomit实现原理Atomic包的类的实现绝大部分调用Unsafe类的本地方法，而Unsafe底层实际上是调用C代码，C代码调用汇编，最后生成出一条CPU指令cmpxchg，完成操作。 原子操作三大问题及解决方案1）ABA问题 因为CAS需要在操作值的时候，检查值有没有发生变化，如果没有发生变化 则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它 的值没有发生变化，但是实际上却变化了。 ABA问题的解决思路就是使用版本号。在变量前面 追加上版本号，每次变量更新的时候把版本号加1，那么A→B→A就会变成1A→2B→3A。从 Java 1.5开始，JDK的Atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个 类的compareAndSet方法的作用是首先检查当前引用是否等于预期引用，并且检查当前标志是等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值。 2）循环时间长开销大 自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。 在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。 3）只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候可以用锁。 从Java 1.5开始，JDK提供了AtomicReference类来保证引用对象之前的原子性，就可以把多个变量放在一个对象里来进行CAS操作。 一、什么是多线程1.1 进程与线程1.1.1 定义进程进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程 线程线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈和本地方法栈，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 1.1.2 区别从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区DK18之后的元空间）资源，但是每个线程有自己的程序计数器、虚拟机栈和本地方法栈。 总结：线程是进程划分成的更小的运行单位。线程和进程最大的不同在于基本上各进程是独立的，而各线程则不一定，因为同一进程中的线程极有可能会相互影响。线程执行开销小，但不利于资源的管理和保护；而进程正相反 线程计数器为什么是私有的 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了需要注意的是，如果执行的是 native方法，那么程序计数器记录的是 undefined地址，只有执行的是Java代码时程序计数器记录的才是下一条指令的地址。 所以，程序计数器私有主要是为了线程切换后能恢复到正确的执行位置。 虚拟机栈和本地方法栈为什么是私有的虚拟机栈：每个Java方法在执行的同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息。从方法调用直至执行完成的过程，就对应着一个栈帧在Java虚拟机栈中入栈和出栈的过程 本地方法栈：和虚拟机栈所发挥的作用非常相似，区别是：虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则为虚拟机使用到的 Native方法服务。在 HotSpot虚拟机中和Java虚拟机栈合二为所以，为了保证线程中的局部变量不被别的线程访问到，虚拟机栈和本地方法栈是线程私有的。 共享堆和方法区堆和方法区是所有线程共享的资源，其中堆是进程中最大的一块内存，主要用于存放新创建的对象（所有对象都在这里分配内存），方法区主要用于存放已被加载的类信息常量、静态变量、即时编译器编译后的代码等数据。 1.2 并行与并发并行： 并行性是指同一时刻内发生两个或多个事件。 并行是在不同实体上的多个事件 并发： 并发性是指同一时间间隔内发生两个或多个事件。 并发是在同一实体上的多个事件 由此可见：并行是针对进程的，并发是针对线程的。 1.2.1 并发编程3大特性原子性 对于成员变量a来说，如果线程A执行以下操作： a++; 此时需要分三步执行： 读取a的值 将a的值加1 将加1后的值赋给a 在执行以上三步过程中，如果另一个线程B对a进行了操作，那么就不能保证原子性了。 要保证原子性，可以加锁，如synchronized 可见性 要理解可见性，需要先理解cpu的高速缓存。高速缓存是cpu的一块儿缓存区。如果线程修改了某个变量的值，那么是先将修改过的值先放入缓存区，然后满足一定条件后才会同步到内存区。同步到内存区后，其他线程才可以看见变量的改变。 可见性是指，当有一个线程修改某个成员变量的值时，其他变量可以立马看到修改过的值。 比如： int i=0,j=10; i=j; 如果线程A执行了以上代码，这时如果还未将i等于10的结果由高速缓存区同步到内存区，那么B线程读取到的i的值就是0。 用volatile修饰的变量进行写操作时，会多出一些汇编代码，是以 lock为前缀的汇编指令，这个指令会有以下两种效果： 将当前处理器缓存行的数据写回到系统内存。 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 于是就强制了其他线程重新从内存中读取数据。 有序性 处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。 用volatile关键字，禁止被它修饰的变量发生指令重排操作。是通过内存屏障去完成的禁止指令重排序。 1.3 为什么要使用多线程从计算机底层来说：线程可以比作是轻量级的进程，是程序执行的最小单位线程间的切换和调度的成本远远小于进程。另外，多核CPU时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。从当代互联网发展趋势来说：现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 1.4 使用多线程带来的问题并发编程的目的就是为了能提高程序的执行效率提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如：内存泄漏、上下文切换、死锁还有受限于硬件和软件的资源闲置问题。 1.5 线程生命周期和状态由上图可以看出：线程创建之后它将处于NEW（新建）状态，调用 start（）方法后开始运行，线程这时候处于 READY（可运行）状态。可运行状态的线程获得了CPU时间片（ timeslice）后就处于RUNNING（运行）状态。 当线程执行wait（）方法之后，线程进入 WAITING（等待）状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而 TIME WAITING（超时等待）状态相当于在等待状态的基础上增加了超时限制，比如通过 sleep（ Long millis）方法或wait（ Long millis）方法可以将Java线程置于 TIMED WAITING状态。当超时时间到达后java线程将会返回到 RUNNABLE状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到 BLOCKED（阻塞）状态。线程在执行Runnable的run（）方法之后将会进入到 TERMINATED（终止）状态。 1.6 上下文切换概括来说就是：当前任务在执行完CPU时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。 1.7 线程死锁以及避免学过操作系统的朋友都知道产生死锁必须具备以下四个条件 互斥条件：该资源任意一个时刻只由一个线程占用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件: 线程已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系 避免死锁 破坏互斥条件：这个条件我们没有办法破坏，因为我们用锁本来就是想让他们互斥的（临界资源需要互斥访问）。 破坏请求与保持条件：一次性申请所有的资源。 破坏不剥夺条件：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。 破坏循环等待条件：靠按序申请资源来预防。按某一顺序申请资源）释放资源则反序释放。破坏循环等待条件。 1.8 sleep()方法和wait()方法区别和共同点？两者最主要的区别在于： sleep方法没有释放锁，而wait方法释放了锁。 两者都可以暂停线程的执行。 wait通常被用于线程间交互/通信， sleep通常被用于暂停执行。 sleep来自Thread类，wait来自Object类 wait()方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify或者notifyAll方法。sleep方法执行完成后，线程会自动苏醒。或者可以使用 wait（long timeout)超时后线程会自动苏醒。 wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用，而sleep可以在任何地方使用 1.9 为什么我们调用 start()方法时会执行run()方法，为什么我们不能直接调用run()方法？new一个 Thread，线程进入了新建状态；调用start()方法，会启动一个线程并使线程进入了就绪状态当分配到时间片后就可以开始运行了。 start()会执行线程的相应准备工作，然后自动执行run()方法的内容，这是真正的多线程工作。而直接执行run()方法，会把run方法当成一个main线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。总结：调用 start方法方可启动线程并使线程进入就绪状态，而run方法只是 thread的一个普通方法调用，还是在主线程里执行。 1.10 多线程实现的三种方式1.10.1 继承Thread，重写run方法public class MyThread extends Thread &#123; public void run() &#123; // ... &#125;&#125;public static void main(String[] args) &#123; MyThread mt = new MyThread(); mt.start();&#125; 1.10.2 实现Runnable接口，重写run方法public class MyRunnable implements Runnable &#123; @Override public void run() &#123; // ... &#125;&#125;public static void main(String[] args) &#123; MyRunnable instance = new MyRunnable(); Thread thread = new Thread(instance); thread.start();&#125; 1.10.3 实现Callable接口，重写run方法有返回值的任务必须实现Callable接口，类似的，无返回值的任务必须Runnable接口。执行 Callable任务后，可以获取一个Future的对象，在该对象上调用get就可以获取到Callable任务 返回的Object了，再结合线程池接口ExecutorService就可以实现传说中有返回结果的多线程 了 Callable 可以有返回值，返回值通过 FutureTask 进行封装 public class MyCallable implements Callable&lt;Integer&gt; &#123; public Integer call() &#123; return 123; &#125;&#125;public static void main(String[] args) throws ExecutionException, InterruptedException &#123; MyCallable mc = new MyCallable(); FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(mc); Thread thread = new Thread(ft); thread.start(); System.out.println(ft.get());&#125; 1.10.4 实现接口 与继承 Thread区别实现接口会更好一些，因为： Java 不支持多重继承，因此继承了 Thread 类就无法继承其它类，但是可以实现多个接口； 类可能只要求可执行就行，继承整个 Thread 类开销过大。 二、线程池线程池可以看做是线程的集合。线程生命周期的开销非常高，创建和销毁线程所花费的时间和资源可能比处理客户端的任务花费的时间和资源更多，并且还会有某些空闲线程也会占用资源。引入线程池，当请求到来时，线程池给这个请求分配一个空闲的线程，任务完成后回到线程池中等待下次任务(而不是销毁)。这样就实现了线程的重用。 所以说：线程最好是交由线程池来管理，这样可以减少对线程生命周期的管理，一定程度上提高性能。 2.1 线程池分类2.1.1 通过构造方法实现 2.1.2 通过Excutor工具类JDK给我们提供了Excutor框架来使用线程池，它是线程池的基础。但是严格意义上讲 Executor 并不是一个线程池，而 只是一个执行线程的工具。真正的线程池接口是ExecutorService newFixedThreadPool该方法返回一个固定线程数量的线程池。该线程池中的线程数量始终不变。当有一个新的任务提交时，线程池中若有空闲线程，则立即执行。若没有，则新的任务会被暂存在一个任务队列中，待有线程空闲时，便处理在任务队列中的任务。 public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; newCachedThreadPool该方法返回一个可根据实际情况调整线程数量的线程池。线程池的线程数量不确定，但若有空闲线程可以复用，则会优先使用可复用的线程。若所有线程均在工作，又有新的任务提交，则会创建新的线程处理任务。所有线程在当前任务执行完毕后，将返回线程池进行复用。 public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; newSingleThreadExecutor方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务会被保存在一个任务队列中，待线程空闲，按先入先出的顺序执行队列中的任务。 public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 弊端 2.2 ThreadPoolExecutor详解Executor调用创建的三种线程池内部也是通过传递ThreadPoolExecutor不同参数实现的。 2.2.1 构造参数public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; 核心线程数量：定义了最小可以同时运行的线程数量 最大线程数量：当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 允许线程空闲时间：当线程池中的线程数量大于 corePoo lsize的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁 时间对象：keepAliveTime参数的时间单位 阻塞队列：当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中 线程工厂：创建新线程的时候会用到 任务拒绝策略： 2.2.2 参数要点线程数量要点： 如果运行线程的数量少于核心线程数量，则创建新的线程处理请求 如果运行线程的数量大于核心线程数量，小于最大线程数量，则当队列满的时候才创建新的线程 如果核心线程数量等于最大线程数量，那么将创建固定大小的连接池 如果设置了最大线程数量为无穷，那么允许线程池适合任意的并发数量 线程空闲时间要点： 当前线程数大于核心线程数，如果空闲时间已经超过了，那该线程会销毁。 排队策略要点： 同步移交：不会放到队列中，而是等待线程执行它。如果当前线程没有执行，很可能会新开一个线程执行。 无界限策略：如果核心线程都在工作，该线程会放到队列中。所以线程数不会超过核心线程数 有界限策略：可以避免资源耗尽，但是一定程度上减低了吞吐量 当线程关闭或者线程数量满了和队列饱和了，就有拒绝任务的情况了： 拒绝任务策略： 直接抛出异常 使用调用者的线程来处理 直接丢掉这个任务 丢掉最老的任务 2.2.3 拒绝策略AbortPolicy（中止策略）publicstaticclass AbortPolicy implements RejectedExecutionHandler &#123; public AbortPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; thrownew RejectedExecutionException(\"Task \" + r.toString() + \" rejected from \" + e.toString()); &#125; &#125; 功能：当触发拒绝策略时，直接抛出拒绝执行的异常，中止策略的意思也就是打断当前执行流程 使用场景：这个就没有特殊的场景了，但是一点要正确处理抛出的异常。ThreadPoolExecutor中默认的策略就是AbortPolicy，ExecutorService接口的系列ThreadPoolExecutor因为都没有显示的设置拒绝策略，所以默认的都是这个。但是请注意，ExecutorService中的线程池实例队列都是无界的，也就是说把内存撑爆了都不会触发拒绝策略。当自己自定义线程池实例时，使用这个策略一定要处理好触发策略时抛的异常，因为他会打断当前的执行流程。 CallerRunsPolicy（调用者运行策略）publicstaticclass CallerRunsPolicy implements RejectedExecutionHandler &#123; public CallerRunsPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; if (!e.isShutdown()) &#123; r.run(); &#125; &#125; &#125; 功能：当触发拒绝策略时，只要线程池没有关闭，就由提交任务的当前线程处理。 使用场景：一般在不允许失败的、对性能要求不高、并发量较小的场景下使用，因为线程池一般情况下不会关闭，也就是提交的任务一定会被运行，但是由于是调用者线程自己执行的，当多次提交任务时，就会阻塞后续任务执行，性能和效率自然就慢了。 DiscardOldestPolicy（丢弃策略）publicstaticclass DiscardPolicy implements RejectedExecutionHandler &#123; public DiscardPolicy() &#123; &#125; public void rejectedExecution(Runnable r, ThreadPoolExecutor e) &#123; &#125; &#125; 功能：如果线程池未关闭，就弹出队列头部的元素，然后尝试执行 使用场景：这个策略还是会丢弃任务，丢弃时也是毫无声息，但是特点是丢弃的是老的未执行的任务，而且是待执行优先级较高的任务。基于这个特性，我能想到的场景就是，发布消息，和修改消息，当消息发布出去后，还未执行，此时更新的消息又来了，这个时候未执行的消息的版本比现在提交的消息版本要低就可以被丢弃了。因为队列中还有可能存在消息版本更低的消息会排队执行，所以在真正处理消息的时候一定要做好消息的版本比较 2.2.4 阻塞队列无界队列（newFixedThreadPool、newSingleThreadExecutor） 队列大小无限制，常用的为无界的LinkedBlockingQueue，使用该队列做为阻塞队列时要尤其当心，当任务耗时较长时可能会导致大量新任务在队列中堆积最终导致OOM。阅读代码发现，Executors.newFixedThreadPool 采用就是 LinkedBlockingQueue，当QPS很高，发送数据很大，大量的任务被添加到这个无界LinkedBlockingQueue 中，导致cpu和内存飙升服务器挂掉。 有界队列 常用的有两类，一类是遵循FIFO原则的队列如ArrayBlockingQueue，另一类是优先级队列如PriorityBlockingQueue。PriorityBlockingQueue中的优先级由任务的Comparator决定。使用有界队列时队列大小需和线程池大小互相配合，线程池较小有界队列较大时可减少内存消耗，降低cpu使用率和上下文切换，但是可能会限制系统吞吐量。 同步移交队列（newCachedThreadPool） 如果不希望任务在队列中等待而是希望将任务直接移交给工作线程，可使用SynchronousQueue作为等待队列。SynchronousQueue不是一个真正的队列，而是一种线程之间移交的机制。要将一个元素放入SynchronousQueue中，必须有另一个线程正在等待接收这个元素。只有在使用无界线程池或者有饱和策略时才建议使用该队列 2.3 execute执行方法先了解下Executor与Callable 2.3.1 实现 Runnable接口和 Callable接口的区别Runnable自Java1.0以来一直存在，但 Callable仅在Java1.5中引入目的就是为了来处理 Runnable不支持的用例。Runnable接口不会返回结果或抛出检查异常，但是 Callable接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 Runnable接口，这样代码看起来会更加简洁。 工具类 Executors可以实现 Runnable对象和 Callable对象之间的相互转换。Executors callable（ Runnable task ） 或 Executors, callable（Runnable task, object resule）） 2.3.2 execute()和submit方法区别submit方法execute()方法 public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); //如果线程池中运行的线程数量&lt;corePoolSize，则创建新线程来处理请求，即使其他辅助线程是空闲的。 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //如果线程池中运行的线程数量&gt;=corePoolSize，且线程池处于RUNNING状态，且把提交的任务成功放入阻塞队列中，就再次检查线程池的状态， // 1.如果线程池不是RUNNING状态，且成功从阻塞队列中删除任务，则该任务由当前 RejectedExecutionHandler 处理。 // 2.否则如果线程池中运行的线程数量为0，则通过addWorker(null, false)尝试新建一个线程，新建线程对应的任务为null。 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; // 如果以上两种case不成立，即没能将任务成功放入阻塞队列中，且addWoker新建线程失败，则该任务由当前 RejectedExecutionHandler 处理。 else if (!addWorker(command, false)) reject(command); &#125; 三、Java锁3.1 锁的分类3.1.1 同步锁与死锁同步锁当多个线程同时访问同一个数据时，很容易出现问题。为了避免这种情况出现，我们要保证线程 同步互斥，就是指并发执行的多个线程，在同一时间内只允许一个线程访问共享数据。 Java 中可 以使用synchronized关键字来取得一个对象的同步锁。死锁何为死锁，就是多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放 3.1.2 乐观锁乐观锁是一种乐观思想，即认为读多写少，遇到并发写的可能性低，每次去拿数据的时候都认为 别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数 据，采取在写时先读出当前版本号，然后加锁操作（比较跟上一次的版本号，如果一样则更新）， 如果失败则要重复读-比较-写的操作。java 中的乐观锁基本都是通过 CAS 操作实现的，CAS 是一种更新的原子操作，比较当前值跟传入 值是否一样，一样则更新，否则失败。 3.1.3 悲观锁悲观锁是就是悲观思想，即认为写多，遇到并发写的可能性高，每次去拿数据的时候都认为别人 会修改，所以每次在读写数据的时候都会上锁，这样别人想读写这个数据就会block（阻塞）直到拿到锁。 java中的悲观锁就是Synchronized,AQS框架下的锁则是先尝试cas乐观锁去获取锁，获取不到， 才会转换为悲观锁，如RetreenLock。 Java并发包提供的加锁模式分为独占锁和共享锁。 3.1.4 独占锁独占锁模式下，每次只能有一个线程能持有锁，ReentrantLock 就是以独占方式实现的互斥锁。 独占锁是一种悲观保守的加锁策略，它避免了读/读冲突，如果某个只读线程获取锁，则其他读线 程都只能等待，这种情况下就限制了不必要的并发性，因为读操作并不会影响数据的一致性。 3.1.5 共享锁 共享锁则允许多个线程同时获取锁，并发访问 共享资源，如：ReadWriteLock。共享锁则是一种 乐观锁，它放宽了加锁策略，允许多个执行读操作的线程同时访问共享资源。 AQS的内部类Node定义了两个常量SHARED和EXCLUSIVE，他们分别标识 AQS队列中等 待线程的锁获取模式。 java的并发包中提供了ReadWriteLock，读-写锁。它允许一个资源可以被多个读操作访问， 或者被一个 写操作访问，但两者不能同时进行。 3.1.6 公平锁与非公平锁公平锁指的是锁的分配机制是公平的，通常先对锁提出获取请求的线程会先被分配到锁， ReentrantLock在构造函数中提供了是否公平锁的初始化方式来定义公平锁。 JVM 按随机、就近原则分配锁的机制则称为不公平锁，ReentrantLock 在构造函数中提供了 是否公平锁的初始化方式，默认为非公平锁。非公平锁实际执行的效率要远远超出公平锁，除非 程序有特殊需要，否则常用非公平锁的分配机制。 3.1.7 可重入锁本文里面讲的是广义上的可重入锁，而不是单指JAVA下的ReentrantLock。可重入锁，也叫 做递归锁，指的是同一线程 外层函数获得锁之后 ，内层递归函数仍然有获取该锁的代码，但不受 影响。在JAVA环境下 ReentrantLock 和synchronized 都是 可重入锁。 3.1.8 分段锁分段锁也并非一种实际的锁，而是一种思想ConcurrentHashMap是学习分段锁的好实践 3.2 锁的实现3.2.1 Volatile关键字Java语言提供了一种稍弱的同步机制，即volatile变量，用来确保将变量的更新操作通知到其他 线程。volatile 变量具备两种特性，volatile变量不会被缓存在寄存器或者对其他处理器不可见的 地方，因此在读取volatile类型的变量时总会返回新写入的值。 可见性其一是保证该变量对所有线程可见，这里的可见性指的是当一个线程修改了变量的值，那么新的 值对于其他线程是可以立即获取的。 禁止重排序volatile 禁止了指令重排。 比sychronized关键字更轻量级的同步机制在访问volatile变量时不会执行加锁操作，因此也就不会使执行线程阻塞，因此volatile变量是一 种比sychronized关键字更轻量级的同步机制。volatile适合这种场景：一个变量被多个线程共 享，线程直接给这个变量赋值。 当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到CPU缓存中。如果计算机有 多个CPU，每个线程可能在不同的CPU上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。 适用场景值得说明的是对volatile变量的单次读/写操作可以保证原子性的，如long和double类型变量， 但是并不能保证i++这种操作的原子性，因为本质上i++是读、写两次操作。在某些场景下可以 代替Synchronized。但是,volatile的不能完全取代Synchronized的位置，只有在一些特殊的场景下，才能适用volatile。总的来说，必须同时满足下面两个条件才能保证在并发环境的线程安 全： 对变量的写操作不依赖于当前值（比如 i++），或者说是单纯的变量赋值（boolean flag = true）。 该变量没有包含在具有其他变量的不变式中，也就是说，不同的volatile变量之间，不 能互相依赖。只有在状态真正独立于程序内其他内容时才能使用 volatile 与Synchronized区别 3.2.2 SynchronizedSynchronized属于独占式的悲观锁，同时属于可重 入锁。作用范围 作用于方法时，锁住的是对象的实例(this)； 当作用于静态方法时，锁住的是Class实例，又因为Class的相关数据存储在永久带PermGen （jdk1.8 则是 metaspace），永久带是全局共享的，因此静态方法锁相当于类的一个全局锁， 会锁所有调用该方法的线程； synchronized 作用于一个对象实例时，锁住的是所有以该对象为锁的代码块。它有多个队列， 当多个线程一起访问某个对象监视器的时候，对象监视器会将这些线程存储在不同的容器中。 核心组件 Wait Set：调用wait方法被阻塞的线程被放置在这里； Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：Contention List中那些有资格成为候选资源的线程被移动到Entry List中； OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck； Owner：当前已经获取到所有资源的线程被称为Owner； !Owner：当前释放锁的线程。 实现 JVM 每次从队列的尾部取出一个数据用于锁竞争候选者（OnDeck），但是并发情况下， ContentionList会被大量的并发线程进行CAS访问，为了降低对尾部元素的竞争，JVM会将 一部分线程移动到EntryList中作为候选竞争线程。 Owner 线程会在 unlock 时，将 ContentionList 中的部分线程迁移到 EntryList 中，并指定 EntryList中的某个线程为OnDeck线程（一般是先进去的那个线程）。 Owner 线程并不直接把锁传递给 OnDeck 线程，而是把锁竞争的权利交给 OnDeck， OnDeck需要重新竞争锁。这样虽然牺牲了一些公平性，但是能极大的提升系统的吞吐量，在 JVM中，也把这种选择行为称之为“竞争切换”。 OnDeck线程获取到锁资源后会变为Owner线程，而没有得到锁资源的仍然停留在EntryList 中。如果Owner线程被wait方法阻塞，则转移到WaitSet队列中，直到某个时刻通过notify 或者notifyAll唤醒，会重新进去EntryList中。 处于 ContentionList、EntryList、WaitSet 中的线程都处于阻塞状态，该阻塞是由操作系统 来完成的（Linux内核下采用pthread_mutex_lock内核函数实现的）。 Synchronized是非公平锁。 Synchronized在线程进入ContentionList时，等待的线程会先 尝试自旋获取锁，如果获取不到就进入 ContentionList，这明显对于已经进入队列的线程是 不公平的，还有一个不公平的事情就是自旋获取锁的线程还可能直接抢占 OnDeck 线程的锁 资源。 参考：https://blog.csdn.net/zqz_zqz/article/details/70233767 每个对象都有个 monitor 对象，加锁就是在竞争 monitor 对象，代码块加锁是在前后分别加 上monitorenter和monitorexit指令来实现的，方法加锁是通过一个标记位来判断的 synchronized 是一个重量级操作，需要调用操作系统相关接口，性能是低效的，有可能给线 程加锁消耗的时间比有用操作消耗的时间更多。 Java1.6，synchronized进行了很多的优化，有适应自旋、锁消除、锁粗化、轻量级锁及偏向 锁等，效率有了本质上的提高。在之后推出的 Java1.7 与 1.8 中，均对该关键字的实现机理做 了优化。引入了偏向锁和轻量级锁。都是在对象头中有标记位，不需要经过操作系统加锁。 锁可以从偏向锁升级到轻量级锁，再升级到重量级锁。这种升级过程叫做锁膨胀； 双重检验所锁实现单例模式 3.2.3 RetreenLockReentantLock 继承接口 Lock 并实现了接口中定义的方法，他是一种可重入锁，除了能完 成 synchronized 所能完成的所有工作外，还提供了诸如可响应中断锁、可轮询锁请求、定时锁等 避免多线程死锁的方法。 Lock主要方法 void lock(): 执行此方法时, 如果锁处于空闲状态, 当前线程将获取到锁. 相反, 如果锁已经 被其他线程持有, 将禁用当前线程, 直到当前线程获取到锁. boolean tryLock()：如果锁可用, 则获取锁, 并立即返回 true, 否则返回 false. 该方法和 lock()的区别在于, tryLock()只是”试图”获取锁, 如果锁不可用, 不会导致当前线程被禁用, 当前线程仍然继续往下执行代码. 而 lock()方法则是一定要获取到锁, 如果锁不可用, 就一 直等待, 在未获得锁之前,当前线程并不继续向下执行. void unlock()：执行此方法时, 当前线程将释放持有的锁. 锁只能由持有者释放, 如果线程 并不持有锁, 却执行该方法, 可能导致异常的发生. Condition newCondition()：条件对象，获取等待通知组件。该组件和当前的锁绑定， 当前线程只有获取了锁，才能调用该组件的 await()方法，而调用后，当前线程将缩放锁。 getHoldCount() ：查询当前线程保持此锁的次数，也就是执行此线程执行lock方法的次 数。 getQueueLength（）：返回正等待获取此锁的线程估计数，比如启动 10 个线程，1 个 线程获得锁，此时返回的是9 getWaitQueueLength：（Condition condition）返回等待与此锁相关的给定条件的线 程估计数。比如 10 个线程，用同一个 condition 对象，并且此时这 10 个线程都执行了 condition对象的 await方法，那么此时执行此方法返回10 hasWaiters(Condition condition)：查询是否有线程等待与此锁有关的给定条件 (condition)，对于指定contidion对象，有多少线程执行了condition.await方法 hasQueuedThread(Thread thread)：查询给定线程是否等待获取此锁 hasQueuedThreads()：是否有线程等待此锁 isFair()：该锁是否公平锁 isHeldByCurrentThread()： 当前线程是否保持锁锁定，线程的执行 lock 方法的前后分 别是false和true isLock()：此锁是否有任意线程占用 lockInterruptibly（）：如果当前线程未被中断，获取锁 tryLock（）：尝试获得锁，仅在调用时锁未被线程占用，获得锁 tryLock(long timeout TimeUnit unit)：如果锁在给定等待时间内没有被另一个线程保持， 则获取该锁 ReentrantLock实现 public class MyService &#123; private Lock lock = new ReentrantLock(); //Lock lock=new ReentrantLock(true);//公平锁 //Lock lock=new ReentrantLock(false);//非公平锁 private Condition condition=lock.newCondition();//创建Condition public void testMethod() &#123; try &#123; lock.lock();//lock加锁 //1：wait 方法等待： //System.out.println(\"开始wait\"); condition.await(); //通过创建Condition对象来使线程wait，必须先执行lock.lock方法获得锁 //:2：signal方法唤醒 condition.signal();//condition对象的signal方法可以唤醒wait 线程 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"ThreadName=\" + Thread.currentThread().getName()+ (\" \" + (i + 1))); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; Condition类与Object类区别 Condition类的awiat方法和Object类的wait方法等效 Condition类的signal方法和Object类的notify方法等效 Condition类的signalAll方法和Object类的notifyAll方法等效 ReentrantLock类可以唤醒指定条件的线程，而object的唤醒是随机的 tryLock与 lock 和 lockInterruptibly区别 tryLock能获得锁就返回true，不能就立即返回false，tryLock(long timeout,TimeUnit unit)，可以增加时间限制，如果超过该时间段还没获得锁，返回false lock能获得锁就返回true，不能的话一直等待获得锁 lock 和 lockInterruptibly，如果两个线程分别执行这两个方法，但此时中断这两个线程， lock不会抛出异常，而lockInterruptibly会抛出异常 3.2.4 ReentrantLock与synchronized区别 ReentrantLock 通过方法lock()与unlock()来进行加锁与解锁操作，与synchronized会 被 JVM 自动解锁机制不同，ReentrantLock 加锁后需要手动进行解锁。为了避免程序出 现异常而无法正常解锁的情况，使用 ReentrantLock 必须在 finally 控制块中进行解锁操 作。 ReentrantLock相比synchronized的优势是可中断、公平锁、多个锁。这种情况下需要 使用ReentrantLock。3.2.5 ReentrantReadWriteLock为了提高性能，Java 提供了读写锁，在读的地方使用读锁，在写的地方使用写锁，灵活控制，如 果没有写锁的情况下，读是无阻塞的,在一定程度上提高了程序的执行效率。读写锁分为读锁和写 锁，多个读锁不互斥，读锁与写锁互斥，这是由jvm自己控制的，你只要上好相应的锁即可。 读锁如果你的代码只读数据，可以很多人同时读，但不能同时写，那就上读锁 写锁如果你的代码修改数据，只能有一个人在写，且不能同时读取，那就上写锁。 总之，读的时候上 读锁，写的时候上写锁！Java 中读写锁有个接口 java.util.concurrent.locks.ReadWriteLock ，也有具体的实现 ReentrantReadWriteLock 3.3 锁优化这里的锁优化主要是指 JVM 对 synchronized 的优化。 3.3.1 减少锁持有时间 只用在有线程安全要求的程序上加锁 3.3.2 减小锁粒度 将大对象（这个对象可能会被很多线程访问），拆成小对象，大大增加并行度，降低锁竞争。 降低了锁的竞争，偏向锁，轻量级锁成功率才会提高。典型的减小锁粒度的案例就是 ConcurrentHashMap。 3.3.3 锁分离 常见的锁分离就是读写锁ReadWriteLock，根据功能进行分离成读锁和写锁，这样读读不互 斥，读写互斥，写写互斥，即保证了线程安全，又提高了性能，具体也请查看[高并发Java 五] JDK并发包1。读写分离思想可以延伸，只要操作互不影响，锁就可以分离。比如 LinkedBlockingQueue 从头部取出，从尾部放数据 3.3.4 自旋锁互斥同步进入阻塞状态的开销都很大，应该尽量避免。在许多应用中，共享数据的锁定状态只会持续很短的一段时间。自旋锁的思想是让一个线程在请求一个共享数据的锁时执行忙循环（自旋）一段时间，如果在这段时间内能获得锁，就可以避免进入阻塞状态。 自旋锁虽然能避免进入阻塞状态从而减少开销，但是它需要进行忙循环操作占用 CPU 时间，它只适用于共享数据的锁定状态很短的场景。 在 JDK 1.6 中引入了自适应的自旋锁。自适应意味着自旋的次数不再固定了，而是由前一次在同一个锁上的自旋次数及锁的拥有者的状态来决定。 3.3.5 锁消除锁消除是指对于被检测出不可能存在竞争的共享数据的锁进行消除。 锁消除主要是通过逃逸分析来支持，如果堆上的共享数据不可能逃逸出去被其它线程访问到，那么就可以把它们当成私有数据对待，也就可以将它们的锁进行消除。 对于一些看起来没有加锁的代码，其实隐式的加了很多锁。例如下面的字符串拼接代码就隐式加了锁： public static String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; String 是一个不可变的类，编译器会对 String 的拼接自动优化。在 JDK 1.5 之前，会转化为 StringBuffer 对象的连续 append() 操作： public static String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; 每个 append() 方法中都有一个同步块。虚拟机观察变量 sb，很快就会发现它的动态作用域被限制在 concatString() 方法内部。也就是说，sb 的所有引用永远不会逃逸到 concatString() 方法之外，其他线程无法访问到它，因此可以进行消除。 3.3.6 锁粗化如果一系列的连续操作都对同一个对象反复加锁和解锁，频繁的加锁操作就会导致性能损耗。 上一节的示例代码中连续的 append() 方法就属于这类情况。如果虚拟机探测到由这样的一串零碎的操作都对同一个对象加锁，将会把加锁的范围扩展（粗化）到整个操作序列的外部。对于上一节的示例代码就是扩展到第一个 append() 操作之前直至最后一个 append() 操作之后，这样只需要加锁一次就可以了。 3.4 四种锁状态锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。 3.4.1 重量级锁Synchronized 是通过对象内部的一个叫做监视器锁（monitor）来实现的。但是监视器锁本质又 是依赖于底层的操作系统的Mutex Lock来实现的。而操作系统实现线程之间的切换这就需要从用 户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么 Synchronized 效率低的原因。因此，这种依赖于操作系统 Mutex Lock 所实现的锁我们称之为 “重量级锁”。JDK中对Synchronized做的种种优化，其核心都是为了减少这种重量级锁的使用。 JDK1.6 以后，为了减少获得锁和释放锁所带来的性能消耗，提高性能，引入了“轻量级锁”和 “偏向锁”。 3.4.2 轻量级锁“轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的，它使用 CAS 操作来避免重量级锁使用互斥量的开销。但是，首先需要强调一点的是， 轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量 级锁使用产生的性能消耗。在解释轻量级锁的执行过程之前，先明白一点，轻量级锁所适应的场 景是线程交替执行同步块的情况（整个同步周期内都是不存在竞争的，因此也就不需要都使用互斥量进行同步，可以先采用 CAS 操作进行同步），如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀 为重量级锁（ CAS 失败了再改用互斥量进行同步）。 3.4.3 偏向锁偏向锁的目的是在某个线程获得锁之后，消除这个线程锁重入（CAS）的开销，看起 来让这个线程得到了偏护。这个线程在之后获取该锁就不再需要进行同步操作，甚至连 CAS 操作也不再需要。轻 量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进 一步提高性能。 3.4.4 无锁四、CAS（比较并交换） 比较并交换(compare and swap, CAS)，是原子操作的一种，可用于在多线程编程中实现不被打断的数据交换操作，从而避免多线程同时改写某一数据时由于执行顺序不确定性以及中断的不可预知性产生的数据不一致问题。 该操作通过将内存中的值与指定数据进行比较，当数值一样时将内存中的数据替换为新的值。 它包含 3 个参数 CAS(V,E,N)。V 表示要更新的变量(内存值)，E 表示预期值(旧的)，N 表示新值。当且仅当 V 值等于 E 值时，才会将 V 的值设为 N；如果 V 值和 E 值不同，则说明已经有其他线程做了更新，则当 前线程什么都不做。最后，CAS返回当前V的真实值。 CAS 操作是抱着乐观的态度进行的(乐观锁)，它总是认为自己可以成功完成操作。当多个线程同时 使用 CAS 操作一个变量时，只有一个会胜出，并成功更新，其余均会失败。失败的线程不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。基于这样的原理， CAS操作即使没有锁，也可以发现其他线程对当前线程的干扰，并进行恰当的处理。 4.1 CAS失败的两种情况失败放弃 失败自旋（循环再试） 4.2 原子包Atomic原子变量类在java.util.concurrent.atomic包下我们可以对其进行分类： 基本类型： AtomicBoolean：布尔型 AtomicInteger：整型 AtomicLong：长整型 数组： AtomicIntegerArray：数组里的整型 AtomicLongArray：数组里的长整型 AtomicReferenceArray：数组里的引用类型 引用类型： AtomicReference：引用类型 AtomicStampedReference：带有版本号的引用类型 AtomicMarkableReference：带有标记位的引用类型 对象的属性： AtomicIntegerFieldUpdater：对象的属性是整型 AtomicLongFieldUpdater：对象的属性是长整型 AtomicReferenceFieldUpdater：对象的属性是引用类型 JDK8新增DoubleAccumulator、LongAccumulator、DoubleAdder、LongAdder是对AtomicLong等类的改进。比如LongAccumulator与LongAdder在高并发环境下比AtomicLong更高效。 AtomicInteger实例 class Count&#123; // 共享变量(使用AtomicInteger来替代Synchronized锁) private AtomicInteger count = new AtomicInteger(0); public Integer getCount() &#123; return count.get(); &#125; public void increase() &#123; count.incrementAndGet(); &#125;&#125; Atomic包里的类基本都是使用Unsafe实现的包装类。Unsafe里边有几个我们喜欢的方法(CAS)： // 第一和第二个参数代表对象的实例以及地址，第三个参数代表期望值，第四个参数代表更新值public final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6); 从原理上概述就是：Atomic包的类的实现绝大部分调用Unsafe的方法，而Unsafe底层实际上是调用C代码，C代码调用汇编，最后生成出一条CPU指令cmpxchg，完成操作。这也就为啥CAS是原子性的，因为它是一条CPU指令，不会被打断。 compareAndSwapInt实例 public class AtomicInteger extends Number implements java.io.Serializable &#123; private volatile int value; public final int get() &#123; return value; &#125; public final int getAndIncrement() &#123; for (;;) &#123; //CAS自旋，一直尝试，直达成功 int current = get(); int next = current + 1; if (compareAndSet(current, next)) return current; &#125; &#125; public final boolean compareAndSet(int expect, int update) &#123; return unsafe.compareAndSwapInt(this, valueOffset, expect, update); &#125; &#125; 4.3 ABA问题CAS 会导致“ABA 问题”。CAS 算法实现一个重要前提需要取出内存中某时刻的数据，而在下时 刻比较并替换，那么在这个时间差类会导致数据的变化。 描述比如说一个线程 one 从内存位置 V 中取出 A，这时候另一个线程 two 也从内存中取出 A，并且 two进行了一些操作变成了B，然后two又将V位置的数据变成A，这时候线程one进行CAS操 作发现内存中仍然是A，然后one操作成功。尽管线程one的CAS操作成功，但是不代表这个过 程就是没有问题的。 解决部分乐观锁的实现是通过版本号（version）的方式来解决ABA问题，乐观锁每次在执行数据的修 改操作时，都会带上一个版本号，一旦版本号和数据的版本号一致就可以执行修改操作并对版本 号执行+1 操作，否则就执行失败。因为每次操作的版本号都会随之增加，所以不会出现 ABA 问 题，因为版本号只会增加不会减少。 五、AQS（抽象队列同步器）AbstractQueuedSynchronizer类如其名，抽象的队列式的同步器，AQS定义了一套多线程访问 共享资源的同步器框架，许多同步类实现都依赖于它，Lock的子类实现都是基于AQS的。如常用的 ReentrantLock/Semaphore/CountDownLatch。 5.1 原理通读了一遍，可以总结出以下比较关键的信息： AQS其实就是一个可以给我们实现锁的框架 内部实现的关键是：先进先出的队列、state状态 定义了内部类ConditionObject 拥有两种线程模式 独占模式 共享模式 在LOCK包中的相关锁(常用的有ReentrantLock、ReadWriteLock)都是基于AQS来构建 一般我们叫AQS为同步器 六、同步工具类Java为我们提供了三个同步工具类： CountDownLatch(闭锁、线程计数器) CyclicBarrier(栅栏) Semaphore(信号量)这几个工具类是为了能够更好控制线程之间的通讯问题 6.1 CountDownLatchCountDownLatch是一个同步的辅助类，允许一个或多个线程一直等待，直到其它线程完成它们的操作。（任务A需要等待其它4个任务完成才能执行）它常用的API其实就两个:await()和countDown()用法 public class Test &#123; public static void main(String[] args) &#123; final CountDownLatch countDownLatch = new CountDownLatch(5); // 3y线程启动 new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 这里调用的是await()不是wait() countDownLatch.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"等待其它线程执行完毕才开始\"); &#125; &#125;).start(); // 其他线程启动 for (int i = 0; i &lt; 5; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println(\"其它线程执行完毕\"); countDownLatch.countDown(); &#125; &#125;).start(); &#125; &#125;&#125; 6.2 CyclicBarrierCyclicBarrier允许一组线程互相等待，直到到达某个公共屏障点。叫做cyclic是因为当所有等待线程都被释放以后，CyclicBarrier可以被重用(对比于CountDownLatch是不能重用的) 用法CountDownLatch注重的是等待其他线程完成，CyclicBarrier注重的是：当线程到达某个状态后，暂停下来等待其他线程，所有线程均到达以后，继续执行。 import java.util.concurrent.BrokenBarrierException;import java.util.concurrent.CyclicBarrier;public class Test &#123; public static void main(String[] args) &#123; final CyclicBarrier CyclicBarrier = new CyclicBarrier(2); for (int i = 0; i &lt; 2; i++) &#123; new Thread(() -&gt; &#123; String name = Thread.currentThread().getName(); if (name.equals(\"Thread-0\")) &#123; name = \"线程0\"; &#125; else &#123; name = \"线程1\"; &#125; System.out.println(name + \"等待执行\"); try &#123; CyclicBarrier.await(); System.out.println(name+\"可以开始执行\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (BrokenBarrierException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; &#125;&#125; 6.3 SemaphoreSemaphore(信号量)实际上就是可以控制同时访问的线程个数，它维护了一组”许可证”。 当调用acquire()方法时，会消费一个许可证。如果没有许可证了，会阻塞起来 当调用release()方法时，会添加一个许可证。 这些”许可证”的个数其实就是一个count变量 用法 public class Test &#123; public static void main(String[] args) &#123; // 假设有50个同时来到酸奶店门口 int nums = 50; // 酸奶店只能容纳10个人同时挑选酸奶 Semaphore semaphore = new Semaphore(10); for (int i = 0; i &lt; nums; i++) &#123; int finalI = i; new Thread(() -&gt; &#123; try &#123; // 有\"号\"的才能进酸奶店挑选购买 semaphore.acquire(); System.out.println(\"顾客\" + finalI + \"在挑选商品，购买...\"); // 假设挑选了xx长时间，购买了 Thread.sleep(1000); // 归还一个许可，后边的就可以进来购买了 System.out.println(\"顾客\" + finalI + \"购买完毕了...\"); semaphore.release(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;).start(); &#125; &#125;&#125; 6.4 总结Java为我们提供了三个同步工具类： CountDownLatch(闭锁)某个线程等待其他线程执行完毕后，它才执行(其他线程等待某个线程执行完毕后，它才执行) CyclicBarrier(栅栏)一组线程互相等待至某个状态，这组线程再同时执行。 Semaphore(信号量)控制一组线程同时执行。七、ThreadLocalThreadLocal 提供了线程的局部变量，每个线程都可以通过set()和get()来对这个局部变量进行操作，但不会和其他线程的局部变量进行冲突，实现了线程的数据隔离。 ThreadLocal设计的目的是为了能够在当前线程中有属于自己的变量，并不是为了解决并发或者共享变量的问题 7.1 实现原理Set和Get方法 public void set(T value) &#123; // 得到当前线程对象 Thread t = Thread.currentThread(); // 这里获取ThreadLocalMap ThreadLocalMap map = getMap(t); // 如果map存在，则将当前线程对象t作为key，要存储的对象作为value存到map里面去 if (map != null) map.set(this, value); else createMap(t, value); &#125; public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; 通过currentThread()方法可以获得Thread对象t，通过getMap(t)就可以获得当前的ThreadLocalMap对象 static class ThreadLocalMap &#123; static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; //....很长&#125; ThreadLocalMap是ThreadLocal的一个内部类。用Entry类来进行存储。我们的值都是存储到这个Map上的，key是当前ThreadLocal对象，而value为要存储的对象！ 7.2 内存泄漏问题ThreadLocal的对象关系引用图： 另一种说法：ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 想要避免内存泄露就要手动remove() 7.3 总结 每个Thread维护着一个ThreadLocalMap的引用 ThreadLocalMap是ThreadLocal的内部类，用Entry来进行存储 调用ThreadLocal的set()方法时，实际上就是往ThreadLocalMap设置值，key是ThreadLocal对象，值是传递进来的对象 调用ThreadLocal的get()方法时，实际上就是往ThreadLocalMap获取值，key是ThreadLocal对象 ThreadLocal本身并不存储值，它只是作为一个key来让线程从ThreadLocalMap获取value。 正因为这个原理，所以ThreadLocal能够实现“数据隔离”，获取当前线程的局部变量值，不受其他线程影响。","categories":[{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"多线程","slug":"Java/多线程","permalink":"http://hofe.work/categories/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"http://hofe.work/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"项目开发记录","slug":"微服务分布式集群/Dubbo项目开发记录","date":"2020-04-14T16:00:00.000Z","updated":"2020-04-27T07:34:22.362Z","comments":true,"path":"2020/04/15/微服务分布式集群/Dubbo项目开发记录/","link":"","permalink":"http://hofe.work/2020/04/15/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/Dubbo%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95/","excerpt":"","text":"应该说这个项目完成的不是特别理想，有一些模块还没完成，但单点登录、redis缓存、购物车的设计部分值得参考，以及项目如何分包，结构的描述都挺详细的，对于微服务分布式入门来说，还是挺有价值的。可以访问我个人网站hofe 的个人网站，阅读体验更佳，需要源代码，有疑问的也可在评论区留言 项目介绍用到的技术 微服务架构：Dubbo+zookeeper 中间件：SpringBoot+SpringMVC+Sprint+MyBatis 数据库：MySQL 分布式文件系统：FastDFS 搜索引擎：Solr 分布式缓存：Redis 消息中间件：RabbitMQ 一、项目骨架搭建1.1 新建一个maven项目以及多个maven模块只作为一个目录使用，不需要配置pom，可以删除src文件。 如在dubbo-shop项目下新建shop-service模块，将Parent设为none。同样步骤创建shop-basic、shop-api、shop-web模块。 1.1.1 在maven模块下新建maven模块在shop-basic模块中新建entity，需要注意的是entity模块目录在shop-basic至下。同理创建common、mapper模块 1.1.2 在maven模块下新建springboot模块在shop-service下新建springboot模块。 1.2 初始项目结构dubbo-shop shop-api shop-service profuct-service（SpringBoot） shop-web backend（SpringBoot） shop-basic entity common 存放公共组件如dao的泛型接口 dao 1.3 basic中的公用组件1.3.1 common组件IBaseDao和IBaseService由于增删查改等操作比较频繁且可重用性大，故在common模块中新建com.hofe.common.base目录，新建dao与service接口；在com.hofe.common.impl中实现service接口的抽象类。dao与service接口代码相同 public interface IBaseDao&lt;T&gt; &#123; T queryById(Long id); List&lt;T&gt; queryAllByLimit(int offset, int limit); List&lt;T&gt; queryAll(); int insert(T record); int update(T record); int deleteById(Long id);&#125; serviceImpl由于使用泛型，无法返回准确类型，故将其abstract抽象，由继承的子类返回类型。 import com.hofe.common.base.IBaseDao;import com.hofe.common.base.IBaseService;import java.util.List;public abstract class BaseServiceImpl&lt;T&gt; implements IBaseService&lt;T&gt; &#123; public abstract IBaseDao&lt;T&gt; getBaseDao(); public T queryById(Long id) &#123; return getBaseDao().queryById(id); &#125; public List&lt;T&gt; queryAllByLimit(int offset, int limit) &#123; return getBaseDao().queryAllByLimit(offset, limit); &#125; public List&lt;T&gt; queryAll()&#123; return getBaseDao().queryAll(); &#125; public int insert(T record) &#123; return getBaseDao().insert(record); &#125; public int update(T record) &#123; return getBaseDao().update(record); &#125; public int deleteById(Long id) &#123; return getBaseDao().deleteById(id); &#125; 1.4 记录下用到的端口号dubbo管理中心端口：2180zookeeper端口：2181fdfs文件服务器：tracker server port: 22122storage_server: 23000RabbitMQ: 15672(admin:admin) 二、商品类别接口服务2.1 basic模块配置这一模块我后来回头来看，其实不是特别需要，虽然重用了很多代码，但对小规模的项目来说，可以不需要，不利于项目的维护。 2.4.1 entity配置可以用easycode插件根据数据库生成实体类。注意类需要实现Serializable接口 2.4.2 dao配置dao类 public interface TProductTypeDao extends IBaseDao&lt;TProductType&gt;&#123;&#125; 由于IBaseService有具体实现故这里可以省略。 dao.xml这里需要注意的是xml配置的路径是否正确、是否在bean扫描路径下、表字段名name、desc会冲突等问题pom配置由于用到entity、common中的基本类，故需引入模块； &lt;dependencies&gt; &lt;!--集成mybatis--&gt; &lt;!--集成事务--&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.47&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;entity&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.2 api接口模块配置 api服务接口 集成common中的基本业务接口，后续该接口由Service实现。 import com.hofe.common.base.IBaseService;import com.hofe.entity.TProductType;// 商品类别接口, 集成common中的Service，通过泛型确定类，再由service模块实现api模块的这个接口public interface IProductTypeService extends IBaseService&lt;TProductType&gt;&#123;&#125; pom配置由于需要TProductType、IBaseService故引入entity、common依赖。 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;common&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;entity&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.3 服务提供者配置启动类加入@EnableDubbo注解以及需要加入@MapperScan(“com.hofe.dao”)注解 @SpringBootApplication@EnableDubbo@MapperScan(\"com.hofe.dao\")public class ProductServiceApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ProductServiceApplication.class, args); &#125;&#125; 接口Service实现类依赖Dao层，调用相应dao执行具体业务。这里的getBaseDao()方法用于dao被调用时返回具体dao类型（抽象类、接口用的泛型） @Component@Service // dubbo的servicepublic class ProductTypeService extends BaseServiceImpl&lt;TProductType&gt; implements IProductTypeService &#123; @Autowired private TProductTypeDao productTypeDao; @Override public IBaseDao&lt;TProductType&gt; getBaseDao() &#123; return productTypeDao; &#125;&#125; application.yml server: port: 8080dubbo: application: name: product-service registry: protocol: zookeeper address: ip:2181 protocol: 28801spring: datasource: # 数据库配置 driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://ip:3306/dubbo_shop?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10 username: root password: *** hikari: maximum-pool-size: 10 # 最大连接池数 max-lifetime: 1770000mybatis: # 指定别名设置的包为所有entity type-aliases-package: cn.hofe.entity configuration: map-underscore-to-camel-case: true # 驼峰命名规范 mapper-locations: # mapper映射文件位置 - classpath:mapper/*.xml pom配置加入api、dao依赖包也会加入它们各自的依赖包；所以即使用到mybatis也可以不用引入依赖。 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Dubbo依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;product-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;dao&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2.3 服务消费者配置启动类在启动类中添加@EnableDubbo注解 import com.alibaba.dubbo.config.spring.context.annotation.EnableDubbo;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@EnableDubbo@SpringBootApplicationpublic class BackendApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(BackendApplication.class, args); &#125;&#125; controller在Controller中依赖处添加@Reference应用。将由服务提供者的Service执行具体业务。 import com.alibaba.dubbo.config.annotation.Reference;import com.hofe.api.product.IProductTypeService;import com.hofe.entity.TProductType;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.List;@RestController@RequestMapping(\"productType\")public class ProductTypeController &#123; @Reference private IProductTypeService productTypeService; // 引用 Servic模块的服务 @GetMapping(\"list\") public List&lt;TProductType&gt; productTypeList()&#123; return productTypeService.queryAll(); &#125;&#125; application.yml server: port: 9090dubbo: application: name: backend registry: protocol: zookeeper address: ip:2181 pom配置由于需要用到服务消费者故需引入api模块 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Dubbo依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.boot&lt;/groupId&gt; &lt;artifactId&gt;dubbo-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;0.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;product-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 三、RabbitMQ消息队列（未完成）3.1 消息队列的应用场景异步、削峰、解耦。详情点击(1) 通过异步处理提高系统性能（削峰、减少响应所需时间）比如秒杀背景下的商品下单，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即返回，再由消息队列的消费者进程从消息队列中获取数据，异步的对数据库进行操作。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 (2) 降低系统耦合性 我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 通过将系统同步交互改为异步交互提高系统的处理的吞吐量发送者：1、声明交换机2、发送消息到交换机 接受者者：1、声明队列2、建立队列与交换机的绑定关系3、创建一个类的方法来监听队列，接收消息 四、单点登陆4.1 登录方案设计4.1.1 前后端未分离下的登陆系统的设计 Session服务端提供了一种叫 Session 的机制，对于每个用户的请求，会生成一个唯一的标识。当程序需要为某个客户端的请求创建一个 session 的时候，服务器首先检查这个客户端的请求是否包含了一个 session 标识（session id）。如果已包含一个 session id 则说明以前已经为客户端创建过 session，服务器就按照 session id 把这个 session 检索出来使用。如果客户端请求不包含 session id，则为此客户端创建一个session 并且生成一个与此 session 相关联的 session id，session id 的值是一个既不会重复，又不容易被找到规律的字符串。Cookie浏览器提供了一种叫 cookie 的机制，保存当前会话的唯一标识。每次 HTTP 请求，客户端都会发送相应的 cookie 信息到服务端。客户端第一次请求，由于 cookie 中并没有携带 session id，服务端会创建一个session id，并写入到客户端的 cookie 中。以后每次请求，客户端都会携带这个 id 发给服务器端。这样一来，便解决了无状态的问题。 通过设置浏览器的cookie（sessionid），映射服务器的用户session来判断用户登录状况：每次点击登录按钮时后台的response将sessionid的cookie添加到浏览器，后面所有访问请求都带着该cookie，shiro的相关方法就会获取cookie匹配session确定用户登录信息。 4.1.2前后端分离下的登陆系统设计 前后端分离的核心概念是后端仅返回前端所需的数据，不再渲染HTML页面，前端HTML页面通过AJAX调用后端的RESTFUL API接口并使用JSON数据进行交互 目前大多数都采用请求头携带 Token 的形式。 1、首次登录时，后端服务器判断用户账号密码正确之后，根据用户id、用户名、定义好的秘钥、过期时间 生成 token ，返回给前端2、前端拿到后端返回的 token ,存储在 localStroage 里3、前端每次路由跳转， 判断 localStroage 有无 token ，没有则跳转到登录页，有则请求获取用户信息，改变登录状态4、每次请求接口，在 请求头里携带 token5、后端接口 判断 请求头有无 token，没有或者 token 过期，返回4016、前端得到 401 状态码，重定向 到登录页面 4.1.3 微服务分布式下的登陆系统设计单点登录就是在A系统登录以后，跳转到B系统，此时可以直接访问B系统的资源，即只需要登录一次，就可以访问其他相互信任的应用系统，免除多次登录的烦恼。实现单点登录说到底就是要解决如何产生和存储那个信任，再就是其他系统如何验证这个信任的有效性，因此要点也就以下两个：存储信任、验证信任。 4.2 服务消费者配置SSOController.java用户登录通过前端传递的user信息，与数据库交互判断是否合法，若合法生成uuid，创建名为”user_token”，内容为uuid的cookie，设置cookie的域名为父域名，这样所有子域名系统都可以访问该cookie，解决cookie的跨域问题等等，并将其通过HttpServletResponse添加cookie到浏览器中。同时，通过redisTemplate设置key为user:token:uuid，值为该用户信息的redis数据，并设置过期时间，这样redis服务器就会保存本次登陆的信息。 @PostMapping(\"checkLogin\") public String checkLogin(TUser user, HttpServletResponse response)&#123; TUser currentUser = userService.checkLogin(user); if(currentUser == null)&#123; return \"fail\"; &#125; String uuid = UUID.randomUUID().toString(); // 在浏览器中可以看到user_token对应的uuid Cookie cookie = new Cookie(\"user_token\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); // redis中的key位user:token:uuid StringBuilder redisKey = new StringBuilder(\"user:token:\").append(uuid); redisTemplate.setValueSerializer(new StringRedisSerializer()); // 客户端得到uuid，就可以用user:token:uuid从redis中取得currentUser信息 redisTemplate.opsForValue().set(redisKey, currentUser); //设置有效期 redisTemplate.expire(redisKey.toString(), 30, TimeUnit.MINUTES); response.addCookie(cookie); return \"login success\"; &#125; 判断是否登录通过HttpServletRequest获取请求报文带来的cookies信息，判断是否包含user_token，若包含则获取该cookie对应的uuid，通过键user:token:uuid获取redis保存的用户信息，并刷新其有效期。 @PostMapping(\"checkIsLogin\") public String checkIsLogin(HttpServletRequest request)&#123; Cookie[] cookies = request.getCookies(); if(cookies == null)&#123; return \"用户未登陆\"; &#125; for (Cookie cookie : cookies)&#123; if(\"user_token\".equals(cookie.getName()))&#123; String uuid = cookie.getValue(); StringBuilder redisKey = new StringBuilder(\"user:token:\").append(uuid); TUser currentUser = (TUser) redisTemplate.opsForValue().get(redisKey.toString()); if(currentUser != null)&#123; //刷新有效期 redisTemplate.expire(redisKey.toString(), 30, TimeUnit.MINUTES); return currentUser.getUsername(); &#125; &#125; &#125; return \"用户为登陆\"; &#125; 注销通过uuid新建cookie，并设置其失效，重新添加回浏览器的客户端，同时也要删除其对应的redis数据。 @PostMapping(\"logout\") public String logout(@CookieValue(name = \"user_token\", required = false) String uuid, HttpServletResponse response)&#123; if(uuid == null)&#123; return \"注销失败\"; &#125; Cookie cookie = new Cookie(\"user_token\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); // 使cookie失效 cookie.setMaxAge(0); response.addCookie(cookie); // 删除redis StringBuilder redisKey = new StringBuilder(\"user:token:\").append(uuid); redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.delete(redisKey.toString()); return \"注销成功\"; &#125; pom配置与普通的消费者配置差不多，只是多了redis引入。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; application.yml配置 server: port: 9099dubbo: application: name: sso registry: protocol: zookeeper address: ip:2181 consumer: timeout: 30000spring: redis: host: ip port: 6379 jedis: pool: max-active: 20 4.3 服务提供者配置UserService.java和普通的提供者一样，数据库的增删查改操作。 application.yml配置 dubbo: application: name: user-service registry: protocol: zookeeper address: ip:2181 protocol: 28808 provider: timeout: 30000spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://ip:3306/dubbo_shop?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10 username: root password: 123456 hikari: maximum-pool-size: 10 max-lifetime: 1770000mybatis: # entity路径 type-aliases-package: com.hofe configuration: map-underscore-to-camel-case: true mapper-locations: # mapper资源路径 - classpath:mapper/*.xml 五、购物车5.1 购物车方案设计需要实现以下功能：1、未登录时可操作购物车且退出浏览器之后依然存在信息2、未登录时购物车支持增删查改、排序商品3、登陆时自动合并未登录与登陆时的商品，且未登录时购物车商品将清空。4、商品信息变更，购物车也需变更5、已登录购物车在任何地方信息都一样 未登录购物车实现方案一：cookie用list保存购物车中的商品信息和对应数量的对象，再将其转化为json，最后用cookie保存购物车和list的键值对。但如果商品信息变更，cookie保存的是商品的信息，则无法修改，故最好改成商品id与数量的对应关系。不足：1、展示购物车需要查看数据库，可以把20%的商品放到缓存中（缓存预热）2、cookie的物理上限4k3、更新删除操作需要遍历 未登录购物车实现方案二：cookie+redis购物车保存信息：每一项（id+数量+操作时间），购物车包含多项。后端使用redis存储购物车信息，并做热门商品的缓存。前端cookie保存信息简化，只作保存凭证（user_cart—-uuid）。这样后端也省略了json转换。cookie不是会话级cookie，否则退出浏览器即消失。不足：更新需要遍历 已登录购物车方案一：数据库每个用户一辆购物车，字段需要有id\\user_id\\product_id\\count\\update，每行是一条购物车记录，user_id和product_id是唯一约束。 已登录购物车方案二：redis每个用户一辆购物车，user:cart:uuid改为user:cart:userid控制层通过判断用户是否登录决定用user:cart:uuid还是user:cart:userid，从客户端到controller之间可以加个拦截器，任何时候都放行，只记录状态，放入request中。好处：性能坏处：虽然是持久化机制，但可能有数据间歇性丢失 合并购物车方案：通过user:cart:uuid/userId的方式判断有无登陆状态下的购物车，通过遍历两种购物车，用map存商品id对应的表项，最后合并成一个list，同时还需清除原先的cookie，因为未登录状态下的购物车信息是由cookie保存的。 5.2 服务提供者配置未登录时购物车的Service package com.hofe.cartservice.service;import com.alibaba.dubbo.config.annotation.Service;import com.hofe.api.cart.ICartService;import com.hofe.api.cart.pojo.CartItem;@Servicepublic class CartServiceImpl implements ICartService &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private TProductDao productDao; @Override public String add(String key, Long productId, Integer count) &#123; List&lt;CartItem&gt; cart = (List&lt;CartItem&gt;) redisTemplate.opsForValue().get(key); // 购物车不存在 if(cart == null)&#123; cart = new ArrayList&lt;&gt;(); cart.add(new CartItem(productId, count, new Date())); redisTemplate.opsForValue().set(key, cart); redisTemplate.expire(key, 30, TimeUnit.DAYS); return \"添加成功: \"+productId+\"目前数量: \"+count; &#125; // 购物车存在 for (CartItem cartItem : cart)&#123; if(cartItem.getProductId().longValue() == productId.longValue())&#123; cartItem.setCount(cartItem.getCount()+count); cartItem.setUpdateTime(new Date()); redisTemplate.opsForValue().set(key, cart); redisTemplate.expire(key, 30, TimeUnit.DAYS); return \"添加成功: \"+productId+\"目前数量: \"+count; &#125; &#125; // 购物车不存在该商品 cart.add(new CartItem(productId, count, new Date())); redisTemplate.opsForValue().set(key, cart); redisTemplate.expire(key, 30, TimeUnit.DAYS); return \"添加成功: \"+productId+\"目前数量: \"+count; &#125; @Override public String updateCount(String key, Long productId, Integer count) &#123; List&lt;CartItem&gt; carts = (List&lt;CartItem&gt;) redisTemplate.opsForValue().get(key); if(carts != null)&#123; for (CartItem item :carts)&#123; if(productId.longValue() == item.getProductId().longValue())&#123; item.setCount(item.getCount()+count); item.setUpdateTime(new Date()); redisTemplate.opsForValue().set(key, item); redisTemplate.expire(key, 30, TimeUnit.DAYS); return \"更新成功: \"+productId+\"目前数量: \"+count; &#125; &#125; &#125; return \"更新失败\"; &#125; @Override public String del(String key, Long productId) &#123; List&lt;CartItem&gt; carts = (List&lt;CartItem&gt;) redisTemplate.opsForValue().get(key); if(carts != null)&#123; for(CartItem item : carts)&#123; if(item.getProductId().longValue() == productId.longValue())&#123; carts.remove(item); redisTemplate.opsForValue().set(key, carts); redisTemplate.expire(key, 30, TimeUnit.DAYS); return \"删除成功\"; &#125; &#125; &#125; return \"删除失败\"; &#125; @Override public List&lt;CartItemVo&gt; list(String key) &#123; List&lt;CartItem&gt; cartItems = (List&lt;CartItem&gt;) redisTemplate.opsForValue().get(key); if(cartItems == null)&#123; return new ArrayList&lt;CartItemVo&gt;(); &#125; List&lt;CartItemVo&gt; cartItemVos = new ArrayList&lt;&gt;(cartItems.size()); for(CartItem cartItem : cartItems)&#123; // 将redis存储的List&lt;CartItem&gt;加入到List&lt;CartItemVo&gt;中 CartItemVo cartItemVo = new CartItemVo(); cartItemVo.setCount(cartItem.getCount()); cartItemVo.setUpdateTime(cartItem.getUpdateTime()); StringBuilder stringBuilder = new StringBuilder(\"productId:\").append(cartItem.getProductId()); TProduct product = (TProduct) redisTemplate.opsForValue().get(stringBuilder.toString()); if(product == null)&#123; // redis中没有，则查询数据库 product = productDao.queryById(cartItem.getProductId()); redisTemplate.opsForValue().set(stringBuilder.toString(), product); // 不是热门商品，不用设置太久 redisTemplate.expire(stringBuilder.toString(), 60, TimeUnit.MINUTES); &#125; cartItemVo.setProduct(product); cartItemVos.add(cartItemVo); &#125; Collections.sort(cartItemVos, new Comparator&lt;CartItemVo&gt;() &#123; @Override public int compare(CartItemVo o1, CartItemVo o2) &#123; return (int) (o2.getUpdateTime().getTime()-o1.getUpdateTime().getTime()); &#125; &#125;); return cartItemVos; &#125;&#125; 合并购物车 @Override public List merge(String nologinKey, String loginKey) &#123; List&lt;CartItem&gt; noLoginCart = (List&lt;CartItem&gt;) redisTemplate.opsForValue().get(nologinKey); if(noLoginCart == null)&#123; // 不存在未登陆购物车 return new ArrayList(); &#125; List&lt;CartItem&gt; loginCart = (List&lt;CartItem&gt;) redisTemplate.opsForValue().get(loginKey); if(loginCart == null)&#123; // 不存在登陆购物车，则将未登录时购物车加入已登录时购物车 redisTemplate.opsForValue().set(loginKey, noLoginCart); redisTemplate.expire(loginKey, 30, TimeUnit.DAYS); &#125; // 两者都存在 HashMap&lt;Long, CartItem&gt; map = new HashMap&lt;&gt;(); for(CartItem cartItem :noLoginCart)&#123; map.put(cartItem.getProductId(), cartItem); &#125; for(CartItem cartItem :loginCart)&#123; CartItem item = map.get(cartItem.getProductId()); if(item == null) &#123; map.put(cartItem.getProductId(), cartItem); &#125;else&#123; item.setCount(cartItem.getCount()+item.getCount()); &#125; &#125; // hashmap----&gt;list Collection&lt;CartItem&gt; values = map.values(); List&lt;CartItem&gt; list = new ArrayList&lt;&gt;(values); // list写入已登录购物车，删除未登录购物车 redisTemplate.delete(noLoginCart); // 已登录购物车加入redis redisTemplate.opsForValue().set(loginKey, loginCart); redisTemplate.expire(loginKey, 30, TimeUnit.DAYS); return list; &#125; 5.3 服务消费者配置未登录时购物车的Controller package com.hofe.cart.controller;import com.alibaba.dubbo.config.annotation.Reference;@RestController@RequestMapping(\"cart\")public class CartController &#123; @Reference private ICartService cartService; @GetMapping(\"hello\") public String hello()&#123; return \"hello world\"; &#125; @GetMapping(\"add/&#123;productId&#125;/&#123;count&#125;\") public String add(@PathVariable(\"productId\") Long productId, @PathVariable(\"count\") Integer count, @CookieValue(name = \"user_cart\",required = false) String uuid, HttpServletResponse response)&#123; if(uuid == null)&#123; uuid = UUID.randomUUID().toString(); &#125; StringBuilder key = new StringBuilder(\"user:cart:\").append(uuid); //写cookie到客户端,更新有效期 Cookie cookie = new Cookie(\"user_cart\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); cookie.setMaxAge(30*24*60*60); response.addCookie(cookie); return cartService.add(key.toString(),productId,count); &#125; @GetMapping(\"update/&#123;productId&#125;/&#123;count&#125;\") public String update(@PathVariable(\"productId\") Long productId, @PathVariable(\"count\") Integer count, @CookieValue(name = \"user_cart\",required = false) String uuid, HttpServletResponse response)&#123; if(uuid == null &amp;&amp; \"\".equals(uuid))&#123; return \"更新失败\"; &#125; StringBuilder key = new StringBuilder(\"user:cart:\").append(uuid); //写cookie到客户端,更新有效期 Cookie cookie = new Cookie(\"user_cart\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); cookie.setMaxAge(30*24*60*60); response.addCookie(cookie); return cartService.updateCount(key.toString(),productId,count); &#125; @GetMapping(\"delete/&#123;productId&#125;\") public String delete(@PathVariable(\"productId\") Long productId, @CookieValue(name = \"user_cart\",required = false) String uuid, HttpServletResponse response)&#123; if(uuid == null &amp;&amp; \"\".equals(uuid))&#123; return \"删除失败\"; &#125; StringBuilder key = new StringBuilder(\"user:cart:\").append(uuid); //写cookie到客户端,更新有效期 Cookie cookie = new Cookie(\"user_cart\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); cookie.setMaxAge(30*24*60*60); response.addCookie(cookie); return cartService.del(key.toString(), productId); &#125; @GetMapping(\"list\") public List list(@CookieValue(name = \"user_cart\",required = false) String uuid, HttpServletResponse response)&#123; if(uuid == null)&#123; uuid = UUID.randomUUID().toString(); &#125; StringBuilder key = new StringBuilder(\"user:cart:\").append(uuid); //写cookie到客户端,更新有效期 Cookie cookie = new Cookie(\"user_cart\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); cookie.setMaxAge(30*24*60*60); response.addCookie(cookie); return cartService.list(key.toString()); &#125;&#125; 已登录时购物车的Controller在客户端发起请求到controller之间加入拦截器，用于获取用户登录状态，并将结果封装在HttpServletRequest request中 配置（加入拦截器） @Configurationpublic class WebConfig implements WebMvcConfigurer&#123; @Autowired private AuthInterceptor authInterceptor; @Override public void addInterceptors(InterceptorRegistry registry)&#123; registry.addInterceptor(authInterceptor).addPathPatterns(\"/**\"); &#125;&#125; 拦截器 @Componentpublic class AuthInterceptor implements HandlerInterceptor &#123; @Reference private IUserService userService; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; // 获取用户登录状态 Cookie[] cookies = request.getCookies(); if(cookies == null)&#123; return true; &#125; for (Cookie cookie :cookies)&#123; if(\"user_token\".equals(cookie.getName()))&#123; String uuid = cookie.getValue(); TUser user = userService.checkIsLogin(uuid); if(user != null)&#123; request.setAttribute(\"user\", user); return true; &#125; &#125; &#125; // controller级别用httpclient接口 // 无论是否登录，都放行到购物车 return true; &#125;&#125; controller如果已登录，则RedisKey为user:cart:userId；否则为user:cart:uuid @GetMapping(\"add/&#123;productId&#125;/&#123;count&#125;\") public String add(@PathVariable(\"productId\") Long productId, @PathVariable(\"count\") Integer count, @CookieValue(name = \"user_cart\",required = false) String uuid, HttpServletResponse response, HttpServletRequest request)&#123; String key = \"\"; // 通过拦截器获取的登陆状态 TUser user = (TUser) request.getAttribute(\"user\"); if(user != null)&#123; key = new StringBuilder(\"user:cart:\").append(user.getId()).toString(); &#125;else&#123; if(uuid == null)&#123; uuid = UUID.randomUUID().toString(); &#125; key = new StringBuilder(\"user:cart:\").append(uuid).toString(); &#125; //写cookie到客户端,更新有效期 Cookie cookie = new Cookie(\"user_cart\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); cookie.setMaxAge(30*24*60*60); response.addCookie(cookie); return cartService.add(key.toString(),productId,count); &#125; 合并购物车，修改已登录购物车的商品记录，同时清除未登录时购物车的记录 @RequestMapping(\"merge\") public List merge(@CookieValue(name = \"user_cart\",required = false) String uuid, HttpServletResponse response, HttpServletRequest request) &#123; String key = \"\"; // 通过拦截器获取的登陆状态 TUser user = (TUser) request.getAttribute(\"user\"); if (user == null) &#123; return new ArrayList(); &#125; if (uuid == null || \"\".equals(uuid)) &#123; return new ArrayList(); &#125; String loginKey = new StringBuilder(\"user:cart:\").append(user.getId()).toString(); String noLoginKey = new StringBuilder(\"user:cart:\").append(uuid).toString(); List merge = cartService.merge(noLoginKey, loginKey); //写cookie到客户端,清除未登录时购物车的id Cookie cookie = new Cookie(\"user_cart\", uuid); cookie.setPath(\"/\"); cookie.setHttpOnly(true); cookie.setMaxAge(0); response.addCookie(cookie); return merge; &#125; 六、购物车优化6.1 回顾下之前购物车方案设计首先在客户端向服务器发起请求后，通过拦截器获取当前是否登录，如果登陆的话就可以获得cookie: user_token对应的value（用户信息），无论登陆与否都放行。控制层接收请求，并将cookie: user_cart对应的value（购物车信息）注入到uuid中。获取拦截器封装在request中的用户信息，如果不为null，说明用户已登录，那这时候需要在用户的购物车中添加记录，故传递redis: user:cart:userid更改在redis中保存的用户购物车信息；如果用户未登录，则在客户端中的购物车中添加记录，先传递redis: user:cart:uuid用于变更redis中保存的客户端购物车信息，同时需要修改客户端保存的cookie，因为未登录时的购物车是通过客户端保存的cookie的uuid区别不同客户端的。当controller调用service时传入key(userid或uuid)及记录，service首先判断客户端或者用户是否已有购物车，如果没有则新建一个list作为购物车；如果存在购物车，就判断购物车中是否有该条记录，有的话则修改记录的商品数，没有的话则新增该商品记录，并存入到redis；存入到未登录的购物车还是已登录的购物车由传递来的key(userid或uuid)决定。用户未登陆时加入购物车的商品需要合并到登陆后的购物车中，首先获取uuid代表客户端的购物车和userid代表用户的购物车，调用合并函数将两辆购物车合并，同时需要清除客户端的购物车信息，通过将cookie存活时间设置为0即可。merge函数通过传递进来的uuid和userid获取redis中保存的数据，如果客户端购物车为空，则不用进行合并；如果如果用户购物车为空，则将未登录的购物车加入到已登录购物车的redis记录中，并删除未登录购物车的记录。如果两者都不为空，则先遍历未登录购物车获取productId即记录，将其加入map；再遍历登陆购物车，判断map中是否存在相同的productId，存在则修改其对应的数量，如果不存在则加入map。最后通过collection将map的value即商品记录转成list返回，同时删除未登录购物车对应的redis记录。 6.2 优化的购物车设计方案之前的设计方案存在着更新删除需要遍历List的缺点，故改用Hash存储，可直接通过Key获得Value，而不需要再遍历列表。 七、订单模块（未完成）7.1 方案设计这一模块有几个过程：购物车结算-&gt;订单确认-&gt;生成订单-&gt;确认并支付。购物车结算-&gt;确认页面点击购物车结算后，跳转至确认页面，订单确认页面需要有用户地址信息、货物详情。故需要用户先登录，当用户访问确认也时，加入拦截器将页面从定向到登陆页面，记录referer，当用户登录成功后跳转回referer页面；货物详情通过uuid或者userid获得。 订单确认-&gt;生成订单订单基本表和订单明细表：订单基本表需要有id/orderId/userId/transfer_no/name/phone/address（历史快照）/status/total_money/pay_type；订单明细表需要有id/order_id/product_id/count/name/price（历史快照）。 订单编号：唯一性、有序性（查询）、时间性。采用基于时间戳的方式+userID生成 库存的扣减：库存预扣减、超时未支付的检测。采用定时任务now-create_time。 选择购物车的部分商品下单：获取用户勾选id集合–等于批量删除，根据id集合选择购物车的部分商品信息，才将商品信息转换为订单。 安全问题短信平台验证签名正确与否。发送签名：将用户id、token、时间戳通过MD5加密算法计算出sign发送给短信平台。短信平台验证签名：通过传递过来的sign获取用户id、token、时间戳，重新计算MD5判断是否与sign一致。缺陷：只保证了调用合法，明文传输无法保证内容合法。 对接支付宝调用支付宝接口需要先生成RAS密钥，包含应用私钥与应用公钥。生存密钥后才可以获取支付宝公钥。 八出现的问题问题：在service中因为无法找到dao实现类导致的自动注入失败Field productTypeDao in com.hofe.productservice.service.ProductTypeService required a bean of type 'com.hofe.dao.TProductTypeDao' that could not be found.The injection point has the following annotations: - @org.springframework.beans.factory.annotation.Autowired(required=true) 原因：考虑Spring未实例化对象找不到bean的配置，以往是使用bean.xml文件进行配置，SpringBoot使用注解，就需要在入口程序中添加@MapperScan(“dao路径”)注解；同时也要考虑在注解方式下dao是否@mapper，xml方式下.xml文件存放路径是否正确；以及考虑application.yml中是否正确配置数据源。可去掉Service上的@Component注解 问题：找不到dao中的方法 Invalid bound statement (not found): com.hofe.dao.TProductTypeDao.queryAll解决办法：在dao的pom中添加以下配置，标明xml配置存放路径 &lt;build&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; 或者在service的application.yml指明mybatis配置 mybatis: # 指定别名设置的包为所有entity type-aliases-package: cn.hofe.entity configuration: map-underscore-to-camel-case: true # 驼峰命名规范 mapper-locations: # mapper映射文件位置 - classpath:mapper/*.xml 问题：字段名为name、desc时查询出错原因：数据表字段名不要用name，desc会与msql自带语法冲突解决方法：1、在要用到关键字的地方加反引号标识；2、修改表字段名 问题：@Reference、@Service注解错误原因：错用了java的注解导致Service无法正常注入 import com.alibaba.dubbo.config.annotation.Service;import com.alibaba.dubbo.config.annotation.Reference; 问题：平级转树形结构问题遇到需要返回树形结构的json数据，如多级菜单、多级类别等，可以使用VO与resultMap搭配，mybatis递归查询。需要注意的是，resultMap中的collection字段的column=“字段名”，不需要加{}；如需传递多个参数可加如{id, code}。这一字段代表的意思是第一次执行queryAllByCategory之后递归查询用到的parentId是这个category_type字段。树形结构的数据，可以不用外键进行关联；外键关联之后，无法将顶级的parentId设置值，只能默认null。 以下链接可参考：https://www.hangge.com/blog/cache/detail_2715.htmlhttps://blog.csdn.net/janet796/article/details/79500349https://blog.csdn.net/qq_38164123/article/details/94358131 问题： This application has no explicit mapping for /error, so you are seeing this as a fallback.There was an unexpected error (type=Internal Server Error, status=500).org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.reflection.ReflectionException: There is no getter for property named 'parentId' in 'class java.lang.Integer' org.mybatis.spring.MyBatisSystemException: nested exception is org.apache.ibatis.reflection.ReflectionException 在使用choose之类条件控制语句的时候，要注意它判断的是传进来的参数的拥有的字段，如果传进来是个user可以使用；如果只传进来username，则无法控制password条件 问题：dubbo超时Invoke remote method timeout. method: queryCategorys, provider: dubbo:默认超时时间是1000ms，120条数据加上递归就超时了，可以在消费者和提供者端设置timeout=30000 问题：SSO单点登录模块跨域问题认证模块通过了，可其他模块在发送ajax请求时还是无法携带cookie。在SSO登录之后，浏览器返回cookie，其携带着uuid信息，其他系统在发送ajax请求时，携带这个cookie信息就可免登录。但需要在Controller的类或者方法上加上注解@CrossOrigin(origins = “*”, allowCredentials = “true”)，允许所有端口的访问，并允许携带cookie前端ajax代码需要withCredentials:true &amp;ajax(&#123; url:\"http://localhost:9098/sso/checkIsLogin\", xhrFields:&#123;withCredentials:true&#125;, crossDomain:true&#125;) 需要注意的是，cart.hofe.com并不能保存shop.hofe.com的cookie，故设置的时候要设置父域名cookie.setDomain(“hofe.com”)","categories":[{"name":"微服务","slug":"微服务","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Dubbo","slug":"微服务/Dubbo","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Dubbo/"},{"name":"入门实战","slug":"微服务/Dubbo/入门实战","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Dubbo/%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"http://hofe.work/tags/Dubbo/"}]},{"title":"Spring Cloud入门实战(三)--集成Ribbon","slug":"微服务分布式集群/SpringCloud入门实战(二)集成Ribbon","date":"2020-04-09T10:01:13.000Z","updated":"2020-04-27T07:04:03.757Z","comments":true,"path":"2020/04/09/微服务分布式集群/SpringCloud入门实战(二)集成Ribbon/","link":"","permalink":"http://hofe.work/2020/04/09/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/SpringCloud%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98(%E4%BA%8C)%E9%9B%86%E6%88%90Ribbon/","excerpt":"","text":"前言 正在入门SpringCloud中，在学习的过程中也正好做个项目练手。这个项目是想做成一个模板，这样之后遇到同规模项目的时候可以拿来就用，版本也好控制。涉及到的中间件会有Eureka、Ribbon、Feign、HyStrix、Zuul、ConfigServer。这一节项目将改写Ribbon。版本SpringBoot：2.2.1.RELEASESpring Cloud：Finchley.RS1spring-cloud-starter-netflix-eureka-client：2.2.1.RELEASE Eureka：2.2.1.RELEASE Ribbon：2.2.1.RELEASE 目录结构 如果为同一个的提供者在Eureka中注册了多个服务，那么客户端该如何选择服务呢？这时，就需要在客户端实现服务的负载均衡。 Ribbon是Netflixⅸx发布的负载均衡器，它有助于控制HTTP和TCP客户端的行为。为Ribbon配置服务提供者地址列表后， Ribbon就可基于某种负载均衡算法，自动地帮助服务消费者去请求。 Ribbon默认为我们提供了很多的负载均衡算法，例如轮询、随机等。当然，我们也可为 Ribbon实现自定义的负载均衡算法。 一、简单的负载均衡程序从consume-user复制粘贴出一个新的consume-user-ribbon，也可以不重新创建，直接修改consume-user，需要修改的是Controller和启动程序。 修改入口函数@SpringBootApplication@EnableEurekaClient@RibbonClient(\"PROVIDER-USER\") // 启用Ribbon并对PROVIDER-USER负载均衡public class ConsumeUserRibbonApplication &#123; public static void main( String[] args ) &#123; SpringApplication.run(ConsumeUserRibbonApplication.class, args); &#125;&#125; 修改消费者Controller修改获取url的方式 private EurekaClient eurekaClient;@LoadBalanced private RestTemplate restTemplate = new RestTemplate(); @GetMapping(\"getUserByEureka/&#123;id&#125;\") public User getUser(@PathVariable Long id)&#123; User user = restTemplate.getForObject(\"http://PROVIDER-USER/user/\"+id, User.class); return user; &#125; 开启两个生产者，注意端口不同但我通过消费者7901获取用户时，出现错误 I/O error on GET request for \"http://PROVIDER-USER/user/1\": PROVIDER-USER; nested exception is java.net.UnknownHostException: PROVIDER-USER 这是因为获取RestTemplat对象时要加上@LoadBalanced注解 ，否则restTemplate.getForObject方法会报java.net.UnknownHostException。要解决这个问题还有一点要改，就是Controller中的restTemplete要交由Spring容器来管理。我原先是直接new RestTtemplete()，在负载均衡的场景下不可用 二、修改负载均衡策略修改消费者的application.ymlPROVIDER-USER为生产者中配置的应用名，指定其策略为随机，这样就会随机使用两个生产者 PROVIDER-USER: ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 修改消费者Controller方便查看输出 @RestControllerpublic class UserController &#123; /* 使用eureka动态获取url */ @Autowired private EurekaClient eurekaClient; @Autowired private RestTemplate restTemplate; @Autowired private LoadBalancerClient loadBalancerClient; @GetMapping(\"getUserByEureka/&#123;id&#125;\") public User getUser(@PathVariable Long id)&#123; ServiceInstance instance = loadBalancerClient.choose(\"PROVIDER-USER\"); System.out.println(instance.getHost()+\":\"+instance.getPort()); User user = restTemplate.getForObject(\"http://PROVIDER-USER/user/\"+id, User.class); return user; &#125; /* 硬编码url private RestTemplate restTemplate = new RestTemplate(); // spring提供的用于访问接rest口的模板对象 @GetMapping(\"user/&#123;id&#125;\") public User getUser(@PathVariable Long id)&#123; User user = restTemplate.getForObject(\"http://localhost:7900/user/\"+id, User.class); return user; &#125; */ /* 服务提供者中的控制器 @GetMapping(\"user/&#123;id&#125;\") public User getUser(@PathVariable Long id)&#123; return new User(id); &#125; */&#125; 三、总结通过restTemplete获得服务生产者实例时，restTemplete需由Spring容器来生成并加上LoadBlance注解，不能直接new一个实例。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Spring Cloud","slug":"微服务/Spring-Cloud","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/"},{"name":"入门实战","slug":"微服务/Spring-Cloud/入门实战","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Ribbon","slug":"Ribbon","permalink":"http://hofe.work/tags/Ribbon/"}]},{"title":"Spring Cloud入门实战(二)--集成Eureka","slug":"微服务分布式集群/SpringCloud入门实战(二)集成Eureka","date":"2020-04-09T06:01:13.000Z","updated":"2020-04-27T07:03:58.542Z","comments":true,"path":"2020/04/09/微服务分布式集群/SpringCloud入门实战(二)集成Eureka/","link":"","permalink":"http://hofe.work/2020/04/09/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/SpringCloud%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98(%E4%BA%8C)%E9%9B%86%E6%88%90Eureka/","excerpt":"","text":"前言 正在入门SpringCloud中，在学习的过程中也正好做个项目练手。这个项目是想做成一个模板，这样之后遇到同规模项目的时候可以拿来就用，版本也好控制。涉及到的中间件会有Eureka、Ribbon、Feign、HyStrix、Zuul、ConfigServer。这一节将项目改装成Spring Cloud并集成Eureka。版本 SpringBoot：2.2.1.RELEASESpring Cloud：Finchley.RS1Eureka：2.2.1.RELEASE 目录结构 1、如果商品微服务的ip地址发生了变更，订单微服务中的配置文件也需要跟着修改2、如果商品微服务有多个，那么在订单微服务中又该如何写地址？因此引入注册中心用来管理接口配置 一、Eureka服务注册中心配置在项目pom文件中加入SpringCloud管理依赖&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- 导入Spring Cloud的依赖管理 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.RS1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在eureka模块pom中加入eureka依赖&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--springboot 整合eureka服务端--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 在这一块就卡了很久，会出现以下报错，这个错误到最后我也没解决，之后就换了版本，新建了个maven仓库。（一）需要注意的是SpringBoot的版本和eureka的版本有关联，比如我原先用的2.2.1的SpringBoot和1.4.3的eureka会有如下报错（二）不管springcloud还是springboot版本我都换了很多次，都是出现这个问题，新建了个maven仓库就解决了，判断应该是eureka中jersey出现了版本冲突。 2020-04-09 11:05:11.146 INFO 13864 --- [ main] com.hofe.EurekaApplication : No active profile set, falling back to default profiles: default2020-04-09 11:05:12.091 WARN 13864 --- [ main] o.s.boot.actuate.endpoint.EndpointId : Endpoint ID 'service-registry' contains invalid characters, please migrate to a valid format.2020-04-09 11:05:12.263 INFO 13864 --- [ main] o.s.cloud.context.scope.GenericScope : BeanFactory id=e29d8b1a-b705-3aaa-acfe-1ff43115d3582020-04-09 11:05:12.422 INFO 13864 --- [ main] trationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration' of type [org.springframework.cloud.autoconfigure.ConfigurationPropertiesRebinderAutoConfiguration$$EnhancerBySpringCGLIB$$4b153431] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)2020-04-09 11:05:12.720 INFO 13864 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 7999 (http)2020-04-09 11:05:12.733 INFO 13864 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat]2020-04-09 11:05:12.733 INFO 13864 --- [ main] org.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/9.0.27]2020-04-09 11:05:12.974 INFO 13864 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext2020-04-09 11:05:12.975 INFO 13864 --- [ main] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 1811 ms2020-04-09 11:05:13.069 WARN 13864 --- [ main] c.n.c.sources.URLConfigurationSource : No URLs will be polled as dynamic configuration sources.2020-04-09 11:05:13.069 INFO 13864 --- [ main] c.n.c.sources.URLConfigurationSource : To enable URLs as dynamic configuration sources, define System property archaius.configurationSource.additionalUrls or make config.properties available on classpath.2020-04-09 11:05:13.076 INFO 13864 --- [ main] c.netflix.config.DynamicPropertyFactory : DynamicPropertyFactory is initialized with configuration sources: com.netflix.config.ConcurrentCompositeConfiguration@12d1f1d42020-04-09 11:05:13.390 ERROR 13864 --- [ main] o.s.b.web.embedded.tomcat.TomcatStarter : Error starting Tomcat context. Exception: org.springframework.beans.factory.BeanCreationException. Message: Error creating bean with name 'jerseyFilterRegistration' defined in class path resource [org/springframework/cloud/netflix/eureka/server/EurekaServerAutoConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.FilterRegistrationBean]: Factory method 'jerseyFilterRegistration' threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/jersey/spi/container/servlet/ServletContainer2020-04-09 11:05:13.420 INFO 13864 --- [ main] o.apache.catalina.core.StandardService : Stopping service [Tomcat]2020-04-09 11:05:13.427 WARN 13864 --- [ main] ConfigServletWebServerApplicationContext : Exception encountered during context initialization - cancelling refresh attempt: org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat2020-04-09 11:05:13.443 INFO 13864 --- [ main] ConditionEvaluationReportLoggingListener : Error starting ApplicationContext. To display the conditions report re-run your application with 'debug' enabled.2020-04-09 11:05:13.457 ERROR 13864 --- [ main] o.s.boot.SpringApplication : Application run failedorg.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:156) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:544) ~[spring-context-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) [spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) [spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) [spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at com.hofe.EurekaApplication.main(EurekaApplication.java:15) [classes/:na]Caused by: org.springframework.boot.web.server.WebServerException: Unable to start embedded Tomcat at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:126) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.&lt;init&gt;(TomcatWebServer.java:88) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getTomcatWebServer(TomcatServletWebServerFactory.java:438) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.embedded.tomcat.TomcatServletWebServerFactory.getWebServer(TomcatServletWebServerFactory.java:191) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.createWebServer(ServletWebServerApplicationContext.java:180) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.onRefresh(ServletWebServerApplicationContext.java:153) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] ... 8 common frames omittedCaused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jerseyFilterRegistration' defined in class path resource [org/springframework/cloud/netflix/eureka/server/EurekaServerAutoConfiguration.class]: Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.FilterRegistrationBean]: Factory method 'jerseyFilterRegistration' threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/jersey/spi/container/servlet/ServletContainer at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:645) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.ConstructorResolver.instantiateUsingFactoryMethod(ConstructorResolver.java:625) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.instantiateUsingFactoryMethod(AbstractAutowireCapableBeanFactory.java:1338) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1177) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:557) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:517) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:323) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:321) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:207) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:211) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.servlet.ServletContextInitializerBeans.getOrderedBeansOfType(ServletContextInitializerBeans.java:202) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.servlet.ServletContextInitializerBeans.addServletContextInitializerBeans(ServletContextInitializerBeans.java:96) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.servlet.ServletContextInitializerBeans.&lt;init&gt;(ServletContextInitializerBeans.java:85) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.getServletContextInitializerBeans(ServletWebServerApplicationContext.java:253) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.selfInitialize(ServletWebServerApplicationContext.java:227) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.springframework.boot.web.embedded.tomcat.TomcatStarter.onStartup(TomcatStarter.java:53) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5135) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_144] at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[na:1.8.0_144] at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.StandardHost.startInternal(StandardHost.java:841) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1384) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.ContainerBase$StartChild.call(ContainerBase.java:1374) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_144] at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:134) ~[na:1.8.0_144] at org.apache.catalina.core.ContainerBase.startInternal(ContainerBase.java:909) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.StandardEngine.startInternal(StandardEngine.java:262) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.StandardService.startInternal(StandardService.java:421) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.core.StandardServer.startInternal(StandardServer.java:930) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:183) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.apache.catalina.startup.Tomcat.start(Tomcat.java:459) ~[tomcat-embed-core-9.0.27.jar:9.0.27] at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.initialize(TomcatWebServer.java:107) ~[spring-boot-2.2.1.RELEASE.jar:2.2.1.RELEASE] ... 13 common frames omittedCaused by: org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.FilterRegistrationBean]: Factory method 'jerseyFilterRegistration' threw exception; nested exception is java.lang.NoClassDefFoundError: com/sun/jersey/spi/container/servlet/ServletContainer at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:185) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.beans.factory.support.ConstructorResolver.instantiate(ConstructorResolver.java:640) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] ... 53 common frames omittedCaused by: java.lang.NoClassDefFoundError: com/sun/jersey/spi/container/servlet/ServletContainer at org.springframework.cloud.netflix.eureka.server.EurekaServerAutoConfiguration.jerseyFilterRegistration(EurekaServerAutoConfiguration.java:184) ~[spring-cloud-netflix-eureka-server-2.1.2.RELEASE.jar:2.1.2.RELEASE] at org.springframework.cloud.netflix.eureka.server.EurekaServerAutoConfiguration$$EnhancerBySpringCGLIB$$e48e3812.CGLIB$jerseyFilterRegistration$0(&lt;generated&gt;) ~[spring-cloud-netflix-eureka-server-2.1.2.RELEASE.jar:2.1.2.RELEASE] at org.springframework.cloud.netflix.eureka.server.EurekaServerAutoConfiguration$$EnhancerBySpringCGLIB$$e48e3812$$FastClassBySpringCGLIB$$b02b22.invoke(&lt;generated&gt;) ~[spring-cloud-netflix-eureka-server-2.1.2.RELEASE.jar:2.1.2.RELEASE] at org.springframework.cglib.proxy.MethodProxy.invokeSuper(MethodProxy.java:244) ~[spring-core-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.context.annotation.ConfigurationClassEnhancer$BeanMethodInterceptor.intercept(ConfigurationClassEnhancer.java:363) ~[spring-context-5.2.1.RELEASE.jar:5.2.1.RELEASE] at org.springframework.cloud.netflix.eureka.server.EurekaServerAutoConfiguration$$EnhancerBySpringCGLIB$$e48e3812.jerseyFilterRegistration(&lt;generated&gt;) ~[spring-cloud-netflix-eureka-server-2.1.2.RELEASE.jar:2.1.2.RELEASE] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_144] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_144] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_144] at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_144] at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:154) ~[spring-beans-5.2.1.RELEASE.jar:5.2.1.RELEASE] ... 54 common frames omittedCaused by: java.lang.ClassNotFoundException: com.sun.jersey.spi.container.servlet.ServletContainer at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[na:1.8.0_144] at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[na:1.8.0_144] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335) ~[na:1.8.0_144] at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[na:1.8.0_144] ... 65 common frames omitted进程完成，退出码 1 创建服务器的application.ymlserver: port: 7999spring: application: name: eureka-servereureka:###客户端调用地址 client: serviceUrl: defaultZone: http://localhost:7999/eureka/###是否将自己注册到Eureka服务中,因为该应用本身就是注册中心，不需要再注册自己（集群的时候为true） register-with-eureka: false###是否从Eureka中获取注册信息,因为自己为注册中心,不会在该应用中的检索服务信息 fetch-registry: false 配置启动类import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;/** * Eureka注册中心 */@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaServerApplication.class, args); &#125;&#125; 启动之后，访问端口，即可登录控制中心 二、Eureka客户端服务发现配置生产者配置在模块pom下加入依赖&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; 在生产者中的application.yml配置以下信息生产者中配置 server: port: 7900spring: application: name: provider-usereureka: client: service-url: defaultZone: http://localhost:7999/eureka/ 在启动类中增加注解@EnableEurekaClient@SpringBootApplication@EnableEurekaClientpublic class ProviderUserApplication &#123; public static void main( String[] args ) &#123; SpringApplication.run(ProviderUserApplication.class, args); &#125;&#125; 消费者配置在模块pom下加入依赖&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; 在消费者的application中添加clientserver: port: 7901spring: application: name: consume-usereureka: client: service-url: defaultZone: http://localhost:7999/eureka/ 修改消费者的UserController@Autowired private EurekaClient eurekaClient; private RestTemplate restTemplate = new RestTemplate(); @GetMapping(\"getUserByEureka/&#123;id&#125;\") public User getUser(@PathVariable Long id)&#123; InstanceInfo info = eurekaClient.getNextServerFromEureka(\"PROVIDER-USER\", false); String homePageUrl = info.getHomePageUrl(); User user = restTemplate.getForObject(homePageUrl+\"/user/\"+id, User.class); return user; &#125; 在启动类中添加注解@EnableEurekaClient@SpringBootApplication@EnableEurekaClientpublic class ConsumeUserApplicaton &#123; public static void main( String[] args ) &#123; SpringApplication.run(ConsumeUserApplicaton.class, args); &#125;&#125; 三、开启eureka服务器和客户端消费者访问自己的端口7901的getUserByEureka/1得到用户信息。 四、总结版本对应很重要，可以通过https://spring.io/projects/spring-cloud来查看spring-cloud和SpringBoot的对应关系，以及Eureka的依赖包是否会发生冲突。消费者通过EurekaClient的getNextServerFromEureka方法动态获取生产者的url，访问`消费者的url/getUserByEureka/1`时，会从`生产者的url/user/1`中获取。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Spring Cloud","slug":"微服务/Spring-Cloud","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/"},{"name":"入门实战","slug":"微服务/Spring-Cloud/入门实战","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://hofe.work/tags/Spring-Cloud/"},{"name":"Eureka","slug":"Eureka","permalink":"http://hofe.work/tags/Eureka/"}]},{"title":"Spring Cloud入门实战(一)","slug":"微服务分布式集群/SpringCloud入门实战(一)","date":"2020-04-08T05:01:13.000Z","updated":"2020-04-27T07:04:08.329Z","comments":true,"path":"2020/04/08/微服务分布式集群/SpringCloud入门实战(一)/","link":"","permalink":"http://hofe.work/2020/04/08/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/SpringCloud%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98(%E4%B8%80)/","excerpt":"","text":"前言 正在入门SpringCloud中，在学习的过程中也正好做个项目练手。这个项目是想做成一个模板，这样之后遇到同规模项目的时候可以拿来就用，版本也好控制。涉及到的中间件会有Eureka、Ribbon、Feign、HyStrix、Zuul、ConfigServer。这一节使用生产者消费者模型体现微服务思想。 版本SpringBoot：2.2.1.RELEASE 项目目录结构如下 一、创建项目先来建个工程ArtifactId为templete 创建服务模块和建立项目一样的步骤建立一个module作为服务提供者同样方式创建服务消费者 pom配置通过刚才那种方式建立的pom如下 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;templete&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;templete&lt;/name&gt; &lt;!-- FIXME change it to the project's website --&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;pluginManagement&gt;&lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --&gt; &lt;plugins&gt; &lt;!-- clean lifecycle, see https://maven.apache.org/ref/current/maven-core/lifecycles.html#clean_Lifecycle --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-clean-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/plugin&gt; &lt;!-- default lifecycle, jar packaging: see https://maven.apache.org/ref/current/maven-core/default-bindings.html#Plugin_bindings_for_jar_packaging --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.8.0&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.22.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-install-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-deploy-plugin&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/plugin&gt; &lt;!-- site lifecycle, see https://maven.apache.org/ref/current/maven-core/lifecycles.html#site_Lifecycle --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-site-plugin&lt;/artifactId&gt; &lt;version&gt;3.7.1&lt;/version&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-project-info-reports-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt;&lt;/project&gt; 修改pom在项目pom中声明为SpringBoot项目，不用导入依赖包；并修改全部pom文件maven构建工具依赖 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;!--将当前项目声明为SpringBoot--&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.2.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;artifactId&gt;templete&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;provider-user&lt;/module&gt; &lt;module&gt;consumer-order&lt;/module&gt; &lt;/modules&gt; &lt;name&gt;templete&lt;/name&gt; &lt;!-- FIXME change it to the project's website --&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;!--可以用以下替换--&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 修改模块pom，并增加Spring-Boot-web依赖 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;parent&gt; &lt;artifactId&gt;templete&lt;/artifactId&gt; &lt;groupId&gt;com.hofe&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;provider-user&lt;/artifactId&gt; &lt;name&gt;provider-user&lt;/name&gt; &lt;!-- FIXME change it to the project's website --&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 二、配置服务服务提供者创建实体类User创建UserController修改Application启动类 添加resources资源文件夹，并创建application.yml配置文件启动服务 服务消费者pom配置、启动类、实体类User同服务提供者，不同点在于application.yml中端口设置和UserController的配置。 application.yml中端口UserController消费者中的user是通过restTemplete访问提供者生产的user得到的运行访问localhost:7901/user/id 三、总结这章节首先创建一个java项目，通过配置声明成springboot风格项目；创建的消费者和生产者模块模拟微服务，在两个模块中引入springboot-web依赖包，生产者提供资源，消费者通过restTemplete访问生产者接口url，从而获取资源。下一节，将会使用Eureka服务注册中心管理url。","categories":[{"name":"微服务","slug":"微服务","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Spring Cloud","slug":"微服务/Spring-Cloud","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/"},{"name":"入门实战","slug":"微服务/Spring-Cloud/入门实战","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98/"}],"tags":[{"name":"生产者消费者","slug":"生产者消费者","permalink":"http://hofe.work/tags/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85/"}]},{"title":"Spring Boot学习笔记","slug":"Java/SpringBoot学习笔记","date":"2020-04-05T16:00:00.000Z","updated":"2020-04-27T07:06:55.951Z","comments":true,"path":"2020/04/06/Java/SpringBoot学习笔记/","link":"","permalink":"http://hofe.work/2020/04/06/Java/SpringBoot%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"","text":"跟武哥一起学习Spring Boot作者信息本课程首发于 CSDN GitChat 达人课《跟武哥一起学Spring Boot》，该文档为课程详细笔记。作者：倪升武（武哥）微信公众号：武哥聊编程 版权申明：本课程已经免费，但版权属于 CSDN 和作者，未经允许，不得将该笔记用于商业用途和其他不正当用途，否则追究法律责任。 导读：课程概览1. Spring Boot是什么我们知道，从 2002 年开始，Spring 一直在飞速的发展，如今已经成为了在Java EE（Java Enterprise Edition）开发中真正意义上的标准，但是随着技术的发展，Java EE使用 Spring 逐渐变得笨重起来，大量的 XML 文件存在于项目之中。繁琐的配置，整合第三方框架的配置问题，导致了开发和部署效率的降低。 2012 年 10 月，Mike Youngstrom 在 Spring jira 中创建了一个功能请求，要求在 Spring 框架中支持无容器 Web 应用程序体系结构。他谈到了在主容器引导 spring 容器内配置 Web 容器服务。这是 jira 请求的摘录： 我认为 Spring 的 Web 应用体系结构可以大大简化，如果它提供了从上到下利用 Spring 组件和配置模型的工具和参考体系结构。在简单的 main()方法引导的 Spring 容器内嵌入和统一这些常用Web 容器服务的配置。 这一要求促使了 2013 年初开始的 Spring Boot 项目的研发，到今天，Spring Boot 的版本已经到了 2.0.3 RELEASE。Spring Boot 并不是用来替代 Spring 的解决方案，而是和 Spring 框架紧密结合用于提升 Spring 开发者体验的工具。 它集成了大量常用的第三方库配置，Spring Boot应用中这些第三方库几乎可以是零配置的开箱即用（out-of-the-box），大部分的 Spring Boot 应用都只需要非常少量的配置代码（基于 Java 的配置），开发者能够更加专注于业务逻辑。 2. 为什么学习Spring Boot2.1 从Spring官方来看我们打开 Spring 的官方网站，可以看到下图： 我们可以看到图中官方对 Spring Boot 的定位：Build Anything， Build任何东西。Spring Boot旨在尽可能快地启动和运行，并且只需最少的 Spring 前期配置。 同时我们也来看一下官方对后面两个的定位： SpringCloud：Coordinate Anything，协调任何事情；SpringCloud Data Flow：Connect everything，连接任何东西。 仔细品味一下，Spring 官网对 Spring Boot、SpringCloud 和 SpringCloud Data Flow三者定位的措辞非常有味道，同时也可以看出，Spring 官方对这三个技术非常重视，是现在以及今后学习的重点（SpringCloud 相关达人课课程届时也会上线）。 2.2 从Spring Boot的优点来看Spring Boot 有哪些优点？主要给我们解决了哪些问题呢？我们以下图来说明： 2.2.1 良好的基因Spring Boot 是伴随着 Spring 4.0 诞生的，从字面理解，Boot是引导的意思，因此 Spring Boot 旨在帮助开发者快速搭建 Spring 框架。Spring Boot 继承了原有 Spring 框架的优秀基因，使 Spring 在使用中更加方便快捷。 2.2.2 简化编码举个例子，比如我们要创建一个 web 项目，使用 Spring 的朋友都知道，在使用 Spring 的时候，需要在 pom 文件中添加多个依赖，而 Spring Boot 则会帮助开发着快速启动一个 web 容器，在 Spring Boot 中，我们只需要在 pom 文件中添加如下一个 starter-web 依赖即可。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt; 我们点击进入该依赖后可以看到，Spring Boot 这个 starter-web 已经包含了多个依赖，包括之前在 Spring 工程中需要导入的依赖，我们看一下其中的一部分，如下： &lt;!-- .....省略其他依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-web&lt;/artifactId&gt; &lt;version&gt;5.0.7.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;5.0.7.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 由此可以看出，Spring Boot 大大简化了我们的编码，我们不用一个个导入依赖，直接一个依赖即可。 2.2.3 简化配置Spring 虽然使Java EE轻量级框架，但由于其繁琐的配置，一度被人认为是“配置地狱”。各种XML、Annotation配置会让人眼花缭乱，而且配置多的话，如果出错了也很难找出原因。Spring Boot更多的是采用 Java Config 的方式，对 Spring 进行配置。举个例子： 我新建一个类，但是我不用 @Service注解，也就是说，它是个普通的类，那么我们如何使它也成为一个 Bean 让 Spring 去管理呢？只需要@Configuration 和@Bean两个注解即可，如下： public class TestService &#123; public String sayHello () &#123; return \"Hello Spring Boot!\"; &#125;&#125; import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class JavaConfig &#123; @Bean public TestService getTestService() &#123; return new TestService(); &#125;&#125; @Configuration表示该类是个配置类，@Bean表示该方法返回一个 Bean。这样就把TestService作为 Bean 让 Spring 去管理了，在其他地方，我们如果需要使用该 Bean，和原来一样，直接使用@Resource注解注入进来即可使用，非常方便。 @Resourceprivate TestService testService; 另外，部署配置方面，原来 Spring 有多个 xml 和 properties配置，在 Spring Boot 中只需要个 application.yml即可。 2.2.4 简化部署在使用 Spring 时，项目部署时需要我们在服务器上部署 tomcat，然后把项目打成 war 包扔到 tomcat里，在使用 Spring Boot 后，我们不需要在服务器上去部署 tomcat，因为 Spring Boot 内嵌了 tomcat，我们只需要将项目打成 jar 包，使用 java -jar xxx.jar一键式启动项目。 另外，也降低对运行环境的基本要求，环境变量中有JDK即可。 2.2.5 简化监控我们可以引入 spring-boot-start-actuator 依赖，直接使用 REST 方式来获取进程的运行期性能参数，从而达到监控的目的，比较方便。但是 Spring Boot 只是个微框架，没有提供相应的服务发现与注册的配套功能，没有外围监控集成方案，没有外围安全管理方案，所以在微服务架构中，还需要 Spring Cloud 来配合一起使用。 2.3 从未来发展的趋势来看微服务是未来发展的趋势，项目会从传统架构慢慢转向微服务架构，因为微服务可以使不同的团队专注于更小范围的工作职责、使用独立的技术、更安全更频繁地部署。而 继承了 Spring 的优良特性，与 Spring 一脉相承，而且 支持各种REST API 的实现方式。Spring Boot 也是官方大力推荐的技术，可以看出，Spring Boot 是未来发展的一个大趋势。 3. 本课程能学到什么本课程使用目前 Spring Boot 最新版本2.0.3 RELEASE，课程文章均为作者在实际项目中剥离出来的场景和demo，目标是带领学习者快速上手 Spring Boot，将 Spring Boot 相关技术点快速运用在微服务项目中。全篇分为两部分：基础篇和进阶篇。 基础篇（01—10课）主要介绍 Spring Boot 在项目中最常使用的一些功能点，旨在带领学习者快速掌握 Spring Boot 在开发时需要的知识点，能够把 Spring Boot 相关技术运用到实际项目架构中去。该部分以 Spring Boot 框架为主线，内容包括Json数据封装、日志记录、属性配置、MVC支持、在线文档、模板引擎、异常处理、AOP 处理、持久层集成等等。 进阶篇（11—17课）主要是介绍 Spring Boot 在项目中拔高一些的技术点，包括集成的一些组件，旨在带领学习者在项目中遇到具体的场景时能够快速集成，完成对应的功能。该部分以 Spring Boot 框架为主线，内容包括拦截器、监听器、缓存、安全认证、分词插件、消息队列等等。 认真读完该系列文章之后，学习者会快速了解并掌握 Spring Boot 在项目中最常用的技术点，作者课程的最后，会基于课程内容搭建一个 Spring Boot 项目的空架构，该架构也是从实际项目中剥离出来，学习者可以运用该架构于实际项目中，具备使用 Spring Boot 进行实际项目开发的能力。 课程所有源码提供免费下载：下载地址。 欢迎关注我的为微信公众号：武哥聊编程 4. 适合阅读的人群本课程适合以下人群阅读： 有一定的Java语言基础，了解Spring、Maven的在校学生或自学者 有传统项目经验，想往微服务方向发展的工作人员 热衷于新技术并对 Spring Boot 感兴趣的人员 希望了解 Spring Boot 2.0.3 的研究人员5. 本课程开发环境和插件 本课程的开发环境： 开发工具：IDEA 2017 JDK版本： JDK 1.8 Spring Boot版本：2.0.3 RELEASE Maven版本：3.5.2 涉及到的插件： FastJson Swagger2 Thymeleaf MyBatis Redis ActiveMQ Shiro Lucence 6. 课程目录 导读：课程概览 第01课：Spring Boot开发环境搭建和项目启动 第02课：Spring Boot返回Json数据及数据封装 第03课：Spring Boot使用slf4j进行日志记录 第04课：Spring Boot中的项目属性配置 第05课：Spring Boot中的MVC支持 第06课：Spring Boot集成Swagger2展现在线接口文档 第07课：Spring Boot集成Thymeleaf模板引擎 第08课：Spring Boot中的全局异常处理 第09课：Spring Boot中的切面AOP处理 第10课：Spring Boot中集成MyBatis 第11课：Spring Boot事务配置管理 第12课：Spring Boot中使用监听器 第13课：Spring Boot中使用拦截器 第14课：Spring Boot中集成Redis 第15课：Spring Boot中集成ActiveMQ 第16课：Spring Boot中集成Shiro 第17课：Spring Boot中结成Lucence 第18课：Spring Boot搭建实际项目开发中的架构 欢迎关注我的为微信公众号：武哥聊编程 第01课：Spring Boot开发环境搭建和项目启动上一节对 SpringBoot 的特性做了一个介绍，本节主要对 jdk 的配置、Spring Boot工程的构建和项目的启动、Spring Boot 项目工程的结构做一下讲解和分析。 1. jdk 的配置本课程是使用 IDEA 进行开发，在IDEA 中配置 jdk 的方式很简单，打开File-&gt;Project Structure，如下图所： 选择 SDKs 在 JDK home path 中选择本地 jdk 的安装目录 在 Name 中为 jdk 自定义名字 通过以上三步骤，即可导入本地安装的 jdk。如果是使用 STS 或者 eclipse 的朋友，可以通过两步骤添加： window-&gt;preference-&gt;java-&gt;Instralled JRES来添加本地 jdk。 window--&gt;preference--&gt;java--&gt;Compiler选择 jre，和 jdk 保持一致。 2. Spring Boot 工程的构建2.1 IDEA 快速构建IDEA 中可以通过File-&gt;New-&gt;Project来快速构建 Spring Boot 工程。如下，选择 Spring Initializr，在 Project SDK 中选择刚刚我们导入的 jdk，点击 Next，到了项目的配置信息。 Group：填企业域名，本课程使用com.itcodai Artifact：填项目名称，本课程中每一课的工程名以course+课号命令，这里使用 course01 Dependencies：可以添加我们项目中所需要的依赖信息，根据实际情况来添加，本课程只需要选择 Web 即可。 2.2 官方构建第二种方式可以通过官方构建，步骤如下： 访问 http://start.spring.io/。 在页面上输入相应的 Spring Boot 版本、Group 和 Artifact 信息以及项目依赖，然后创建项目。 解压后，使用 IDEA 导入该 maven 工程：File-&gt;New-&gt;Model from Existing Source，然后选择解压后的项目文件夹即可。如果是使用 eclipse 的朋友，可以通过Import-&gt;Existing Maven Projects-&gt;Next，然后选择解压后的项目文件夹即可。 2.3 maven配置创建了 Spring Boot 项目之后，需要进行 maven 配置。打开File-&gt;settings，搜索 maven，配置一下本地的 maven 信息。如下： 在 Maven home directory 中选择本地 Maven 的安装路径；在 User settings file 中选择本地 Maven 的配置文件所在路径。在配置文件中，我们配置一下国内阿里的镜像，这样在下载 maven 依赖时，速度很快。 &lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 如果是使用 eclipse 的朋友，可以通过window--&gt;preference--&gt;Maven--&gt;User Settings来配置，配置方式和上面一致。 2.4 编码配置同样地，新建项目后，我们一般都需要配置编码，这点非常重要，很多初学者都会忘记这一步，所以要养成良好的习惯。 IDEA 中，仍然是打开File-&gt;settings，搜索 encoding，配置一下本地的编码信息。如下： 如果是使用 eclipse 的朋友，有两个地方需要设置一下编码： window–&gt; perferences–&gt;General–&gt;Workspace，将Text file encoding改成utf-8 window–&gt;perferences–&gt;General–&gt;content types，选中Text，将Default encoding填入utf-8 OK，编码设置完成即可启动项目工程了。 3. Spring Boot 项目工程结构Spring Boot 项目总共有三个模块，如下图所示： src/main/java路径：主要编写业务程序 src/main/resources路径：存放静态文件和配置文件 src/test/java路径：主要编写测试程序 默认情况下，如上图所示会创建一个启动类 Course01Application，该类上面有个@SpringBootApplication注解，该启动类中有个 main 方法，没错，Spring Boot 启动只要运行该 main 方法即可，非常方便。另外，Spring Boot 内部集成了 tomcat，不需要我们人为手动去配置 tomcat，开发者只需要关注具体的业务逻辑即可。 到此为止，Spring Boot 就启动成功了，为了比较清楚的看到效果，我们写一个 Controller 来测试一下，如下： package com.itcodai.course01.controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(\"/start\")public class StartController &#123; @RequestMapping(\"/springboot\") public String startSpringBoot() &#123; return \"Welcome to the world of Spring Boot!\"; &#125;&#125; 重新运行 main 方法启动项目，在浏览器中输入 localhost:8080/start/springboot，如果看到 “Welcome to the world of Spring Boot!”，那么恭喜你项目启动成功！Spring Boot 就是这么简单方便！端口号默认是8080，如果想要修改，可以在 application.yml 文件中使用 server.port 来人为指定端口，如8001端口： server: port: 8001 4. 总结本节我们快速学习了如何在 IDEA 中导入 jdk，以及使用 IDEA 如何配置 maven 和编码，如何快速的创建和启动 Spring Boot 工程。IDEA 对 Spring Boot 的支持非常友好，建议大家使用 IDEA 进行 Spring Boot 的开发，从下一课开始，我们真正进入 Spring Boot 的学习中。课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第02课：Spring Boot返回Json数据及数据封装在项目开发中，接口与接口之间，前后端之间数据的传输都使用 Json 格式，在 Spring Boot 中，接口返回 Json 格式的数据很简单，在 Controller 中使用@RestController注解即可返回 Json 格式的数据，@RestController也是 Spring Boot 新增的一个注解，我们点进去看一下该注解都包含了哪些东西。 @Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Controller@ResponseBodypublic @interface RestController &#123; String value() default \"\";&#125; 可以看出， @RestController 注解包含了原来的 @Controller 和 @ResponseBody 注解，使用过 Spring 的朋友对 @Controller 注解已经非常了解了，这里不再赘述， @ResponseBody 注解是将返回的数据结构转换为 Json 格式。所以在默认情况下，使用了 @RestController 注解即可将返回的数据结构转换成 Json 格式，Spring Boot 中默认使用的 Json 解析技术框架是 jackson。我们点开 pom.xml 中的 spring-boot-starter-web 依赖，可以看到一个 spring-boot-starter-json 依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-json&lt;/artifactId&gt; &lt;version&gt;2.0.3.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; Spring Boot 中对依赖都做了很好的封装，可以看到很多 spring-boot-starter-xxx 系列的依赖，这是 Spring Boot 的特点之一，不需要人为去引入很多相关的依赖了，starter-xxx 系列直接都包含了所必要的依赖，所以我们再次点进去上面这个 spring-boot-starter-json 依赖，可以看到： &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.9.6&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt; &lt;artifactId&gt;jackson-datatype-jdk8&lt;/artifactId&gt; &lt;version&gt;2.9.6&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.datatype&lt;/groupId&gt; &lt;artifactId&gt;jackson-datatype-jsr310&lt;/artifactId&gt; &lt;version&gt;2.9.6&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt; &lt;artifactId&gt;jackson-module-parameter-names&lt;/artifactId&gt; &lt;version&gt;2.9.6&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 到此为止，我们知道了 Spring Boot 中默认使用的 json 解析框架是 jackson。下面我们看一下默认的 jackson 框架对常用数据类型的转 Json 处理。 1. Spring Boot 默认对Json的处理在实际项目中，常用的数据结构无非有类对象、List对象、Map对象，我们看一下默认的 jackson 框架对这三个常用的数据结构转成 json 后的格式如何。 1.1 创建 User 实体类为了测试，我们需要创建一个实体类，这里我们就用 User 来演示。 public class User &#123; private Long id; private String username; private String password; /* 省略get、set和带参构造方法 */&#125; 1.2 创建Controller类然后我们创建一个 Controller，分别返回 User对象、List&lt;User&gt; 和 Map&lt;String, Object&gt;。 import com.itcodai.course02.entity.User;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.ArrayList;import java.util.HashMap;import java.util.List;import java.util.Map;@RestController@RequestMapping(\"/json\")public class JsonController &#123; @RequestMapping(\"/user\") public User getUser() &#123; return new User(1, \"倪升武\", \"123456\"); &#125; @RequestMapping(\"/list\") public List&lt;User&gt; getUserList() &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(); User user1 = new User(1, \"倪升武\", \"123456\"); User user2 = new User(2, \"达人课\", \"123456\"); userList.add(user1); userList.add(user2); return userList; &#125; @RequestMapping(\"/map\") public Map&lt;String, Object&gt; getMap() &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(3); User user = new User(1, \"倪升武\", \"123456\"); map.put(\"作者信息\", user); map.put(\"博客地址\", \"http://blog.itcodai.com\"); map.put(\"CSDN地址\", \"http://blog.csdn.net/eson_15\"); map.put(\"粉丝数量\", 4153); return map; &#125;&#125; 1.3 测试不同数据类型返回的jsonOK，写好了接口，分别返回了一个 User 对象、一个 List 集合和一个 Map 集合，其中 Map 集合中的 value 存的是不同的数据类型。接下来我们依次来测试一下效果。 在浏览器中输入：localhost:8080/json/user 返回 json 如下： &#123;\"id\":1,\"username\":\"倪升武\",\"password\":\"123456\"&#125; 在浏览器中输入：localhost:8080/json/list 返回 json 如下： [&#123;\"id\":1,\"username\":\"倪升武\",\"password\":\"123456\"&#125;,&#123;\"id\":2,\"username\":\"达人课\",\"password\":\"123456\"&#125;] 在浏览器中输入：localhost:8080/json/map 返回 json 如下： &#123;\"作者信息\":&#123;\"id\":1,\"username\":\"倪升武\",\"password\":\"123456\"&#125;,\"CSDN地址\":\"http://blog.csdn.net/eson_15\",\"粉丝数量\":4153,\"博客地址\":\"http://blog.itcodai.com\"&#125; 可以看出，map 中不管是什么数据类型，都可以转成相应的 json 格式，这样就非常方便。 1.4 jackson 中对null的处理在实际项目中，我们难免会遇到一些 null 值出现，我们转 json 时，是不希望有这些 null 出现的，比如我们期望所有的 null 在转 json 时都变成 “” 这种空字符串，那怎么做呢？在 Spring Boot 中，我们做一下配置即可，新建一个 jackson 的配置类： import com.fasterxml.jackson.core.JsonGenerator;import com.fasterxml.jackson.databind.JsonSerializer;import com.fasterxml.jackson.databind.ObjectMapper;import com.fasterxml.jackson.databind.SerializerProvider;import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingBean;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Primary;import org.springframework.http.converter.json.Jackson2ObjectMapperBuilder;import java.io.IOException;@Configurationpublic class JacksonConfig &#123; @Bean @Primary @ConditionalOnMissingBean(ObjectMapper.class) public ObjectMapper jacksonObjectMapper(Jackson2ObjectMapperBuilder builder) &#123; ObjectMapper objectMapper = builder.createXmlMapper(false).build(); objectMapper.getSerializerProvider().setNullValueSerializer(new JsonSerializer&lt;Object&gt;() &#123; @Override public void serialize(Object o, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException &#123; jsonGenerator.writeString(\"\"); &#125; &#125;); return objectMapper; &#125;&#125; 然后我们修改一下上面返回 map 的接口，将几个值改成 null 测试一下： @RequestMapping(\"/map\")public Map&lt;String, Object&gt; getMap() &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(3); User user = new User(1, \"倪升武\", null); map.put(\"作者信息\", user); map.put(\"博客地址\", \"http://blog.itcodai.com\"); map.put(\"CSDN地址\", null); map.put(\"粉丝数量\", 4153); return map;&#125; 重启项目，再次输入：localhost:8080/json/map，可以看到 jackson 已经将所有 null 字段转成了空字符串了。 &#123;\"作者信息\":&#123;\"id\":1,\"username\":\"倪升武\",\"password\":\"\"&#125;,\"CSDN地址\":\"\",\"粉丝数量\":4153,\"博客地址\":\"http://blog.itcodai.com\"&#125; 2. 使用阿里巴巴FastJson的设置2.1 jackson 和 fastJson 的对比有很多朋友习惯于使用阿里巴巴的 fastJson 来做项目中 json 转换的相关工作，目前我们项目中使用的就是阿里的 fastJson，那么 jackson 和 fastJson 有哪些区别呢？根据网上公开的资料比较得到下表。| 选项 | fastJson | jackson || :-: | :-: | :-: || 上手难易程度 | 容易 | 中等 || 高级特性支持 | 中等 | 丰富 || 官方文档、Example支持 | 中文 | 英文 || 处理json速度 | 略快 | 快 | 关于 fastJson 和 jackson 的对比，网上有很多资料可以查看，主要是根据自己实际项目情况来选择合适的框架。从扩展上来看，fastJson 没有 jackson 灵活，从速度或者上手难度来看，fastJson 可以考虑，我们项目中目前使用的是阿里的 fastJson，挺方便的。 2.2 fastJson依赖导入使用 fastJson 需要导入依赖，本课程使用 1.2.35 版本，依赖如下： &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.35&lt;/version&gt;&lt;/dependency&gt; 2.2 使用 fastJson 处理 null使用 fastJson 时，对 null 的处理和 jackson 有些不同，需要继承 WebMvcConfigurationSupport 类，然后覆盖 configureMessageConverters 方法，在方法中，我们可以选择对要实现 null 转换的场景，配置好即可。如下： import com.alibaba.fastjson.serializer.SerializerFeature;import com.alibaba.fastjson.support.config.FastJsonConfig;import com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter;import org.springframework.context.annotation.Configuration;import org.springframework.http.MediaType;import org.springframework.http.converter.HttpMessageConverter;import org.springframework.web.servlet.config.annotation.WebMvcConfigurationSupport;import java.nio.charset.Charset;import java.util.ArrayList;import java.util.List;@Configurationpublic class fastJsonConfig extends WebMvcConfigurationSupport &#123; /** * 使用阿里 FastJson 作为JSON MessageConverter * @param converters */ @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); FastJsonConfig config = new FastJsonConfig(); config.setSerializerFeatures( // 保留map空的字段 SerializerFeature.WriteMapNullValue, // 将String类型的null转成\"\" SerializerFeature.WriteNullStringAsEmpty, // 将Number类型的null转成0 SerializerFeature.WriteNullNumberAsZero, // 将List类型的null转成[] SerializerFeature.WriteNullListAsEmpty, // 将Boolean类型的null转成false SerializerFeature.WriteNullBooleanAsFalse, // 避免循环引用 SerializerFeature.DisableCircularReferenceDetect); converter.setFastJsonConfig(config); converter.setDefaultCharset(Charset.forName(\"UTF-8\")); List&lt;MediaType&gt; mediaTypeList = new ArrayList&lt;&gt;(); // 解决中文乱码问题，相当于在Controller上的@RequestMapping中加了个属性produces = \"application/json\" mediaTypeList.add(MediaType.APPLICATION_JSON); converter.setSupportedMediaTypes(mediaTypeList); converters.add(converter); &#125;&#125; 3. 封装统一返回的数据结构以上是 Spring Boot 返回 json 的几个代表的例子，但是在实际项目中，除了要封装数据之外，我们往往需要在返回的 json 中添加一些其他信息，比如返回一些状态码 code ，返回一些 msg 给调用者，这样调用者可以根据 code 或者 msg 做一些逻辑判断。所以在实际项目中，我们需要封装一个统一的 json 返回结构存储返回信息。 3.1 定义统一的 json 结构由于封装的 json 数据的类型不确定，所以在定义统一的 json 结构时，我们需要用到泛型。统一的 json 结构中属性包括数据、状态码、提示信息即可，构造方法可以根据实际业务需求做相应的添加即可，一般来说，应该有默认的返回结构，也应该有用户指定的返回结构。如下： public class JsonResult&lt;T&gt; &#123; private T data; private String code; private String msg; /** * 若没有数据返回，默认状态码为0，提示信息为：操作成功！ */ public JsonResult() &#123; this.code = \"0\"; this.msg = \"操作成功！\"; &#125; /** * 若没有数据返回，可以人为指定状态码和提示信息 * @param code * @param msg */ public JsonResult(String code, String msg) &#123; this.code = code; this.msg = msg; &#125; /** * 有数据返回时，状态码为0，默认提示信息为：操作成功！ * @param data */ public JsonResult(T data) &#123; this.data = data; this.code = \"0\"; this.msg = \"操作成功！\"; &#125; /** * 有数据返回，状态码为0，人为指定提示信息 * @param data * @param msg */ public JsonResult(T data, String msg) &#123; this.data = data; this.code = \"0\"; this.msg = msg; &#125; // 省略get和set方法&#125; 3.2 修改 Controller 中的返回值类型及测试由于 JsonResult 使用了泛型，所以所有的返回值类型都可以使用该统一结构，在具体的场景将泛型替换成具体的数据类型即可，非常方便，也便于维护。在实际项目中，还可以继续封装，比如状态码和提示信息可以定义一个枚举类型，以后我们只需要维护这个枚举类型中的数据即可（在本课程中就不展开了）。根据以上的 JsonResult，我们改写一下 Controller，如下： @RestController@RequestMapping(\"/jsonresult\")public class JsonResultController &#123; @RequestMapping(\"/user\") public JsonResult&lt;User&gt; getUser() &#123; User user = new User(1, \"倪升武\", \"123456\"); return new JsonResult&lt;&gt;(user); &#125; @RequestMapping(\"/list\") public JsonResult&lt;List&gt; getUserList() &#123; List&lt;User&gt; userList = new ArrayList&lt;&gt;(); User user1 = new User(1, \"倪升武\", \"123456\"); User user2 = new User(2, \"达人课\", \"123456\"); userList.add(user1); userList.add(user2); return new JsonResult&lt;&gt;(userList, \"获取用户列表成功\"); &#125; @RequestMapping(\"/map\") public JsonResult&lt;Map&gt; getMap() &#123; Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(3); User user = new User(1, \"倪升武\", null); map.put(\"作者信息\", user); map.put(\"博客地址\", \"http://blog.itcodai.com\"); map.put(\"CSDN地址\", null); map.put(\"粉丝数量\", 4153); return new JsonResult&lt;&gt;(map); &#125;&#125; 我们重新在浏览器中输入：localhost:8080/jsonresult/user 返回 json 如下： &#123;\"code\":\"0\",\"data\":&#123;\"id\":1,\"password\":\"123456\",\"username\":\"倪升武\"&#125;,\"msg\":\"操作成功！\"&#125; 输入：localhost:8080/jsonresult/list，返回 json 如下： &#123;\"code\":\"0\",\"data\":[&#123;\"id\":1,\"password\":\"123456\",\"username\":\"倪升武\"&#125;,&#123;\"id\":2,\"password\":\"123456\",\"username\":\"达人课\"&#125;],\"msg\":\"获取用户列表成功\"&#125; 输入：localhost:8080/jsonresult/map，返回 json 如下： &#123;\"code\":\"0\",\"data\":&#123;\"作者信息\":&#123;\"id\":1,\"password\":\"\",\"username\":\"倪升武\"&#125;,\"CSDN地址\":null,\"粉丝数量\":4153,\"博客地址\":\"http://blog.itcodai.com\"&#125;,\"msg\":\"操作成功！\"&#125; 通过封装，我们不但将数据通过 json 传给前端或者其他接口，还带上了状态码和提示信息，这在实际项目场景中应用非常广泛。 4. 总结本节主要对 Spring Boot 中 json 数据的返回做了详细的分析，从 Spring Boot 默认的 jackson 框架到阿里巴巴的 fastJson 框架，分别对它们的配置做了相应的讲解。另外，结合实际项目情况，总结了实际项目中使用的 json 封装结构体，加入了状态码和提示信息，使得返回的 json 数据信息更加完整。课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第03课：Spring Boot使用slf4j进行日志记录在开发中，我们经常使用 System.out.println() 来打印一些信息，但是这样不好，因为大量的使用 System.out 会增加资源的消耗。我们实际项目中使用的是 slf4j 的 logback 来输出日志，效率挺高的，Spring Boot 提供了一套日志系统，logback 是最优的选择。 1. slf4j 介绍引用百度百科里的一段话： SLF4J，即简单日志门面（Simple Logging Facade for Java），不是具体的日志解决方案，它只服务于各种各样的日志系统。按照官方的说法，SLF4J是一个用于日志系统的简单Facade，允许最终用户在部署其应用时使用其所希望的日志系统。 这段的大概意思是：你只需要按统一的方式写记录日志的代码，而无需关心日志是通过哪个日志系统，以什么风格输出的。因为它们取决于部署项目时绑定的日志系统。例如，在项目中使用了 slf4j 记录日志，并且绑定了 log4j（即导入相应的依赖），则日志会以 log4j 的风格输出；后期需要改为以 logback 的风格输出日志，只需要将 log4j 替换成 logback 即可，不用修改项目中的代码。这对于第三方组件的引入的不同日志系统来说几乎零学习成本，况且它的优点不仅仅这一个而已，还有简洁的占位符的使用和日志级别的判断。 正因为 sfl4j 有如此多的优点，阿里巴巴已经将 slf4j 作为他们的日志框架了。在《阿里巴巴Java开发手册(正式版)》中，日志规约一项第一条就强制要求使用 slf4j： 1.【强制】应用中不可直接使用日志系统（Log4j、Logback）中的API，而应依赖使用日志框架SLF4J中的API，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。 “强制”两个字体现出了 slf4j 的优势，所以建议在实际项目中，使用 slf4j 作为自己的日志框架。使用 slf4j 记录日志非常简单，直接使用 LoggerFactory 创建即可。 import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class Test &#123; private static final Logger logger = LoggerFactory.getLogger(Test.class); // ……&#125; 2. application.yml 中对日志的配置Spring Boot 对 slf4j 支持的很好，内部已经集成了 slf4j，一般我们在使用的时候，会对slf4j 做一下配置。application.yml 文件是 Spring Boot 中唯一一个需要配置的文件，一开始创建工程的时候是 application.properties 文件，个人比较细化用 yml 文件，因为 yml 文件的层次感特别好，看起来更直观，但是 yml 文件对格式要求比较高，比如英文冒号后面必须要有个空格，否则项目估计无法启动，而且也不报错。用 properties 还是 yml 视个人习惯而定，都可以。本课程使用 yml。 我们看一下 application.yml 文件中对日志的配置： logging: config: logback.xml level: com.itcodai.course03.dao: trace logging.config 是用来指定项目启动的时候，读取哪个配置文件，这里指定的是日志配置文件是根路径下的 logback.xml 文件，关于日志的相关配置信息，都放在 logback.xml 文件中了。logging.level 是用来指定具体的 mapper 中日志的输出级别，上面的配置表示 com.itcodai.course03.dao 包下的所有 mapper 日志输出级别为 trace，会将操作数据库的 sql 打印出来，开发时设置成 trace 方便定位问题，在生产环境上，将这个日志级别再设置成 error 级别即可（本节课不讨论 mapper 层，在后面 Spring Boot 集成 MyBatis 时再详细讨论）。 常用的日志级别按照从高到低依次为：ERROR、WARN、INFO、DEBUG。 3. logback.xml 配置文件解析在上面 application.yml 文件中，我们指定了日志配置文件 logback.xml，logback.xml 文件中主要用来做日志的相关配置。在 logback.xml 中，我们可以定义日志输出的格式、路径、控制台输出格式、文件大小、保存时长等等。下面来分析一下： 3.1 定义日志输出格式和存储路径&lt;configuration&gt; &lt;property name=\"LOG_PATTERN\" value=\"%date&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n\" /&gt; &lt;property name=\"FILE_PATH\" value=\"D:/logs/course03/demo.%d&#123;yyyy-MM-dd&#125;.%i.log\" /&gt;&lt;/configuration&gt; 我们来看一下这个定义的含义：首先定义一个格式，命名为 “LOG_PATTERN”，该格式中 %date 表示日期，%thread 表示线程名，%-5level 表示级别从左显示5个字符宽度，%logger{36} 表示 logger 名字最长36个字符，%msg 表示日志消息，%n 是换行符。 然后再定义一下名为 “FILE_PATH” 文件路径，日志都会存储在该路径下。%i 表示第 i 个文件，当日志文件达到指定大小时，会将日志生成到新的文件里，这里的 i 就是文件索引，日志文件允许的大小可以设置，下面会讲解。这里需要注意的是，不管是 windows 系统还是 Linux 系统，日志存储的路径必须要是绝对路径。 3.2 定义控制台输出&lt;configuration&gt; &lt;appender name=\"CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder&gt; &lt;!-- 按照上面配置的LOG_PATTERN来打印日志 --&gt; &lt;pattern&gt;$&#123;LOG_PATTERN&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt;&lt;/configuration&gt; 使用 &lt;appender&gt; 节点设置个控制台输出（class=&quot;ch.qos.logback.core.ConsoleAppender&quot;）的配置，定义为 “CONSOLE”。使用上面定义好的输出格式（LOG_PATTERN）来输出，使用 ${} 引用进来即可。 3.3 定义日志文件的相关参数&lt;configuration&gt; &lt;appender name=\"FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!-- 按照上面配置的FILE_PATH路径来保存日志 --&gt; &lt;fileNamePattern&gt;$&#123;FILE_PATH&#125;&lt;/fileNamePattern&gt; &lt;!-- 日志保存15天 --&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;!-- 单个日志文件的最大，超过则新建日志文件存储 --&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;!-- 按照上面配置的LOG_PATTERN来打印日志 --&gt; &lt;pattern&gt;$&#123;LOG_PATTERN&#125;&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt;&lt;/configuration&gt; 使用 &lt;appender&gt; 定义一个名为 “FILE” 的文件配置，主要是配置日志文件保存的时间、单个日志文件存储的大小、以及文件保存的路径和日志的输出格式。 3.4 定义日志输出级别&lt;configuration&gt; &lt;logger name=\"com.itcodai.course03\" level=\"INFO\" /&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"CONSOLE\" /&gt; &lt;appender-ref ref=\"FILE\" /&gt; &lt;/root&gt;&lt;/configuration&gt; 有了上面那些定义后，最后我们使用 &lt;logger&gt; 来定义一下项目中默认的日志输出级别，这里定义级别为 INFO，然后针对 INFO 级别的日志，使用 &lt;root&gt; 引用上面定义好的控制台日志输出和日志文件的参数。这样 logback.xml 文件中的配置就设置完了。 4. 使用Logger在项目中打印日志在代码中，我们一般使用 Logger 对象来打印出一些 log 信息，可以指定打印出的日志级别，也支持占位符，很方便。 import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(\"/test\")public class TestController &#123; private final static Logger logger = LoggerFactory.getLogger(TestController.class); @RequestMapping(\"/log\") public String testLog() &#123; logger.debug(\"=====测试日志debug级别打印====\"); logger.info(\"======测试日志info级别打印=====\"); logger.error(\"=====测试日志error级别打印====\"); logger.warn(\"======测试日志warn级别打印=====\"); // 可以使用占位符打印出一些参数信息 String str1 = \"blog.itcodai.com\"; String str2 = \"blog.csdn.net/eson_15\"; logger.info(\"======倪升武的个人博客：&#123;&#125;；倪升武的CSDN博客：&#123;&#125;\", str1, str2); return \"success\"; &#125;&#125; 启动该项目，在浏览器中输入 localhost:8080/test/log 后可以看到控制台的日志记录： ======测试日志info级别打印==========测试日志error级别打印==========测试日志warn级别打印===========倪升武的个人博客：blog.itcodai.com；倪升武的CSDN博客：blog.csdn.net/eson_15 因为 INFO 级别比 DEBUG 级别高，所以 debug 这条没有打印出来，如果将 logback.xml 中的日志级别设置成 DEBUG，那么四条语句都会打印出来，这个大家自己去测试了。同时可以打开 D:\\logs\\course03\\ 目录，里面有刚刚项目启动，以后后面生成的所有日志记录。在项目部署后，我们大部分都是通过查看日志文件来定位问题。 5. 总结本节课主要对 slf4j 做了一个简单的介绍，并且对 Spring Boot 中如何使用 slf4j 输出日志做了详细的说明，着重分析了 logback.xml 文件中对日志相关信息的配置，包括日志的不同级别。最后针对这些配置，在代码中使用 Logger 打印出一些进行测试。在实际项目中，这些日志都是排查问题的过程中非常重要的资料。课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第04课：Spring Boot中的项目属性配置我们知道，在项目中，很多时候需要用到一些配置的信息，这些信息可能在测试环境和生产环境下会有不同的配置，后面根据实际业务情况有可能还会做修改，针对这种情况，我们不能将这些配置在代码中写死，最好就是写到配置文件中。比如可以把这些信息写到 application.yml 文件中。 1. 少量配置信息的情形举个例子，在微服务架构中，最常见的就是某个服务需要调用其他服务来获取其提供的相关信息，那么在该服务的配置文件中需要配置被调用的服务地址，比如在当前服务里，我们需要调用订单微服务获取订单相关的信息，假设 订单服务的端口号是 8002，那我们可以做如下配置： server: port: 8001# 配置微服务的地址url: # 订单微服务的地址 orderUrl: http://localhost:8002 然后在业务代码中如何获取到这个配置的订单服务地址呢？我们可以使用 @Value 注解来解决。在对应的类中加上一个属性，在属性上使用 @Value 注解即可获取到配置文件中的配置信息，如下： import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Value;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(\"/test\")public class ConfigController &#123; private static final Logger LOGGER = LoggerFactory.getLogger(ConfigController.class); @Value(\"$&#123;url.orderUrl&#125;\") private String orderUrl; @RequestMapping(\"/config\") public String testConfig() &#123; LOGGER.info(\"=====获取的订单服务地址为：&#123;&#125;\", orderUrl); return \"success\"; &#125;&#125; @Value 注解上通过 ${key} 即可获取配置文件中和 key 对应的 value 值。我们启动一下项目，在浏览器中输入 localhost:8080/test/config 请求服务后，可以看到控制台会打印出订单服务的地址： &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;获取的订单服务地址为：http:&#x2F;&#x2F;localhost:8002 说明我们成功获取到了配置文件中的订单微服务地址，在实际项目中也是这么用的，后面如果因为服务器部署的原因，需要修改某个服务的地址，那么只要在配置文件中修改即可。 2. 多个配置信息的情形这里再引申一个问题，随着业务复杂度的增加，一个项目中可能会有越来越多的微服务，某个模块可能需要调用多个微服务获取不同的信息，那么就需要在配置文件中配置多个微服务的地址。可是，在需要调用这些微服务的代码中，如果这样一个个去使用 @Value 注解引入相应的微服务地址的话，太过于繁琐，也不科学。 所以，在实际项目中，业务繁琐，逻辑复杂的情况下，需要考虑封装一个或多个配置类。举个例子：假如在当前服务中，某个业务需要同时调用订单微服务、用户微服务和购物车微服务，分别获取订单、用户和购物车相关信息，然后对这些信息做一定的逻辑处理。那么在配置文件中，我们需要将这些微服务的地址都配置好： # 配置多个微服务的地址url: # 订单微服务的地址 orderUrl: http://localhost:8002 # 用户微服务的地址 userUrl: http://localhost:8003 # 购物车微服务的地址 shoppingUrl: http://localhost:8004 也许实际业务中，远远不止这三个微服务，甚至十几个都有可能。对于这种情况，我们可以先定义一个 MicroServiceUrl 类来专门保存微服务的 url，如下： @Component@ConfigurationProperties(prefix = \"url\")public class MicroServiceUrl &#123; private String orderUrl; private String userUrl; private String shoppingUrl; // 省去get和set方法&#125; 细心的朋友应该可以看到，使用 @ConfigurationProperties 注解并且使用 prefix 来指定一个前缀，然后该类中的属性名就是配置中去掉前缀后的名字，一一对应即可。即：前缀名 + 属性名就是配置文件中定义的 key。同时，该类上面需要加上 @Component 注解，把该类作为组件放到Spring容器中，让 Spring 去管理，我们使用的时候直接注入即可。 需要注意的是，使用 @ConfigurationProperties 注解需要导入它的依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; OK，到此为止，我们将配置写好了，接下来写个 Controller 来测试一下。此时，不需要在代码中一个个引入这些微服务的 url 了，直接通过 @Resource 注解将刚刚写好配置类注入进来即可使用了，非常方便。如下： @RestController@RequestMapping(\"/test\")public class TestController &#123; private static final Logger LOGGER = LoggerFactory.getLogger(TestController.class); @Resource private MicroServiceUrl microServiceUrl; @RequestMapping(\"/config\") public String testConfig() &#123; LOGGER.info(\"=====获取的订单服务地址为：&#123;&#125;\", microServiceUrl.getOrderUrl()); LOGGER.info(\"=====获取的用户服务地址为：&#123;&#125;\", microServiceUrl.getUserUrl()); LOGGER.info(\"=====获取的购物车服务地址为：&#123;&#125;\", microServiceUrl.getShoppingUrl()); return \"success\"; &#125;&#125; 再次启动项目，请求一下可以看到，控制台打印出如下信息，说明配置文件生效，同时正确获取配置文件内容： &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;获取的订单服务地址为：http:&#x2F;&#x2F;localhost:8002&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;获取的订单服务地址为：http:&#x2F;&#x2F;localhost:8002&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;获取的用户服务地址为：http:&#x2F;&#x2F;localhost:8003&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;获取的购物车服务地址为：http:&#x2F;&#x2F;localhost:8004 3. 指定项目配置文件我们知道，在实际项目中，一般有两个环境：开发环境和生产环境。开发环境中的配置和生产环境中的配置往往不同，比如：环境、端口、数据库、相关地址等等。我们不可能在开发环境调试好之后，部署到生产环境后，又要将配置信息全部修改成生产环境上的配置，这样太麻烦，也不科学。 最好的解决方法就是开发环境和生产环境都有一套对用的配置信息，然后当我们在开发时，指定读取开发环境的配置，当我们将项目部署到服务器上之后，再指定去读取生产环境的配置。 我们新建两个配置文件： application-dev.yml 和 application-pro.yml，分别用来对开发环境和生产环境进行相关配置。这里为了方便，我们分别设置两个访问端口号，开发环境用 8001，生产环境用 8002. # 开发环境配置文件server: port: 8001 # 开发环境配置文件server: port: 8002 然后在 application.yml 文件中指定读取哪个配置文件即可。比如我们在开发环境下，指定读取 applicationn-dev.yml 文件，如下： spring: profiles: active: - dev 这样就可以在开发的时候，指定读取 application-dev.yml 文件，访问的时候使用 8001 端口，部署到服务器后，只需要将 application.yml 中指定的文件改成 application-pro.yml 即可，然后使用 8002 端口访问，非常方便。 4. 总结本节课主要讲解了 Spring Boot 中如何在业务代码中读取相关配置，包括单一配置和多个配置项，在微服务中，这种情况非常常见，往往会有很多其他微服务需要调用，所以封装一个配置类来接收这些配置是个很好的处理方式。除此之外，例如数据库相关的连接参数等等，也可以放到一个配置类中，其他遇到类似的场景，都可以这么处理。最后介绍了开发环境和生产环境配置的快速切换方式，省去了项目部署时，诸多配置信息的修改。课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第05课：Spring Boot中的MVC支持Spring Boot 的 MVC 支持主要来介绍实际项目中最常用的几个注解，包括 @RestController、 @RequestMapping、@PathVariable、@RequestParam 以及 @RequestBody。主要介绍这几个注解常用的使用方式和特点。 1. @RestController@RestController 是 Spring Boot 新增的一个注解，我们看一下该注解都包含了哪些东西。 @Target(&#123;ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Controller@ResponseBodypublic @interface RestController &#123; String value() default \"\";&#125; 可以看出， @RestController 注解包含了原来的 @Controller 和 @ResponseBody 注解，使用过 Spring 的朋友对 @Controller 注解已经非常了解了，这里不再赘述， @ResponseBody 注解是将返回的数据结构转换为 Json 格式。所以 @RestController 可以看作是 @Controller 和 @ResponseBody 的结合体，相当于偷个懒，我们使用 @RestController 之后就不用再使用 @Controller 了。但是需要注意一个问题：如果是前后端分离，不用模板渲染的话，比如 Thymeleaf，这种情况下是可以直接使用@RestController 将数据以 json 格式传给前端，前端拿到之后解析；但如果不是前后端分离，需要使用模板来渲染的话，一般 Controller 中都会返回到具体的页面，那么此时就不能使用@RestController了，比如： public String getUser() &#123; return \"user\";&#125; 其实是需要返回到 user.html 页面的，如果使用 @RestController 的话，会将 user 作为字符串返回的，所以这时候我们需要使用 @Controller 注解。这在下一节 Spring Boot 集成 Thymeleaf 模板引擎中会再说明。 2. @RequestMapping@RequestMapping 是一个用来处理请求地址映射的注解，它可以用于类上，也可以用于方法上。在类的级别上的注解会将一个特定请求或者请求模式映射到一个控制器之上，表示类中的所有响应请求的方法都是以该地址作为父路径；在方法的级别表示进一步指定到处理方法的映射关系。 该注解有6个属性，一般在项目中比较常用的有三个属性：value、method 和 produces。 value 属性：指定请求的实际地址，value 可以省略不写 method 属性：指定请求的类型，主要有 GET、PUT、POST、DELETE，默认为 GET produces属性：指定返回内容类型，如 produces = “application/json; charset=UTF-8” @RequestMapping 注解比较简单，举个例子： @RestController@RequestMapping(value = \"/test\", produces = \"application/json; charset=UTF-8\")public class TestController &#123; @RequestMapping(value = \"/get\", method = RequestMethod.GET) public String testGet() &#123; return \"success\"; &#125;&#125; 这个很简单，启动项目在浏览器中输入 localhost:8080/test/get 测试一下即可。 针对四种不同的请求方式，是有相应注解的，不用每次在 @RequestMapping 注解中加 method 属性来指定，上面的 GET 方式请求可以直接使用 @GetMapping(&quot;/get&quot;) 注解，效果一样。相应地，PUT 方式、POST 方式和 DELETE 方式对应的注解分别为 @PutMapping、@PostMapping 和 DeleteMapping。 3. @PathVariable@PathVariable 注解主要是用来获取 url 参数，Spring Boot 支持 restfull 风格的 url，比如一个 GET 请求携带一个参数 id 过来，我们将 id 作为参数接收，可以使用 @PathVariable 注解。如下： @GetMapping(\"/user/&#123;id&#125;\")public String testPathVariable(@PathVariable Integer id) &#123; System.out.println(\"获取到的id为：\" + id); return \"success\";&#125; 这里需要注意一个问题，如果想要 url 中占位符中的 id 值直接赋值到参数 id 中，需要保证 url 中的参数和方法接收参数一致，否则就无法接收。如果不一致的话，其实也可以解决，需要用 @PathVariable 中的 value 属性来指定对应关系。如下： @RequestMapping(\"/user/&#123;idd&#125;\")public String testPathVariable(@PathVariable(value = \"idd\") Integer id) &#123; System.out.println(\"获取到的id为：\" + id); return \"success\";&#125; 对于访问的 url，占位符的位置可以在任何位置，不一定非要在最后，比如这样也行：/xxx/{id}/user。另外，url 也支持多个占位符，方法参数使用同样数量的参数来接收，原理和一个参数是一样的，例如： @GetMapping(\"/user/&#123;idd&#125;/&#123;name&#125;\") public String testPathVariable(@PathVariable(value = \"idd\") Integer id, @PathVariable String name) &#123; System.out.println(\"获取到的id为：\" + id); System.out.println(\"获取到的name为：\" + name); return \"success\"; &#125; 运行项目，在浏览器中请求 localhost:8080/test/user/2/zhangsan 可以看到控制台输出如下信息： 获取到的id为：2获取到的name为：zhangsan 所以支持多个参数的接收。同样地，如果 url 中的参数和方法中的参数名称不同的话，也需要使用 value 属性来绑定两个参数。 4. @RequestParam@RequestParam 注解顾名思义，也是获取请求参数的，上面我们介绍了 @PathValiable 注解也是获取请求参数的，那么 @RequestParam 和 @PathVariable 有什么不同呢？主要区别在于： @PathValiable 是从 url 模板中获取参数值， 即这种风格的 url：http://localhost:8080/user/{id} ；而 @RequestParam 是从 request 里面获取参数值，即这种风格的 url：http://localhost:8080/user?id=1 。我们使用该 url 带上参数 id 来测试一下如下代码： @GetMapping(\"/user\")public String testRequestParam(@RequestParam Integer id) &#123; System.out.println(\"获取到的id为：\" + id); return \"success\";&#125; 可以正常从控制台打印出 id 信息。同样地，url 上面的参数和方法的参数需要一致，如果不一致，也需要使用 value 属性来说明，比如 url 为：http://localhost:8080/user?idd=1 @RequestMapping(\"/user\")public String testRequestParam(@RequestParam(value = \"idd\", required = false) Integer id) &#123; System.out.println(\"获取到的id为：\" + id); return \"success\";&#125; 除了 value 属性外，还有个两个属性比较常用： required 属性：true 表示该参数必须要传，否则就会报 404 错误，false 表示可有可无。 defaultValue 属性：默认值，表示如果请求中没有同名参数时的默认值。 从 url 中可以看出，@RequestParam 注解用于 GET 请求上时，接收拼接在 url 中的参数。除此之外，该注解还可以用于 POST 请求，接收前端表单提交的参数，假如前端通过表单提交 username 和 password 两个参数，那我们可以使用 @RequestParam 来接收，用法和上面一样。 @PostMapping(\"/form1\") public String testForm(@RequestParam String username, @RequestParam String password) &#123; System.out.println(\"获取到的username为：\" + username); System.out.println(\"获取到的password为：\" + password); return \"success\"; &#125; 我们使用 postman 来模拟一下表单提交，测试一下接口： 那么问题来了，如果表单数据很多，我们不可能在后台方法中写上很多参数，每个参数还要 @RequestParam 注解。针对这种情况，我们需要封装一个实体类来接收这些参数，实体中的属性名和表单中的参数名一致即可。 public class User &#123; private String username; private String password; // set get&#125; 使用实体接收的话，我们不能在前面加 @RequestParam 注解了，直接使用即可。 @PostMapping(\"/form2\") public String testForm(User user) &#123; System.out.println(\"获取到的username为：\" + user.getUsername()); System.out.println(\"获取到的password为：\" + user.getPassword()); return \"success\"; &#125; 使用 postman 再次测试一下表单提交，观察一下返回值和控制台打印出的日志即可。在实际项目中，一般都是封装一个实体类来接收表单数据，因为实际项目中表单数据一般都很多。 5. @RequestBody@RequestBody 注解用于接收前端传来的实体，接收参数也是对应的实体，比如前端通过 json 提交传来两个参数 username 和 password，此时我们需要在后端封装一个实体来接收。在传递的参数比较多的情况下，使用 @RequestBody 接收会非常方便。例如： public class User &#123; private String username; private String password; // set get&#125; @PostMapping(\"/user\")public String testRequestBody(@RequestBody User user) &#123; System.out.println(\"获取到的username为：\" + user.getUsername()); System.out.println(\"获取到的password为：\" + user.getPassword()); return \"success\";&#125; 我们使用 postman 工具来测试一下效果，打开 postman，然后输入请求地址和参数，参数我们用 json 来模拟，如下图所有，调用之后返回 success。 同时看一下后台控制台输出的日志： 获取到的username为：倪升武获取到的password为：123456 可以看出，@RequestBody 注解用于 POST 请求上，接收 json 实体参数。它和上面我们介绍的表单提交有点类似，只不过参数的格式不同，一个是 json 实体，一个是表单提交。在实际项目中根据具体场景和需要使用对应的注解即可。 6. 总结本节课主要讲解了 Spring Boot 中对 MVC 的支持，分析了 @RestController、 @RequestMapping、@PathVariable、 @RequestParam 和 @RequestBody 四个注解的使用方式，由于 @RestController 中集成了 @ResponseBody 所以对返回 json 的注解不再赘述。以上四个注解是使用频率很高的注解，在所有的实际项目中基本都会遇到，要熟练掌握。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第06课：Spring Boot集成 Swagger2 展现在线接口文档1. Swagger 简介1.1 解决的问题随着互联网技术的发展，现在的网站架构基本都由原来的后端渲染，变成了前后端分离的形态，而且前端技术和后端技术在各自的道路上越走越远。前端和后端的唯一联系，变成了 API 接口，所以 API 文档变成了前后端开发人员联系的纽带，变得越来越重要。 那么问题来了，随着代码的不断更新，开发人员在开发新的接口或者更新旧的接口后，由于开发任务的繁重，往往文档很难持续跟着更新，Swagger 就是用来解决该问题的一款重要的工具，对使用接口的人来说，开发人员不需要给他们提供文档，只要告诉他们一个 Swagger 地址，即可展示在线的 API 接口文档，除此之外，调用接口的人员还可以在线测试接口数据，同样地，开发人员在开发接口时，同样也可以利用 Swagger 在线接口文档测试接口数据，这给开发人员提供了便利。 1.2 Swagger 官方我们打开 Swagger 官网，官方对 Swagger 的定义为： The Best APIs are Built with Swagger Tools 翻译成中文是：“最好的 API 是使用 Swagger 工具构建的”。由此可见，Swagger 官方对其功能和所处的地位非常自信，由于其非常好用，所以官方对其定位也合情合理。如下图所示： 本文主要讲解在 Spring Boot 中如何导入 Swagger2 工具来展现项目中的接口文档。本节课使用的 Swagger 版本为 2.2.2。下面开始进入 Swagger2 之旅。 2. Swagger2 的 maven 依赖使用 Swagger2 工具，必须要导入 maven 依赖，当前官方最高版本是 2.8.0，我尝试了一下，个人感觉页面展示的效果不太好，而且不够紧凑，不利于操作。另外，最新版本并不一定是最稳定版本，当前我们实际项目中使用的是 2.2.2 版本，该版本稳定，界面友好，所以本节课主要围绕着 2.2.2 版本来展开，依赖如下： &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt; 3. Swagger2 的配置使用 Swagger2 需要进行配置，Spring Boot 中对 Swagger2 的配置非常方便，新建一个配置类，Swagger2 的配置类上除了添加必要的 @Configuration 注解外，还需要添加 @EnableSwagger2 注解。 import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import springfox.documentation.builders.ApiInfoBuilder;import springfox.documentation.builders.PathSelectors;import springfox.documentation.builders.RequestHandlerSelectors;import springfox.documentation.service.ApiInfo;import springfox.documentation.spi.DocumentationType;import springfox.documentation.spring.web.plugins.Docket;import springfox.documentation.swagger2.annotations.EnableSwagger2;/** * @author shengwu ni */@Configuration@EnableSwagger2public class SwaggerConfig &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) // 指定构建api文档的详细信息的方法：apiInfo() .apiInfo(apiInfo()) .select() // 指定要生成api接口的包路径，这里把controller作为包路径，生成controller中的所有接口 .apis(RequestHandlerSelectors.basePackage(\"com.itcodai.course06.controller\")) .paths(PathSelectors.any()) .build(); &#125; /** * 构建api文档的详细信息 * @return */ private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() // 设置页面标题 .title(\"Spring Boot集成Swagger2接口总览\") // 设置接口描述 .description(\"跟武哥一起学Spring Boot第06课\") // 设置联系方式 .contact(\"倪升武，\" + \"CSDN：http://blog.csdn.net/eson_15\") // 设置版本 .version(\"1.0\") // 构建 .build(); &#125;&#125; 在该配置类中，已经使用注释详细解释了每个方法的作用了，在此不再赘述。到此为止，我们已经配置好了 Swagger2 了。现在我们可以测试一下配置有没有生效，启动项目，在浏览器中输入 localhost:8080/swagger-ui.html，即可看到 swagger2 的接口页面，如下图所示，说明Swagger2 集成成功。 结合该图，对照上面的 Swagger2 配置文件中的配置，可以很明确的知道配置类中每个方法的作用。这样就很容易理解和掌握 Swagger2 中的配置了，也可以看出，其实 Swagger2 配置很简单。 【友情提示】可能有很多朋友在配置 Swagger 的时候会遇到下面的情况，而且还关不掉的，这是因为浏览器缓存引起的，清空一下浏览器缓存即可解决问题。 4. Swagger2 的使用上面我们已经配置好了 Swagger2，并且也启动测试了一下，功能正常，下面我们开始使用 Swagger2，主要来介绍 Swagger2 中的几个常用的注解，分别在实体类上、 Controller 类上以及 Controller 中的方法上，最后我们看一下 Swagger2 是如何在页面上呈现在线接口文档的，并且结合 Controller 中的方法在接口中测试一下数据。 4.1 实体类注解本节我们建一个 User 实体类，主要介绍一下 Swagger2 中的 @ApiModel 和 @ApiModelProperty 注解，同时为后面的测试做准备。 import io.swagger.annotations.ApiModel;import io.swagger.annotations.ApiModelProperty;@ApiModel(value = \"用户实体类\")public class User &#123; @ApiModelProperty(value = \"用户唯一标识\") private Long id; @ApiModelProperty(value = \"用户姓名\") private String username; @ApiModelProperty(value = \"用户密码\") private String password; // 省略set和get方法&#125; 解释下 @ApiModel 和 @ApiModelProperty 注解： @ApiModel 注解用于实体类，表示对类进行说明，用于参数用实体类接收。@ApiModelProperty 注解用于类中属性，表示对 model 属性的说明或者数据操作更改。 该注解在在线 API 文档中的具体效果在下文说明。 4.2 Controller 类中相关注解我们写一个 TestController，再写几个接口，然后学习一下 Controller 中和 Swagger2 相关的注解。 import com.itcodai.course06.entiy.JsonResult;import com.itcodai.course06.entiy.User;import io.swagger.annotations.Api;import io.swagger.annotations.ApiOperation;import io.swagger.annotations.ApiParam;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(\"/swagger\")@Api(value = \"Swagger2 在线接口文档\")public class TestController &#123; @GetMapping(\"/get/&#123;id&#125;\") @ApiOperation(value = \"根据用户唯一标识获取用户信息\") public JsonResult&lt;User&gt; getUserInfo(@PathVariable @ApiParam(value = \"用户唯一标识\") Long id) &#123; // 模拟数据库中根据id获取User信息 User user = new User(id, \"倪升武\", \"123456\"); return new JsonResult(user); &#125;&#125; 我们来学习一下 @Api 、 @ApiOperation 和 @ApiParam 注解。 @Api 注解用于类上，表示标识这个类是 swagger 的资源。@ApiOperation 注解用于方法，表示一个 http 请求的操作。@ApiParam 注解用于参数上，用来标明参数信息。 这里返回的是 JsonResult，是第02课中学习返回 json 数据时封装的实体。以上是 Swagger 中最常用的 5 个注解，接下来运行一下项目工程，在浏览器中输入 localhost:8080/swagger-ui.html 看一下 Swagger 页面的接口状态。 可以看出，Swagger 页面对该接口的信息展示的非常全面，每个注解的作用以及展示的地方在上图中已经标明，通过页面即可知道该接口的所有信息，那么我们直接在线测试一下该接口返回的信息，输入id为1，看一下返回数据： 可以看出，直接在页面返回了 json 格式的数据，开发人员可以直接使用该在线接口来测试数据的正确与否，非常方便。上面是对于单个参数的输入，如果输入参数为某个对象这种情况，Swagger 是什么样子呢？我们再写一个接口。 @PostMapping(\"/insert\") @ApiOperation(value = \"添加用户信息\") public JsonResult&lt;Void&gt; insertUser(@RequestBody @ApiParam(value = \"用户信息\") User user) &#123; // 处理添加逻辑 return new JsonResult&lt;&gt;(); &#125; 重启项目，在浏览器中输入 localhost:8080/swagger-ui.html 看一下效果： 5. 总结OK，本节课详细分析了 Swagger 的优点，以及 Spring Boot 如何集成 Swagger2，包括配置，相关注解的讲解，涉及到了实体类和接口类，以及如何使用。最后通过页面测试，体验了 Swagger 的强大之处，基本上是每个项目组中必备的工具之一，所以要掌握该工具的使用，也不难。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第07课：Spring Boot集成Thymeleaf模板引擎1. Thymeleaf 介绍 Thymeleaf 是适用于 Web 和独立环境的现代服务器端 Java 模板引擎。Thymeleaf 的主要目标是为您的开发工作流程带来优雅的自然模板 - 可以在浏览器中正确显示的HTML，也可以用作静态原型，从而在开发团队中实现更强大的协作。 以上翻译自 Thymeleaf 官方网站。传统的 JSP+JSTL 组合是已经过去了，Thymeleaf 是现代服务端的模板引擎，与传统的 JSP 不同，Thymeleaf 可以使用浏览器直接打开，因为可以忽略掉拓展属性，相当于打开原生页面，给前端人员也带来一定的便利。 什么意思呢？就是说在本地环境或者有网络的环境下，Thymeleaf 均可运行。由于 thymeleaf 支持 html 原型，也支持在 html 标签里增加额外的属性来达到 “模板+数据” 的展示方式，所以美工可以直接在浏览器中查看页面效果，当服务启动后，也可以让后台开发人员查看带数据的动态页面效果。比如： &lt;div class=\"ui right aligned basic segment\"&gt; &lt;div class=\"ui orange basic label\" th:text=\"$&#123;blog.flag&#125;\"&gt;静态原创信息&lt;/div&gt;&lt;/div&gt;&lt;h2 class=\"ui center aligned header\" th:text=\"$&#123;blog.title&#125;\"&gt;这是静态标题&lt;/h2&gt; 类似与上面这样，在静态页面时，会展示静态信息，当服务启动后，动态获取数据库中的数据后，就可以展示动态数据，th:text 标签是用来动态替换文本的，这会在下文说明。该例子说明浏览器解释 html 时会忽略 html 中未定义的标签属性（比如 th:text），所以 thymeleaf 的模板可以静态地运行；当有数据返回到页面时，Thymeleaf 标签会动态地替换掉静态内容，使页面动态显示数据。 2. 依赖导入在 Spring Boot 中使用 thymeleaf 模板需要引入依赖，可以在创建项目工程时勾选 Thymeleaf，也可以创建之后再手动导入，如下： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt;&lt;/dependency&gt; 另外，在 html 页面上如果要使用 thymeleaf 模板，需要在页面标签中引入： &lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt; 3. Thymeleaf相关配置因为 Thymeleaf 中已经有默认的配置了，我们不需要再对其做过多的配置，有一个需要注意一下，Thymeleaf 默认是开启页面缓存的，所以在开发的时候，需要关闭这个页面缓存，配置如下。 spring: thymeleaf: cache: false #关闭缓存 否则会有缓存，导致页面没法及时看到更新后的效果。 比如你修改了一个文件，已经 update 到 tomcat 了，但刷新页面还是之前的页面，就是因为缓存引起的。 4. Thymeleaf 的使用4.1 访问静态页面这个和 Thymeleaf 没啥关系，应该说是通用的，我把它一并写到这里的原因是一般我们做网站的时候，都会做一个 404 页面和 500 页面，为了出错时给用户一个友好的展示，而不至于一堆异常信息抛出来。Spring Boot 中会自动识别模板目录（templates/）下的 404.html 和 500.html 文件。我们在 templates/ 目录下新建一个 error 文件夹，专门放置错误的 html 页面，然后分别打印些信息。以 404.html 为例： &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; 这是404页面&lt;/body&gt;&lt;/html&gt; 我们再写一个 controller 来测试一下 404 和 500 页面： @Controller@RequestMapping(\"/thymeleaf\")public class ThymeleafController &#123; @RequestMapping(\"/test404\") public String test404() &#123; return \"index\"; &#125; @RequestMapping(\"/test500\") public String test500() &#123; int i = 1 / 0; return \"index\"; &#125;&#125; 当我们在浏览器中输入 localhost:8080/thymeleaf/test400 时，故意输入错误，找不到对应的方法，就会跳转到 404.html 显示。当我们在浏览器中输入 localhost:8088/thymeleaf/test505 时，会抛出异常，然后会自动跳转到 500.html 显示。 【注】这里有个问题需要注意一下，前面的课程中我们说了微服务中会走向前后端分离，我们在 Controller 层上都是使用的 @RestController 注解，自动会把返回的数据转成 json 格式。但是在使用模板引擎时，Controller 层就不能用 @RestController 注解了，因为在使用 thymeleaf 模板时，返回的是视图文件名，比如上面的 Controller 中是返回到 index.html 页面，如果使用 @RestController 的话，会把 index 当作 String 解析了，直接返回到页面了，而不是去找 index.html 页面，大家可以试一下。所以在使用模板时要用 @Controller 注解。 4.2 Thymeleaf 中处理对象我们来看一下 thymeleaf 模板中如何处理对象信息，假如我们在做个人博客的时候，需要给前端传博主相关信息来展示，那么我们会封装成一个博主对象，比如： public class Blogger &#123; private Long id; private String name; private String pass; // 省去set和get&#125; 然后在controller层中初始化一下： @GetMapping(\"/getBlogger\")public String getBlogger(Model model) &#123; Blogger blogger = new Blogger(1L, \"倪升武\", \"123456\"); model.addAttribute(\"blogger\", blogger); return \"blogger\";&#125; 我们先初始化一个 Blogger 对象，然后将该对象放到 Model 中，然后返回到 blogger.html 页面去渲染。接下来我们再写一个 blogger.html 来渲染 blogger 信息： &lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;博主信息&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action=\"\" th:object=\"$&#123;blogger&#125;\" &gt; 用户编号：&lt;input name=\"id\" th:value=\"$&#123;blogger.id&#125;\"/&gt;&lt;br&gt; 用户姓名：&lt;input type=\"text\" name=\"username\" th:value=\"$&#123;blogger.getName()&#125;\" /&gt;&lt;br&gt; 登陆密码：&lt;input type=\"text\" name=\"password\" th:value=\"*&#123;pass&#125;\" /&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 可以看出，在 thymeleaf 模板中，使用 th:object=&quot;${}&quot; 来获取对象信息，然后在表单里面可以有三种方式来获取对象属性。如下： 使用 th:value=&quot;*{属性名}&quot;使用 th:value=&quot;${对象.属性名}&quot;，对象指的是上面使用 th:object 获取的对象使用 th:value=&quot;${对象.get方法}&quot;，对象指的是上面使用 th:object 获取的对象 可以看出，在 Thymeleaf 中可以像写 java 一样写代码，很方便。我们在浏览器中输入 localhost:8080/thymeleaf/getBlogger 来测试一下数据： 4.3 Thymeleaf 中处理 List处理 List 的话，和处理上面介绍的对象差不多，但是需要在 thymeleaf 中进行遍历。我们先在 Controller 中模拟一个 List。 @GetMapping(\"/getList\")public String getList(Model model) &#123; Blogger blogger1 = new Blogger(1L, \"倪升武\", \"123456\"); Blogger blogger2 = new Blogger(2L, \"达人课\", \"123456\"); List&lt;Blogger&gt; list = new ArrayList&lt;&gt;(); list.add(blogger1); list.add(blogger2); model.addAttribute(\"list\", list); return \"list\";&#125; 接下来我们写一个 list.html 来获取该 list 信息，然后在 list.html 中遍历这个list。如下： &lt;!DOCTYPE html&gt;&lt;html xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;博主信息&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form action=\"\" th:each=\"blogger : $&#123;list&#125;\" &gt; 用户编号：&lt;input name=\"id\" th:value=\"$&#123;blogger.id&#125;\"/&gt;&lt;br&gt; 用户姓名：&lt;input type=\"text\" name=\"password\" th:value=\"$&#123;blogger.name&#125;\"/&gt;&lt;br&gt; 登录密码：&lt;input type=\"text\" name=\"username\" th:value=\"$&#123;blogger.getPass()&#125;\"/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 可以看出，其实和处理单个对象信息差不多，Thymeleaf 使用 th:each 进行遍历，${} 取 model 中传过来的参数，然后自定义 list 中取出来的每个对象，这里定义为 blogger。表单里面可以直接使用 ${对象.属性名} 来获取 list 中对象的属性值，也可以使用 ${对象.get方法} 来获取，这点和上面处理对象信息是一样的，但是不能使用 *{属性名} 来获取对象中的属性，thymeleaf 模板获取不到。 4.4 其他常用 thymeleaf 操作我们来总结一下 thymeleaf 中的一些常用的标签操作，如下： 标签 功能 例子 th:value 给属性赋值 &lt;input th:value=&quot;${blog.name}&quot; /&gt; th:style 设置样式 th:style=&quot;&#39;display:&#39;+@{(${sitrue}?&#39;none&#39;:&#39;inline-block&#39;)} + &#39;&#39;&quot; th:onclick 点击事件 th:onclick=&quot;&#39;getInfo()&#39;&quot; th:if 条件判断 &lt;a th:if=&quot;${userId == collect.userId}&quot; &gt; th:href 超链接 &lt;a th:href=&quot;@{/blogger/login}&quot;&gt;Login&lt;/a&gt; /&gt; th:unless 条件判断和th:if相反 &lt;a th:href=&quot;@{/blogger/login}&quot; th:unless=${session.user != null}&gt;Login&lt;/a&gt; th:switch 配合th:case &lt;div th:switch=&quot;${user.role}&quot;&gt; th:case 配合th:switch &lt;p th:case=&quot;&#39;admin&#39;&quot;&gt;administator&lt;/p&gt; th:src 地址引入 &lt;img alt=&quot;csdn logo&quot; th:src=&quot;@{/img/logo.png}&quot; /&gt; th:action 表单提交的地址 &lt;form th:action=&quot;@{/blogger/update}&quot;&gt; Thymeleaf 还有很多其他用法，这里就不总结了，具体的可以参考Thymeleaf的官方文档（v3.0）。主要要学会如何在 Spring Boot 中去使用 thymeleaf，遇到对应的标签或者方法，查阅官方文档即可。 5. 总结Thymeleaf 在 Spring Boot 中使用非常广泛，本节课主要分析了 thymeleaf 的优点，以及如何在 Spring Boot 中集成并使用 thymeleaf 模板，包括依赖、配置，相关数据的获取、以及一些注意事项等等。最后列举了一些 thymeleaf 中常用的标签，在实际项目中多使用，多查阅就能熟练掌握，thymeleaf 中的一些标签或者方法不用死记硬背，用到什么去查阅什么，关键是要会在 Spring Boot 中集成，用的多了就熟能生巧。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第08课：Spring Boot中的全局异常处理在项目开发过程中，不管是对底层数据库的操作过程，还是业务层的处理过程，还是控制层的处理过程，都不可避免会遇到各种可预知的、不可预知的异常需要处理。如果对每个过程都单独作异常处理，那系统的代码耦合度会变得很高，此外，开发工作量也会加大而且不好统一，这也增加了代码的维护成本。针对这种实际情况，我们需要将所有类型的异常处理从各处理过程解耦出来，这样既保证了相关处理过程的功能单一，也实现了异常信息的统一处理和维护。同时，我们也不希望直接把异常抛给用户，应该对异常进行处理，对错误信息进行封装，然后返回一个友好的信息给用户。这节主要总结一下项目中如何使用 Spring Boot 如何拦截并处理全局的异常。 1. 定义返回的统一 json 结构前端或者其他服务请求本服务的接口时，该接口需要返回对应的 json 数据，一般该服务只需要返回请求着需要的参数即可，但是在实际项目中，我们需要封装更多的信息，比如状态码 code、相关信息 msg 等等，这一方面是在项目中可以有个统一的返回结构，整个项目组都适用，另一方面是方便结合全局异常处理信息，因为异常处理信息中一般我们需要把状态码和异常内容反馈给调用方。这个统一的 json 结构这可以参考第02课：Spring Boot 返回 JSON 数据及数据封装中封装的统一 json 结构，本节内容我们简化一下，只保留状态码 code 和异常信息 msg即可。如下： public class JsonResult &#123; /** * 异常码 */ protected String code; /** * 异常信息 */ protected String msg; public JsonResult() &#123; this.code = \"200\"; this.msg = \"操作成功\"; &#125; public JsonResult(String code, String msg) &#123; this.code = code; this.msg = msg; &#125; // get set&#125; 2. 处理系统异常新建一个 GlobalExceptionHandler 全局异常处理类，然后加上 @ControllerAdvice 注解即可拦截项目中抛出的异常，如下： @ControllerAdvice@ResponseBodypublic class GlobalExceptionHandler &#123; // 打印log private static final Logger logger = LoggerFactory.getLogger(GlobalExceptionHandler.class); // ……&#125; 我们点开 @ControllerAdvice 注解可以看到，@ControllerAdvice 注解包含了 @Component 注解，说明在 Spring Boot 启动时，也会把该类作为组件交给 Spring 来管理。除此之外，该注解还有个 basePackages 属性，该属性是用来拦截哪个包中的异常信息，一般我们不指定这个属性，我们拦截项目工程中的所有异常。@ResponseBody 注解是为了异常处理完之后给调用方输出一个 json 格式的封装数据。在项目中如何使用呢？Spring Boot 中很简单，在方法上通过 @ExceptionHandler 注解来指定具体的异常，然后在方法中处理该异常信息，最后将结果通过统一的 json 结构体返回给调用者。下面我们举几个例子来说明如何来使用。 2.1 处理参数缺失异常在前后端分离的架构中，前端请求后台的接口都是通过 rest 风格来调用，有时候，比如 POST 请求 需要携带一些参数，但是往往有时候参数会漏掉。另外，在微服务架构中，涉及到多个微服务之间的接口调用时，也可能出现这种情况，此时我们需要定义一个处理参数缺失异常的方法，来给前端或者调用方提示一个友好信息。 参数缺失的时候，会抛出 HttpMessageNotReadableException，我们可以拦截该异常，做一个友好处理，如下： /*** 缺少请求参数异常* @param ex HttpMessageNotReadableException* @return*/@ExceptionHandler(MissingServletRequestParameterException.class)@ResponseStatus(value = HttpStatus.BAD_REQUEST)public JsonResult handleHttpMessageNotReadableException( MissingServletRequestParameterException ex) &#123; logger.error(\"缺少请求参数，&#123;&#125;\", ex.getMessage()); return new JsonResult(\"400\", \"缺少必要的请求参数\");&#125; 我们来写个简单的 Controller 测试一下该异常，通过 POST 请求方式接收两个参数：姓名和密码。 @RestController@RequestMapping(\"/exception\")public class ExceptionController &#123; private static final Logger logger = LoggerFactory.getLogger(ExceptionController.class); @PostMapping(\"/test\") public JsonResult test(@RequestParam(\"name\") String name, @RequestParam(\"pass\") String pass) &#123; logger.info(\"name：&#123;&#125;\", name); logger.info(\"pass：&#123;&#125;\", pass); return new JsonResult(); &#125;&#125; 然后使用 Postman 来调用一下该接口，调用的时候，只传姓名，不传密码，就会抛缺少参数异常，该异常被捕获之后，就会进入我们写好的逻辑，给调用方返回一个友好信息，如下： 2.2 处理空指针异常空指针异常是开发中司空见惯的东西了，一般发生的地方有哪些呢？先来聊一聊一些注意的地方，比如在微服务中，经常会调用其他服务获取数据，这个数据主要是 json 格式的，但是在解析 json 的过程中，可能会有空出现，所以我们在获取某个 jsonObject 时，再通过该 jsonObject 去获取相关信息时，应该要先做非空判断。还有一个很常见的地方就是从数据库中查询的数据，不管是查询一条记录封装在某个对象中，还是查询多条记录封装在一个 List 中，我们接下来都要去处理数据，那么就有可能出现空指针异常，因为谁也不能保证从数据库中查出来的东西就一定不为空，所以在使用数据时一定要先做非空判断。对空指针异常的处理很简单，和上面的逻辑一样，将异常信息换掉即可。如下： @ControllerAdvice@ResponseBodypublic class GlobalExceptionHandler &#123; private static final Logger logger = LoggerFactory.getLogger(GlobalExceptionHandler.class); /** * 空指针异常 * @param ex NullPointerException * @return */ @ExceptionHandler(NullPointerException.class) @ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR) public JsonResult handleTypeMismatchException(NullPointerException ex) &#123; logger.error(\"空指针异常，&#123;&#125;\", ex.getMessage()); return new JsonResult(\"500\", \"空指针异常了\"); &#125;&#125; 这个我就不测试了，代码中 ExceptionController 有个 testNullPointException 方法，模拟了一个空指针异常，我们在浏览器中请求一下对应的 url 即可看到返回的信息： &#123;\"code\":\"500\",\"msg\":\"空指针异常了\"&#125; 2.3 一劳永逸？当然了，异常很多，比如还有 RuntimeException，数据库还有一些查询或者操作异常等等。由于 Exception 异常是父类，所有异常都会继承该异常，所以我们可以直接拦截 Exception 异常，一劳永逸： @ControllerAdvice@ResponseBodypublic class GlobalExceptionHandler &#123; private static final Logger logger = LoggerFactory.getLogger(GlobalExceptionHandler.class); /** * 系统异常 预期以外异常 * @param ex * @return */ @ExceptionHandler(Exception.class) @ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR) public JsonResult handleUnexpectedServer(Exception ex) &#123; logger.error(\"系统异常：\", ex); return new JsonResult(\"500\", \"系统发生异常，请联系管理员\"); &#125;&#125; 但是项目中，我们一般都会比较详细的去拦截一些常见异常，拦截 Exception 虽然可以一劳永逸，但是不利于我们去排查或者定位问题。实际项目中，可以把拦截 Exception 异常写在 GlobalExceptionHandler 最下面，如果都没有找到，最后再拦截一下 Exception 异常，保证输出信息友好。 3. 拦截自定义异常在实际项目中，除了拦截一些系统异常外，在某些业务上，我们需要自定义一些业务异常，比如在微服务中，服务之间的相互调用很平凡，很常见。要处理一个服务的调用时，那么可能会调用失败或者调用超时等等，此时我们需要自定义一个异常，当调用失败时抛出该异常，给 GlobalExceptionHandler 去捕获。 3.1 定义异常信息由于在业务中，有很多异常，针对不同的业务，可能给出的提示信息不同，所以为了方便项目异常信息管理，我们一般会定义一个异常信息枚举类。比如： /** * 业务异常提示信息枚举类 * @author shengwu ni */public enum BusinessMsgEnum &#123; /** 参数异常 */ PARMETER_EXCEPTION(\"102\", \"参数异常!\"), /** 等待超时 */ SERVICE_TIME_OUT(\"103\", \"服务调用超时！\"), /** 参数过大 */ PARMETER_BIG_EXCEPTION(\"102\", \"输入的图片数量不能超过50张!\"), /** 500 : 一劳永逸的提示也可以在这定义 */ UNEXPECTED_EXCEPTION(\"500\", \"系统发生异常，请联系管理员！\"); // 还可以定义更多的业务异常 /** * 消息码 */ private String code; /** * 消息内容 */ private String msg; private BusinessMsgEnum(String code, String msg) &#123; this.code = code; this.msg = msg; &#125; // set get方法&#125; 3.2 拦截自定义异常然后我们可以定义一个业务异常，当出现业务异常时，我们就抛这个自定义的业务异常即可。比如我们定义一个 BusinessErrorException 异常，如下： /** * 自定义业务异常 * @author shengwu ni */public class BusinessErrorException extends RuntimeException &#123; private static final long serialVersionUID = -7480022450501760611L; /** * 异常码 */ private String code; /** * 异常提示信息 */ private String message; public BusinessErrorException(BusinessMsgEnum businessMsgEnum) &#123; this.code = businessMsgEnum.code(); this.message = businessMsgEnum.msg(); &#125; // get set方法&#125; 在构造方法中，传入我们上面自定义的异常枚举类，所以在项目中，如果有新的异常信息需要添加，我们直接在枚举类中添加即可，很方便，做到统一维护，然后再拦截该异常时获取即可。 @ControllerAdvice@ResponseBodypublic class GlobalExceptionHandler &#123; private static final Logger logger = LoggerFactory.getLogger(GlobalExceptionHandler.class); /** * 拦截业务异常，返回业务异常信息 * @param ex * @return */ @ExceptionHandler(BusinessErrorException.class) @ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR) public JsonResult handleBusinessError(BusinessErrorException ex) &#123; String code = ex.getCode(); String message = ex.getMessage(); return new JsonResult(code, message); &#125;&#125; 在业务代码中，我们可以直接模拟一下抛出业务异常，测试一下： @RestController@RequestMapping(\"/exception\")public class ExceptionController &#123; private static final Logger logger = LoggerFactory.getLogger(ExceptionController.class); @GetMapping(\"/business\") public JsonResult testException() &#123; try &#123; int i = 1 / 0; &#125; catch (Exception e) &#123; throw new BusinessErrorException(BusinessMsgEnum.UNEXPECTED_EXCEPTION); &#125; return new JsonResult(); &#125;&#125; 运行一下项目，测试一下，返回 json 如下，说明我们自定义的业务异常捕获成功： &#123;\"code\":\"500\",\"msg\":\"系统发生异常，请联系管理员！\"&#125; 4. 总结本节课程主要讲解了Spring Boot 的全局异常处理，包括异常信息的封装、异常信息的捕获和处理，以及在实际项目中，我们用到的自定义异常枚举类和业务异常的捕获与处理，在项目中运用的非常广泛，基本上每个项目中都需要做全局异常处理。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第09课：Spring Boot中的切面AOP处理1. 什么是AOPAOP：Aspect Oriented Programming 的缩写，意为：面向切面编程。面向切面编程的目标就是分离关注点。什么是关注点呢？就是关注点，就是你要做的事情。假如你是一位公子哥，没啥人生目标，每天衣来伸手，饭来张口，整天只知道一件事：玩（这就是你的关注点，你只要做这一件事）！但是有个问题，你在玩之前，你还需要起床、穿衣服、穿鞋子、叠被子、做早饭等等等等，但是这些事情你不想关注，也不用关注，你只想想玩，那么怎么办呢？ 对！这些事情通通交给下人去干。你有一个专门的仆人 A 帮你穿衣服，仆人 B 帮你穿鞋子，仆人 C 帮你叠好被子，仆人 D 帮你做饭，然后你就开始吃饭、去玩（这就是你一天的正事），你干完你的正事之后，回来，然后一系列仆人又开始帮你干这个干那个，然后一天就结束了！ 这就是 AOP。AOP 的好处就是你只需要干你的正事，其它事情别人帮你干。也许有一天，你想裸奔，不想穿衣服，那么你把仆人 A 解雇就是了！也许有一天，出门之前你还想带点钱，那么你再雇一个仆人 E 专门帮你干取钱的活！这就是AOP。每个人各司其职，灵活组合，达到一种可配置的、可插拔的程序结构。 2. Spring Boot 中的 AOP 处理2.1 AOP 依赖使用AOP，首先需要引入AOP的依赖。 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;&lt;/dependency&gt; 2.2 实现 AOP 切面Spring Boot 中使用 AOP 非常简单，假如我们要在项目中打印一些 log，在引入了上面的依赖之后，我们新建一个类 LogAspectHandler，用来定义切面和处理方法。只要在类上加个@Aspect注解即可。@Aspect 注解用来描述一个切面类，定义切面类的时候需要打上这个注解。@Component 注解让该类交给 Spring 来管理。 @Aspect@Componentpublic class LogAspectHandler &#123;&#125; 这里主要介绍几个常用的注解及使用： 1.@Pointcut：定义一个切面，即上面所描述的关注的某件事入口。2.@Before：在做某件事之前做的事。3.@After：在做某件事之后做的事。4.@AfterReturning：在做某件事之后，对其返回值做增强处理。5.@AfterThrowing：在做某件事抛出异常时，处理。 2.2.1 @Pointcut 注解@Pointcut 注解：用来定义一个切面（切入点），即上文中所关注的某件事情的入口。切入点决定了连接点关注的内容，使得我们可以控制通知什么时候执行。 @Aspect@Componentpublic class LogAspectHandler &#123; /** * 定义一个切面，拦截com.itcodai.course09.controller包和子包下的所有方法 */ @Pointcut(\"execution(* com.itcodai.course09.controller..*.*(..))\") public void pointCut() &#123;&#125;&#125; @Pointcut 注解指定一个切面，定义需要拦截的东西，这里介绍两个常用的表达式：一个是使用 execution()，另一个是使用 annotation()。以 execution(* com.itcodai.course09.controller..*.*(..))) 表达式为例，语法如下： execution() 为表达式主体第一个 * 号的位置：表示返回值类型，* 表示所有类型包名：表示需要拦截的包名，后面的两个句点表示当前包和当前包的所有子包，com.itcodai.course09.controller 包、子包下所有类的方法第二个 * 号的位置：表示类名，* 表示所有类*(..) ：这个星号表示方法名，* 表示所有的方法，后面括弧里面表示方法的参数，两个句点表示任何参数 annotation() 方式是针对某个注解来定义切面，比如我们对具有@GetMapping注解的方法做切面，可以如下定义切面： @Pointcut(\"@annotation(org.springframework.web.bind.annotation.GetMapping)\")public void annotationCut() &#123;&#125; 然后使用该切面的话，就会切入注解是 @GetMapping 的方法。因为在实际项目中，可能对于不同的注解有不同的逻辑处理，比如 @GetMapping、@PostMapping、@DeleteMapping 等。所以这种按照注解的切入方式在实际项目中也很常用。 2.2.2 @Before 注解@Before 注解指定的方法在切面切入目标方法之前执行，可以做一些 log 处理，也可以做一些信息的统计，比如获取用户的请求 url 以及用户的 ip 地址等等，这个在做个人站点的时候都能用得到，都是常用的方法。例如： @Aspect@Componentpublic class LogAspectHandler &#123; private final Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 在上面定义的切面方法之前执行该方法 * @param joinPoint jointPoint */ @Before(\"pointCut()\") public void doBefore(JoinPoint joinPoint) &#123; logger.info(\"====doBefore方法进入了====\"); // 获取签名 Signature signature = joinPoint.getSignature(); // 获取切入的包名 String declaringTypeName = signature.getDeclaringTypeName(); // 获取即将执行的方法名 String funcName = signature.getName(); logger.info(\"即将执行方法为: &#123;&#125;，属于&#123;&#125;包\", funcName, declaringTypeName); // 也可以用来记录一些信息，比如获取请求的url和ip ServletRequestAttributes attributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); HttpServletRequest request = attributes.getRequest(); // 获取请求url String url = request.getRequestURL().toString(); // 获取请求ip String ip = request.getRemoteAddr(); logger.info(\"用户请求的url为：&#123;&#125;，ip地址为：&#123;&#125;\", url, ip); &#125;&#125; JointPoint 对象很有用，可以用它来获取一个签名，然后利用签名可以获取请求的包名、方法名，包括参数（通过 joinPoint.getArgs() 获取）等等。 2.2.3 @After 注解@After 注解和 @Before 注解相对应，指定的方法在切面切入目标方法之后执行，也可以做一些完成某方法之后的 log 处理。 @Aspect@Componentpublic class LogAspectHandler &#123; private final Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 定义一个切面，拦截com.itcodai.course09.controller包下的所有方法 */ @Pointcut(\"execution(* com.itcodai.course09.controller..*.*(..))\") public void pointCut() &#123;&#125; /** * 在上面定义的切面方法之后执行该方法 * @param joinPoint jointPoint */ @After(\"pointCut()\") public void doAfter(JoinPoint joinPoint) &#123; logger.info(\"====doAfter方法进入了====\"); Signature signature = joinPoint.getSignature(); String method = signature.getName(); logger.info(\"方法&#123;&#125;已经执行完\", method); &#125;&#125; 到这里，我们来写一个 Controller 来测试一下执行结果，新建一个 AopController 如下： @RestController@RequestMapping(\"/aop\")public class AopController &#123; @GetMapping(\"/&#123;name&#125;\") public String testAop(@PathVariable String name) &#123; return \"Hello \" + name; &#125;&#125; 启动项目，在浏览器中输入 localhost:8080/aop/CSDN，观察一下控制台的输出信息： &#x3D;&#x3D;&#x3D;&#x3D;doBefore方法进入了&#x3D;&#x3D;&#x3D;&#x3D; 即将执行方法为: testAop，属于com.itcodai.course09.controller.AopController包 用户请求的url为：http:&#x2F;&#x2F;localhost:8080&#x2F;aop&#x2F;name，ip地址为：0:0:0:0:0:0:0:1 &#x3D;&#x3D;&#x3D;&#x3D;doAfter方法进入了&#x3D;&#x3D;&#x3D;&#x3D; 方法testAop已经执行完 从打印出来的 log 中可以看出程序执行的逻辑与顺序，可以很直观的掌握 @Before 和 @After 两个注解的实际作用。 2.2.4 @AfterReturning 注解@AfterReturning 注解和 @After 有些类似，区别在于 @AfterReturning 注解可以用来捕获切入方法执行完之后的返回值，对返回值进行业务逻辑上的增强处理，例如： @Aspect@Componentpublic class LogAspectHandler &#123; private final Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 在上面定义的切面方法返回后执行该方法，可以捕获返回对象或者对返回对象进行增强 * @param joinPoint joinPoint * @param result result */ @AfterReturning(pointcut = \"pointCut()\", returning = \"result\") public void doAfterReturning(JoinPoint joinPoint, Object result) &#123; Signature signature = joinPoint.getSignature(); String classMethod = signature.getName(); logger.info(\"方法&#123;&#125;执行完毕，返回参数为：&#123;&#125;\", classMethod, result); // 实际项目中可以根据业务做具体的返回值增强 logger.info(\"对返回参数进行业务上的增强：&#123;&#125;\", result + \"增强版\"); &#125;&#125; 需要注意的是：在 @AfterReturning注解 中，属性 returning 的值必须要和参数保持一致，否则会检测不到。该方法中的第二个入参就是被切方法的返回值，在 doAfterReturning 方法中可以对返回值进行增强，可以根据业务需要做相应的封装。我们重启一下服务，再测试一下（多余的 log 我不贴出来了）： 方法testAop执行完毕，返回参数为：Hello CSDN 对返回参数进行业务上的增强：Hello CSDN增强版 2.2.5 @AfterThrowing 注解顾名思义，@AfterThrowing 注解是当被切方法执行时抛出异常时，会进入 @AfterThrowing 注解的方法中执行，在该方法中可以做一些异常的处理逻辑。要注意的是 throwing 属性的值必须要和参数一致，否则会报错。该方法中的第二个入参即为抛出的异常。 /** * 使用AOP处理log * @author shengwu ni * @date 2018/05/04 20:24 */@Aspect@Componentpublic class LogAspectHandler &#123; private final Logger logger = LoggerFactory.getLogger(this.getClass()); /** * 在上面定义的切面方法执行抛异常时，执行该方法 * @param joinPoint jointPoint * @param ex ex */ @AfterThrowing(pointcut = \"pointCut()\", throwing = \"ex\") public void afterThrowing(JoinPoint joinPoint, Throwable ex) &#123; Signature signature = joinPoint.getSignature(); String method = signature.getName(); // 处理异常的逻辑 logger.info(\"执行方法&#123;&#125;出错，异常为：&#123;&#125;\", method, ex); &#125;&#125; 该方法我就不测试了，大家可以自行测试一下。 3. 总结本节课针对 Spring Boot 中的切面 AOP 做了详细的讲解，主要介绍了 Spring Boot 中 AOP 的引入，常用注解的使用，参数的使用，以及常用 api 的介绍。AOP 在实际项目中很有用，对切面方法执行前后都可以根据具体的业务，做相应的预处理或者增强处理，同时也可以用作异常捕获处理，可以根据具体业务场景，合理去使用 AOP。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第10课：Spring Boot集成MyBatis1. MyBatis 介绍大家都知道，MyBatis 框架是一个持久层框架，是 Apache 下的顶级项目。Mybatis 可以让开发者的主要精力放在 sql 上，通过 Mybatis 提供的映射方式，自由灵活的生成满足需要的 sql 语句。使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs 映射成数据库中的记录，在国内可谓是占据了半壁江山。本节课程主要通过两种方式来对 Spring Boot 集成 MyBatis 做一讲解。重点讲解一下基于注解的方式。因为实际项目中使用注解的方式更多一点，更简洁一点，省去了很多 xml 配置（这不是绝对的，有些项目组中可能也在使用 xml 的方式）。 2. MyBatis 的配置2.1 依赖导入Spring Boot 集成 MyBatis，需要导入 mybatis-spring-boot-starter 和 mysql 的依赖，这里我们使用的版本时 1.3.2，如下： &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt; 我们点开 mybatis-spring-boot-starter 依赖，可以看到我们之前使用 Spring 时候熟悉的依赖，就像我在课程的一开始介绍的那样，Spring Boot 致力于简化编码，使用 starter 系列将相关依赖集成在一起，开发者不需要关注繁琐的配置，非常方便。 &lt;!-- 省去其他 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt;&lt;/dependency&gt; 2.2 properties.yml配置我们再来看一下，集成 MyBatis 时需要在 properties.yml 配置文件中做哪些基本配置呢？ # 服务端口号server: port: 8080# 数据库地址datasource: url: localhost:3306/blog_testspring: datasource: # 数据库配置 driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://$&#123;datasource.url&#125;?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10 username: root password: 123456 hikari: maximum-pool-size: 10 # 最大连接池数 max-lifetime: 1770000mybatis: # 指定别名设置的包为所有entity type-aliases-package: com.itcodai.course10.entity configuration: map-underscore-to-camel-case: true # 驼峰命名规范 mapper-locations: # mapper映射文件位置 - classpath:mapper/*.xml 我们来简单介绍一下上面的这些配置：关于数据库的相关配置，我就不详细的解说了，这点相信大家已经非常熟练了，配置一下用户名、密码、数据库连接等等，这里使用的连接池是 Spring Boot 自带的 hikari，感兴趣的朋友可以去百度或者谷歌搜一搜，了解一下。 这里说明一下 map-underscore-to-camel-case: true， 用来开启驼峰命名规范，这个比较好用，比如数据库中字段名为：user_name， 那么在实体类中可以定义属性为 userName （甚至可以写成 username，也能映射上），会自动匹配到驼峰属性，如果不这样配置的话，针对字段名和属性名不同的情况，会映射不到。 3. 基于 xml 的整合使用原始的 xml 方式，需要新建 UserMapper.xml 文件，在上面的 application.yml 配置文件中，我们已经定义了 xml 文件的路径：classpath:mapper/*.xml，所以我们在 resources 目录下新建一个 mapper 文件夹，然后创建一个 UserMapper.xml 文件。 &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.itcodai.course10.dao.UserMapper\"&gt; &lt;resultMap id=\"BaseResultMap\" type=\"com.itcodai.course10.entity.User\"&gt; &lt;id column=\"id\" jdbcType=\"BIGINT\" property=\"id\" /&gt; &lt;result column=\"user_name\" jdbcType=\"VARCHAR\" property=\"username\" /&gt; &lt;result column=\"password\" jdbcType=\"VARCHAR\" property=\"password\" /&gt; &lt;/resultMap&gt; &lt;select id=\"getUserByName\" resultType=\"User\" parameterType=\"String\"&gt; select * from user where user_name = #&#123;username&#125; &lt;/select&gt;&lt;/mapper&gt; 这和整合 Spring 一样的，namespace 中指定的是对应的 Mapper， &lt;resultMap&gt; 中指定对应的实体类，即 User。然后在内部指定表的字段和实体的属性相对应即可。这里我们写一个根据用户名查询用户的 sql。 实体类中有 id，username 和 password，我不在这贴代码，大家可以下载源码查看。UserMapper.java 文件中写一个接口即可： User getUserByName(String username); 中间省略 service 的代码，我们写一个 Controller 来测试一下： @RestControllerpublic class TestController &#123; @Resource private UserService userService; @RequestMapping(\"/getUserByName/&#123;name&#125;\") public User getUserByName(@PathVariable String name) &#123; return userService.getUserByName(name); &#125;&#125; 启动项目，在浏览器中输入：http://localhost:8080/getUserByName/CSDN 即可查询到数据库表中用户名为 CSDN 的用户信息（事先搞两个数据进去即可）： &#123;\"id\":2,\"username\":\"CSDN\",\"password\":\"123456\"&#125; 这里需要注意一下：Spring Boot 如何知道这个 Mapper 呢？一种方法是在上面的 mapper 层对应的类上面添加 @Mapper 注解即可，但是这种方法有个弊端，当我们有很多个 mapper 时，那么每一个类上面都得添加 @Mapper 注解。另一种比较简便的方法是在 Spring Boot 启动类上添加@MaperScan 注解，来扫描一个包下的所有 mapper。如下： @SpringBootApplication@MapperScan(\"com.itcodai.course10.dao\")public class Course10Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Course10Application.class, args); &#125;&#125; 这样的话，com.itcodai.course10.dao 包下的所有 mapper 都会被扫描到了。 4. 基于注解的整合基于注解的整合就不需要 xml 配置文件了，MyBatis 主要提供了 @Select， @Insert， @Update， Delete 四个注解。这四个注解是用的非常多的，也很简单，注解后面跟上对应的 sql 语句即可，我们举个例子： @Select(\"select * from user where id = #&#123;id&#125;\")User getUser(Long id); 这跟 xml 文件中写 sql 语句是一样的，这样就不需要 xml 文件了，但是有个问题，有人可能会问，如果是两个参数呢？如果是两个参数，我们需要使用 @Param 注解来指定每一个参数的对应关系，如下： @Select(\"select * from user where id = #&#123;id&#125; and user_name=#&#123;name&#125;\")User getUserByIdAndName(@Param(\"id\") Long id, @Param(\"name\") String username); 可以看出，@Param 指定的参数应该要和 sql 中 #{} 取的参数名相同，不同则取不到。可以在 controller 中自行测试一下，接口都在源码中，文章中我就不贴测试代码和结果了。 有个问题需要注意一下，一般我们在设计表字段后，都会根据自动生成工具生成实体类，这样的话，基本上实体类是能和表字段对应上的，最起码也是驼峰对应的，由于在上面配置文件中开启了驼峰的配置，所以字段都是能对的上的。但是，万一有对不上的呢？我们也有解决办法，使用 @Results 注解来解决。 @Select(\"select * from user where id = #&#123;id&#125;\")@Results(&#123; @Result(property = \"username\", column = \"user_name\"), @Result(property = \"password\", column = \"password\")&#125;)User getUser(Long id); @Results 中的 @Result 注解是用来指定每一个属性和字段的对应关系，这样的话就可以解决上面说的这个问题了。 当然了，我们也可以 xml 和注解相结合使用，目前我们实际的项目中也是采用混用的方式，因为有时候 xml 方便，有时候注解方便，比如就上面这个问题来说，如果我们定义了上面的这个 UserMapper.xml，那么我们完全可以使用 @ResultMap 注解来替代 @Results 注解，如下： @Select(\"select * from user where id = #&#123;id&#125;\")@ResultMap(\"BaseResultMap\")User getUser(Long id); @ResultMap 注解中的值从哪来呢？对应的是 UserMapper.xml 文件中定义的 &lt;resultMap&gt; 时对应的 id 值： &lt;resultMap id=\"BaseResultMap\" type=\"com.itcodai.course10.entity.User\"&gt; 这种 xml 和注解结合着使用的情况也很常见，而且也减少了大量的代码，因为 xml 文件可以使用自动生成工具去生成，也不需要人为手动敲，所以这种使用方式也很常见。 5. 总结本节课主要系统的讲解了 Spring Boot 集成 MyBatis 的过程，分为基于 xml 形式和基于注解的形式来讲解，通过实际配置手把手讲解了 Spring Boot 中 MyBatis 的使用方式，并针对注解方式，讲解了常见的问题已经解决方式，有很强的实战意义。在实际项目中，建议根据实际情况来确定使用哪种方式，一般 xml 和注解都在用。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第11课：Spring Boot事务配置管理1. 事务相关场景：我们在开发企业应用时，由于数据操作在顺序执行的过程中，线上可能有各种无法预知的问题，任何一步操作都有可能发生异常，异常则会导致后续的操作无法完成。此时由于业务逻辑并未正确的完成，所以在之前操作过数据库的动作并不可靠，需要在这种情况下进行数据的回滚。 事务的作用就是为了保证用户的每一个操作都是可靠的，事务中的每一步操作都必须成功执行，只要有发生异常就回退到事务开始未进行操作的状态。这很好理解，转账、购票等等，必须整个事件流程全部执行完才能人为该事件执行成功，不能转钱转到一半，系统死了，转账人钱没了，收款人钱还没到。 事务管理是 Spring Boot 框架中最为常用的功能之一，我们在实际应用开发时，基本上在 service 层处理业务逻辑的时候都要加上事务，当然了，有时候可能由于场景需要，也不用加事务（比如我们就要往一个表里插数据，相互没有影响，插多少是多少，不能因为某个数据挂了，把之前插的全部回滚）。 2. Spring Boot 事务配置2.1 依赖导入在 Spring Boot 中使用事务，需要导入 mysql 依赖： &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt; 导入了 mysql 依赖后，Spring Boot 会自动注入 DataSourceTransactionManager，我们不需要任何其他的配置就可以用 @Transactional 注解进行事务的使用。关于 mybatis 的配置，在上一节课中已经说明了，这里还是使用上一节课中的 mybatis 配置即可。 2.2 事务的测试我们首先在数据库表中插入一条数据：|id|user_name|password||:–:|:–:|:–:||1|倪升武|123456| 然后我们写一个插入的 mapper： public interface UserMapper &#123; @Insert(\"insert into user (user_name, password) values (#&#123;username&#125;, #&#123;password&#125;)\") Integer insertUser(User user);&#125; OK，接下来我们来测试一下 Spring Boot 中的事务处理，在 service 层，我们手动抛出个异常来模拟实际中出现的异常，然后观察一下事务有没有回滚，如果数据库中没有新的记录，则说明事务回滚成功。 @Servicepublic class UserServiceImpl implements UserService &#123; @Resource private UserMapper userMapper; @Override @Transactional public void isertUser(User user) &#123; // 插入用户信息 userMapper.insertUser(user); // 手动抛出异常 throw new RuntimeException(); &#125;&#125; 我们来测试一下： @RestControllerpublic class TestController &#123; @Resource private UserService userService; @PostMapping(\"/adduser\") public String addUser(@RequestBody User user) throws Exception &#123; if (null != user) &#123; userService.isertUser(user); return \"success\"; &#125; else &#123; return \"false\"; &#125; &#125;&#125; 我们使用 postman 调用一下该接口，因为在程序中抛出了个异常，会造成事务回滚，我们刷新一下数据库，并没有增加一条记录，说明事务生效了。事务很简单，我们平时在使用的时候，一般不会有多少问题，但是并不仅仅如此…… 3. 常见问题总结从上面的内容中可以看出，Spring Boot 中使用事务非常简单，@Transactional 注解即可解决问题，说是这么说，但是在实际项目中，是有很多小坑在等着我们，这些小坑是我们在写代码的时候没有注意到，而且正常情况下不容易发现这些小坑，等项目写大了，某一天突然出问题了，排查问题非常困难，到时候肯定是抓瞎，需要费很大的精力去排查问题。 这一小节，我专门针对实际项目中经常出现的，和事务相关的细节做一下总结，希望读者在读完之后，能够落实到自己的项目中，能有所受益。 3.1 异常并没有被 ”捕获“ 到首先要说的，就是异常并没有被 ”捕获“ 到，导致事务并没有回滚。我们在业务层代码中，也许已经考虑到了异常的存在，或者编辑器已经提示我们需要抛出异常，但是这里面有个需要注意的地方：并不是说我们把异常抛出来了，有异常了事务就会回滚，我们来看一个例子： @Servicepublic class UserServiceImpl implements UserService &#123; @Resource private UserMapper userMapper; @Override @Transactional public void isertUser2(User user) throws Exception &#123; // 插入用户信息 userMapper.insertUser(user); // 手动抛出异常 throw new SQLException(\"数据库异常\"); &#125;&#125; 我们看上面这个代码，其实并没有什么问题，手动抛出一个 SQLException 来模拟实际中操作数据库发生的异常，在这个方法中，既然抛出了异常，那么事务应该回滚，实际却不如此，读者可以使用我源码中 controller 的接口，通过 postman 测试一下，就会发现，仍然是可以插入一条用户数据的。 那么问题出在哪呢？因为 Spring Boot 默认的事务规则是遇到运行异常（RuntimeException）和程序错误（Error）才会回滚。比如上面我们的例子中抛出的 RuntimeException 就没有问题，但是抛出 SQLException 就无法回滚了。针对非运行时异常，如果要进行事务回滚的话，可以在 @Transactional 注解中使用 rollbackFor 属性来指定异常，比如 @Transactional(rollbackFor = Exception.class)，这样就没有问题了，所以在实际项目中，一定要指定异常。 3.2 异常被 ”吃“ 掉这个标题很搞笑，异常怎么会被吃掉呢？还是回归到现实项目中去，我们在处理异常时，有两种方式，要么抛出去，让上一层来捕获处理；要么把异常 try catch 掉，在异常出现的地方给处理掉。就因为有这中 try…catch，所以导致异常被 ”吃“ 掉，事务无法回滚。我们还是看上面那个例子，只不过简单修改一下代码： @Servicepublic class UserServiceImpl implements UserService &#123; @Resource private UserMapper userMapper; @Override @Transactional(rollbackFor = Exception.class) public void isertUser3(User user) &#123; try &#123; // 插入用户信息 userMapper.insertUser(user); // 手动抛出异常 throw new SQLException(\"数据库异常\"); &#125; catch (Exception e) &#123; // 异常处理逻辑 &#125; &#125;&#125; 读者可以使用我源码中 controller 的接口，通过 postman 测试一下，就会发现，仍然是可以插入一条用户数据，说明事务并没有因为抛出异常而回滚。这个细节往往比上面那个坑更难以发现，因为我们的思维很容易导致 try…catch 代码的产生，一旦出现这种问题，往往排查起来比较费劲，所以我们平时在写代码时，一定要多思考，多注意这种细节，尽量避免给自己埋坑。 那这种怎么解决呢？直接往上抛，给上一层来处理即可，千万不要在事务中把异常自己 ”吃“ 掉。 3.3 事务的范围事务范围这个东西比上面两个坑埋的更深！我之所以把这个也写上，是因为这是我之前在实际项目中遇到的，该场景在这个课程中我就不模拟了，我写一个 demo 让大家看一下，把这个坑记住即可，以后在写代码时，遇到并发问题，就会注意这个坑了，那么这节课也就有价值了。 我来写个 demo： @Servicepublic class UserServiceImpl implements UserService &#123; @Resource private UserMapper userMapper; @Override @Transactional(rollbackFor = Exception.class) public synchronized void isertUser4(User user) &#123; // 实际中的具体业务…… userMapper.insertUser(user); &#125;&#125; 可以看到，因为要考虑并发问题，我在业务层代码的方法上加了个 synchronized 关键字。我举个实际的场景，比如一个数据库中，针对某个用户，只有一条记录，下一个插入动作过来，会先判断该数据库中有没有相同的用户，如果有就不插入，就更新，没有才插入，所以理论上，数据库中永远就一条同一用户信息，不会出现同一数据库中插入了两条相同用户的信息。 但是在压测时，就会出现上面的问题，数据库中确实有两条同一用户的信息，分析其原因，在于事务的范围和锁的范围问题。 从上面方法中可以看到，方法上是加了事务的，那么也就是说，在执行该方法开始时，事务启动，执行完了后，事务关闭。但是 synchronized 没有起作用，其实根本原因是因为事务的范围比锁的范围大。也就是说，在加锁的那部分代码执行完之后，锁释放掉了，但是事务还没结束，此时另一个线程进来了，事务没结束的话，第二个线程进来时，数据库的状态和第一个线程刚进来是一样的。即由于mysql Innodb引擎的默认隔离级别是可重复读（在同一个事务里，SELECT的结果是事务开始时时间点的状态），线程二事务开始的时候，线程一还没提交完成，导致读取的数据还没更新。第二个线程也做了插入动作，导致了脏数据。 这个问题可以避免，第一，把事务去掉即可（不推荐）；第二，在调用该 service 的地方加锁，保证锁的范围比事务的范围大即可。 4. 总结本章主要总结了 Spring Boot 中如何使用事务，只要使用 @Transactional 注解即可使用，非常简单方便。除此之外，重点总结了三个在实际项目中可能遇到的坑点，这非常有意义，因为事务这东西不出问题还好，出了问题比较难以排查，所以总结的这三点注意事项，希望能帮助到开发中的朋友。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第12课：Spring Boot中使用监听器1. 监听器介绍什么是 web 监听器？web 监听器是一种 Servlet 中特殊的类，它们能帮助开发者监听 web 中特定的事件，比如 ServletContext, HttpSession, ServletRequest 的创建和销毁；变量的创建、销毁和修改等。可以在某些动作前后增加处理，实现监控。 2. Spring Boot中监听器的使用web 监听器的使用场景很多，比如监听 servlet 上下文用来初始化一些数据、监听 http session 用来获取当前在线的人数、监听客户端请求的 servlet request 对象来获取用户的访问信息等等。这一节中，我们主要通过这三个实际的使用场景来学习一下 Spring Boot 中监听器的使用。 2.1 监听Servlet上下文对象监听 servlet 上下文对象可以用来初始化数据，用于缓存。什么意思呢？我举一个很常见的场景，比如用户在点击某个站点的首页时，一般都会展现出首页的一些信息，而这些信息基本上或者大部分时间都保持不变的，但是这些信息都是来自数据库。如果用户的每次点击，都要从数据库中去获取数据的话，用户量少还可以接受，如果用户量非常大的话，这对数据库也是一笔很大的开销。 针对这种首页数据，大部分都不常更新的话，我们完全可以把它们缓存起来，每次用户点击的时候，我们都直接从缓存中拿，这样既可以提高首页的访问速度，又可以降低服务器的压力。如果做的更加灵活一点，可以再加个定时器，定期的来更新这个首页缓存。就类似与 CSDN 个人博客首页中排名的变化一样。 下面我们针对这个功能，来写一个 demo，在实际中，读者可以完全套用该代码，来实现自己项目中的相关逻辑。首先写一个 Service，模拟一下从数据库查询数据： @Servicepublic class UserService &#123; /** * 获取用户信息 * @return */ public User getUser() &#123; // 实际中会根据具体的业务场景，从数据库中查询对应的信息 return new User(1L, \"倪升武\", \"123456\"); &#125;&#125; 然后写一个监听器，实现 ApplicationListener&lt;ContextRefreshedEvent&gt; 接口，重写 onApplicationEvent 方法，将 ContextRefreshedEvent 对象传进去。如果我们想在加载或刷新应用上下文时，也重新刷新下我们预加载的资源，就可以通过监听 ContextRefreshedEvent 来做这样的事情。如下： /** * 使用ApplicationListener来初始化一些数据到application域中的监听器 * @author shengni ni * @date 2018/07/05 */@Componentpublic class MyServletContextListener implements ApplicationListener&lt;ContextRefreshedEvent&gt; &#123; @Override public void onApplicationEvent(ContextRefreshedEvent contextRefreshedEvent) &#123; // 先获取到application上下文 ApplicationContext applicationContext = contextRefreshedEvent.getApplicationContext(); // 获取对应的service UserService userService = applicationContext.getBean(UserService.class); User user = userService.getUser(); // 获取application域对象，将查到的信息放到application域中 ServletContext application = applicationContext.getBean(ServletContext.class); application.setAttribute(\"user\", user); &#125;&#125; 正如注释中描述的一样，首先通过 contextRefreshedEvent 来获取 application 上下文，再通过 application 上下文来获取 UserService 这个 bean，项目中可以根据实际业务场景，也可以获取其他的 bean，然后再调用自己的业务代码获取相应的数据，最后存储到 application 域中，这样前端在请求相应数据的时候，我们就可以直接从 application 域中获取信息，减少数据库的压力。下面写一个 Controller 直接从 application 域中获取 user 信息来测试一下。 @RestController@RequestMapping(\"/listener\")public class TestController &#123; @GetMapping(\"/user\") public User getUser(HttpServletRequest request) &#123; ServletContext application = request.getServletContext(); return (User) application.getAttribute(\"user\"); &#125;&#125; 启动项目，在浏览器中输入 http://localhost:8080/listener/user 测试一下即可，如果正常返回 user 信息，那么说明数据已经缓存成功。不过 application 这种是缓存在内存中，对内存会有消耗，后面的课程中我会讲到 redis，到时候再给大家介绍一下 redis 的缓存。 2.2 监听HTTP会话 Session对象监听器还有一个比较常用的地方就是用来监听 session 对象，来获取在线用户数量，现在有很多开发者都有自己的网站，监听 session 来获取当前在下用户数量是个很常见的使用场景，下面来介绍一下如何来使用。 /** * 使用HttpSessionListener统计在线用户数的监听器 * @author shengwu ni * @date 2018/07/05 */@Componentpublic class MyHttpSessionListener implements HttpSessionListener &#123; private static final Logger logger = LoggerFactory.getLogger(MyHttpSessionListener.class); /** * 记录在线的用户数量 */ public Integer count = 0; @Override public synchronized void sessionCreated(HttpSessionEvent httpSessionEvent) &#123; logger.info(\"新用户上线了\"); count++; httpSessionEvent.getSession().getServletContext().setAttribute(\"count\", count); &#125; @Override public synchronized void sessionDestroyed(HttpSessionEvent httpSessionEvent) &#123; logger.info(\"用户下线了\"); count--; httpSessionEvent.getSession().getServletContext().setAttribute(\"count\", count); &#125;&#125; 可以看出，首先该监听器需要实现 HttpSessionListener 接口，然后重写 sessionCreated 和 sessionDestroyed 方法，在 sessionCreated 方法中传递一个 HttpSessionEvent 对象，然后将当前 session 中的用户数量加1，sessionDestroyed 方法刚好相反，不再赘述。然后我们写一个 Controller 来测试一下。 @RestController@RequestMapping(\"/listener\")public class TestController &#123; /** * 获取当前在线人数，该方法有bug * @param request * @return */ @GetMapping(\"/total\") public String getTotalUser(HttpServletRequest request) &#123; Integer count = (Integer) request.getSession().getServletContext().getAttribute(\"count\"); return \"当前在线人数：\" + count; &#125;&#125; 该 Controller 中是直接获取当前 session 中的用户数量，启动服务器，在浏览器中输入 localhost:8080/listener/total 可以看到返回的结果是1，再打开一个浏览器，请求相同的地址可以看到 count 是 2 ，这没有问题。但是如果关闭一个浏览器再打开，理论上应该还是2，但是实际测试却是 3。原因是 session 销毁的方法没有执行（可以在后台控制台观察日志打印情况），当重新打开时，服务器找不到用户原来的 session，于是又重新创建了一个 session，那怎么解决该问题呢？我们可以将上面的 Controller 方法改造一下： @GetMapping(\"/total2\")public String getTotalUser(HttpServletRequest request, HttpServletResponse response) &#123; Cookie cookie; try &#123; // 把sessionId记录在浏览器中 cookie = new Cookie(\"JSESSIONID\", URLEncoder.encode(request.getSession().getId(), \"utf-8\")); cookie.setPath(\"/\"); //设置cookie有效期为2天，设置长一点 cookie.setMaxAge( 48*60 * 60); response.addCookie(cookie); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; Integer count = (Integer) request.getSession().getServletContext().getAttribute(\"count\"); return \"当前在线人数：\" + count;&#125; 可以看出，该处理逻辑是让服务器记得原来那个 session，即把原来的 sessionId 记录在浏览器中，下次再打开时，把这个 sessionId 传过去，这样服务器就不会重新再创建了。重启一下服务器，在浏览器中再次测试一下，即可避免上面的问题。 2.3 监听客户端请求Servlet Request对象使用监听器获取用户的访问信息比较简单，实现 ServletRequestListener 接口即可，然后通过 request 对象获取一些信息。如下： /** * 使用ServletRequestListener获取访问信息 * @author shengwu ni * @date 2018/07/05 */@Componentpublic class MyServletRequestListener implements ServletRequestListener &#123; private static final Logger logger = LoggerFactory.getLogger(MyServletRequestListener.class); @Override public void requestInitialized(ServletRequestEvent servletRequestEvent) &#123; HttpServletRequest request = (HttpServletRequest) servletRequestEvent.getServletRequest(); logger.info(\"session id为：&#123;&#125;\", request.getRequestedSessionId()); logger.info(\"request url为：&#123;&#125;\", request.getRequestURL()); request.setAttribute(\"name\", \"倪升武\"); &#125; @Override public void requestDestroyed(ServletRequestEvent servletRequestEvent) &#123; logger.info(\"request end\"); HttpServletRequest request = (HttpServletRequest) servletRequestEvent.getServletRequest(); logger.info(\"request域中保存的name值为：&#123;&#125;\", request.getAttribute(\"name\")); &#125;&#125; 这个比较简单，不再赘述，接下来写一个 Controller 测试一下即可。 @GetMapping(\"/request\")public String getRequestInfo(HttpServletRequest request) &#123; System.out.println(\"requestListener中的初始化的name数据：\" + request.getAttribute(\"name\")); return \"success\";&#125; 3. Spring Boot中自定义事件监听在实际项目中，我们往往需要自定义一些事件和监听器来满足业务场景，比如在微服务中会有这样的场景：微服务 A 在处理完某个逻辑之后，需要通知微服务 B 去处理另一个逻辑，或者微服务 A 处理完某个逻辑之后，需要将数据同步到微服务 B，这种场景非常普遍，这个时候，我们可以自定义事件以及监听器来监听，一旦监听到微服务 A 中的某事件发生，就去通知微服务 B 处理对应的逻辑。 3.1 自定义事件自定义事件需要继承 ApplicationEvent 对象，在事件中定义一个 User 对象来模拟数据，构造方法中将 User 对象传进来初始化。如下： /** * 自定义事件 * @author shengwu ni * @date 2018/07/05 */public class MyEvent extends ApplicationEvent &#123; private User user; public MyEvent(Object source, User user) &#123; super(source); this.user = user; &#125; // 省去get、set方法&#125; 3.2 自定义监听器接下来，自定义一个监听器来监听上面定义的 MyEvent 事件，自定义监听器需要实现 ApplicationListener 接口即可。如下： /** * 自定义监听器，监听MyEvent事件 * @author shengwu ni * @date 2018/07/05 */@Componentpublic class MyEventListener implements ApplicationListener&lt;MyEvent&gt; &#123; @Override public void onApplicationEvent(MyEvent myEvent) &#123; // 把事件中的信息获取到 User user = myEvent.getUser(); // 处理事件，实际项目中可以通知别的微服务或者处理其他逻辑等等 System.out.println(\"用户名：\" + user.getUsername()); System.out.println(\"密码：\" + user.getPassword()); &#125;&#125; 然后重写 onApplicationEvent 方法，将自定义的 MyEvent 事件传进来，因为该事件中，我们定义了 User 对象（该对象在实际中就是需要处理的数据，在下文来模拟），然后就可以使用该对象的信息了。 OK，定义好了事件和监听器之后，需要手动发布事件，这样监听器才能监听到，这需要根据实际业务场景来触发，针对本文的例子，我写个触发逻辑，如下： /** * UserService * @author shengwu ni */@Servicepublic class UserService &#123; @Resource private ApplicationContext applicationContext; /** * 发布事件 * @return */ public User getUser2() &#123; User user = new User(1L, \"倪升武\", \"123456\"); // 发布事件 MyEvent event = new MyEvent(this, user); applicationContext.publishEvent(event); return user; &#125;&#125; 在 service 中注入 ApplicationContext，在业务代码处理完之后，通过 ApplicationContext 对象手动发布 MyEvent 事件，这样我们自定义的监听器就能监听到，然后处理监听器中写好的业务逻辑。 最后，在 Controller 中写一个接口来测试一下： @GetMapping(\"/request\")public String getRequestInfo(HttpServletRequest request) &#123; System.out.println(\"requestListener中的初始化的name数据：\" + request.getAttribute(\"name\")); return \"success\";&#125; 在浏览器中输入 http://localhost:8080/listener/publish，然后观察一下控制台打印的用户名和密码，即可说明自定义监听器已经生效。 4. 总结本课系统的介绍了监听器原理，以及在 Spring Boot 中如何使用监听器，列举了监听器的三个常用的案例，有很好的实战意义。最后讲解了项目中如何自定义事件和监听器，并结合微服务中常见的场景，给出具体的代码模型，均能运用到实际项目中去，希望读者认真消化。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第13课：Spring Boot中使用拦截器拦截器的原理很简单，是 AOP 的一种实现，专门拦截对动态资源的后台请求，即拦截对控制层的请求。使用场景比较多的是判断用户是否有权限请求后台，更拔高一层的使用场景也有，比如拦截器可以结合 websocket 一起使用，用来拦截 websocket 请求，然后做相应的处理等等。拦截器不会拦截静态资源，Spring Boot 的默认静态目录为 resources/static，该目录下的静态页面、js、css、图片等等，不会被拦截（也要看如何实现，有些情况也会拦截，我在下文会指出）。 1. 拦截器的快速使用使用拦截器很简单，只需要两步即可：定义拦截器和配置拦截器。在配置拦截器中，Spring Boot 2.0 以后的版本和之前的版本有所不同，我会重点讲解一下这里可能出现的坑。 1.1 定义拦截器定义拦截器，只需要实现 HandlerInterceptor 接口，HandlerInterceptor 接口是所有自定义拦截器或者 Spring Boot 提供的拦截器的鼻祖，所以，首先来了解下该接口。该接口中有三个方法： preHandle(……)、postHandle(……) 和 afterCompletion(……) 。 preHandle(……) 方法：该方法的执行时机是，当某个 url 已经匹配到对应的 Controller 中的某个方法，且在这个方法执行之前。所以 preHandle(……) 方法可以决定是否将请求放行，这是通过返回值来决定的，返回 true 则放行，返回 false 则不会向后执行。postHandle(……) 方法：该方法的执行时机是，当某个 url 已经匹配到对应的 Controller 中的某个方法，且在执行完了该方法，但是在 DispatcherServlet 视图渲染之前。所以在这个方法中有个 ModelAndView 参数，可以在此做一些修改动作。afterCompletion(……) 方法：顾名思义，该方法是在整个请求处理完成后（包括视图渲染）执行，这时做一些资源的清理工作，这个方法只有在 preHandle(……) 被成功执行后并且返回 true 才会被执行。 了解了该接口，接下来自定义一个拦截器。 /** * 自定义拦截器 * @author shengwu ni * @date 2018/08/03 */public class MyInterceptor implements HandlerInterceptor &#123; private static final Logger logger = LoggerFactory.getLogger(MyInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); String methodName = method.getName(); logger.info(\"====拦截到了方法：&#123;&#125;，在该方法执行之前执行====\", methodName); // 返回true才会继续执行，返回false则取消当前请求 return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; logger.info(\"执行完方法之后进执行(Controller方法调用之后)，但是此时还没进行视图渲染\"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; logger.info(\"整个请求都处理完咯，DispatcherServlet也渲染了对应的视图咯，此时我可以做一些清理的工作了\"); &#125;&#125; OK，到此为止，拦截器已经定义完成，接下来就是对该拦截器进行拦截配置。 1.2 配置拦截器在 Spring Boot 2.0 之前，我们都是直接继承 WebMvcConfigurerAdapter 类，然后重写 addInterceptors 方法来实现拦截器的配置。但是在 Spring Boot 2.0 之后，该方法已经被废弃了（当然，也可以继续用），取而代之的是 WebMvcConfigurationSupport 方法，如下： @Configurationpublic class MyInterceptorConfig extends WebMvcConfigurationSupport &#123; @Override protected void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(new MyInterceptor()).addPathPatterns(\"/**\"); super.addInterceptors(registry); &#125;&#125; 在该配置中重写 addInterceptors 方法，将我们上面自定义的拦截器添加进去，addPathPatterns 方法是添加要拦截的请求，这里我们拦截所有的请求。这样就配置好拦截器了，接下来写一个 Controller 测试一下： @Controller@RequestMapping(\"/interceptor\")public class InterceptorController &#123; @RequestMapping(\"/test\") public String test() &#123; return \"hello\"; &#125;&#125; 让其跳转到 hello.html 页面，直接在 hello.html 中输出 hello interceptor 即可。启动项目，在浏览器中输入 localhost:8080/interceptor/test 看一下控制台的日志： &#x3D;&#x3D;&#x3D;&#x3D;拦截到了方法：test，在该方法执行之前执行&#x3D;&#x3D;&#x3D;&#x3D; 执行完方法之后进执行(Controller方法调用之后)，但是此时还没进行视图渲染 整个请求都处理完咯，DispatcherServlet也渲染了对应的视图咯，此时我可以做一些清理的工作了 可以看出拦截器已经生效，并能看出其执行顺序。 1.3 解决静态资源被拦截问题上文中已经介绍了拦截器的定义和配置，但是这样是否就没问题了呢？其实不然，如果使用上面这种配置的话，我们会发现一个缺陷，那就是静态资源被拦截了。可以在 resources/static/ 目录下放置一个图片资源或者 html 文件，然后启动项目直接访问，即可看到无法访问的现象。 也就是说，虽然 Spring Boot 2.0 废弃了WebMvcConfigurerAdapter，但是 WebMvcConfigurationSupport 又会导致默认的静态资源被拦截，这就需要我们手动将静态资源放开。 如何放开呢？除了在 MyInterceptorConfig 配置类中重写 addInterceptors 方法外，还需要再重写一个方法：addResourceHandlers，将静态资源放开： /** * 用来指定静态资源不被拦截，否则继承WebMvcConfigurationSupport这种方式会导致静态资源无法直接访问 * @param registry */@Overrideprotected void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(\"/**\").addResourceLocations(\"classpath:/static/\"); super.addResourceHandlers(registry);&#125; 这样配置好之后，重启项目，静态资源也可以正常访问了。如果你是个善于学习或者研究的人，那肯定不会止步于此，没错，上面这种方式的确能解决静态资源无法访问的问题，但是，还有更方便的方式来配置。 我们不继承 WebMvcConfigurationSupport 类，直接实现 WebMvcConfigurer 接口，然后重写 addInterceptors 方法，将自定义的拦截器添加进去即可，如下： @Configurationpublic class MyInterceptorConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 实现WebMvcConfigurer不会导致静态资源被拦截 registry.addInterceptor(new MyInterceptor()).addPathPatterns(\"/**\"); &#125;&#125; 这样就非常方便了，实现 WebMvcConfigure 接口的话，不会拦截 Spring Boot 默认的静态资源。 这两种方式都可以，具体他们之间的细节，感兴趣的读者可以做进一步的研究，由于这两种方式的不同，继承 WebMvcConfigurationSupport 类的方式可以用在前后端分离的项目中，后台不需要访问静态资源（就不需要放开静态资源了）；实现 WebMvcConfigure 接口的方式可以用在非前后端分离的项目中，因为需要读取一些图片、css、js文件等等。 2. 拦截器使用实例2.1 判断用户有没有登录一般用户登录功能我们可以这么做，要么往 session 中写一个 user，要么针对每个 user 生成一个 token，第二种要更好一点，那么针对第二种方式，如果用户登录成功了，每次请求的时候都会带上该用户的 token，如果未登录，则没有该 token，服务端可以检测这个 token 参数的有无来判断用户有没有登录，从而实现拦截功能。我们改造一下 preHandle 方法，如下： @Overridepublic boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); String methodName = method.getName(); logger.info(\"====拦截到了方法：&#123;&#125;，在该方法执行之前执行====\", methodName); // 判断用户有没有登陆，一般登陆之后的用户都有一个对应的token String token = request.getParameter(\"token\"); if (null == token || \"\".equals(token)) &#123; logger.info(\"用户未登录，没有权限执行……请登录\"); return false; &#125; // 返回true才会继续执行，返回false则取消当前请求 return true;&#125; 重启项目，在浏览器中输入 localhost:8080/interceptor/test 后查看控制台日志，发现被拦截，如果在浏览器中输入 localhost:8080/interceptor/test?token=123 即可正常往下走。 2.2 取消拦截操作根据上文，如果我要拦截所有 /admin 开头的 url 请求的话，需要在拦截器配置中添加这个前缀，但是在实际项目中，可能会有这种场景出现：某个请求也是 /admin 开头的，但是不能拦截，比如 /admin/login 等等，这样的话又需要去配置。那么，可不可以做成一个类似于开关的东西，哪里不需要拦截，我就在哪里弄个开关上去，做成这种灵活的可插拔的效果呢？ 是可以的，我们可以定义一个注解，该注解专门用来取消拦截操作，如果某个 Controller 中的方法我们不需要拦截掉，即可在该方法上加上我们自定义的注解即可，下面先定义一个注解： /** * 该注解用来指定某个方法不用拦截 */@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface UnInterception &#123;&#125; 然后在 Controller 中的某个方法上添加该注解，在拦截器处理方法中添加该注解取消拦截的逻辑，如下： @Overridepublic boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); String methodName = method.getName(); logger.info(\"====拦截到了方法：&#123;&#125;，在该方法执行之前执行====\", methodName); // 通过方法，可以获取该方法上的自定义注解，然后通过注解来判断该方法是否要被拦截 // @UnInterception 是我们自定义的注解 UnInterception unInterception = method.getAnnotation(UnInterception.class); if (null != unInterception) &#123; return true; &#125; // 返回true才会继续执行，返回false则取消当前请求 return true;&#125; Controller 中的方法代码可以参见源码，重启项目在浏览器中输入 http://localhost:8080/interceptor/test2?token=123 测试一下，可以看出，加了该注解的方法不会被拦截。 3. 总结本节主要介绍了 Spring Boot 中拦截器的使用，从拦截器的创建、配置，到拦截器对静态资源的影响，都做了详细的分析。Spring Boot 2.0 之后拦截器的配置支持两种方式，可以根据实际情况选择不同的配置方式。最后结合实际中的使用，举了两个常用的场景，希望读者能够认真消化，掌握拦截器的使用。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第14课：Spring Boot 中集成Redis1. Redis 介绍Redis 是一种非关系型数据库（NoSQL），NoSQL 是以 key-value 的形式存储的，和传统的关系型数据库不一样，不一定遵循传统数据库的一些基本要求，比如说 SQL 标准，ACID 属性，表结构等等，这类数据库主要有以下特点：非关系型的、分布式的、开源的、水平可扩展的。NoSQL 使用场景有：对数据高并发读写、对海量数据的高效率存储和访问、对数据的高可扩展性和高可用性等等。Redis 的 key 可以是字符串、哈希、链表、集合和有序集合。value 类型很多，包括 String、list、set、zset。这些数据类型都支持 push/pop、add/remove、取交集和并集以及更多更丰富的操作，Redis 也支持各种不同方式的排序。为了保证效率，数据都是在缓存在内存中，它也可以周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件中。 有了 redis 有哪些好处呢？举个比较简单的例子，看下图： Redis 集群和 Mysql 是同步的，首先会从 redis 中获取数据，如果 redis 挂了，再从 mysql 中获取数据，这样网站就不会挂掉。更多关于 redis 的介绍以及使用场景，可以谷歌和百度，在这就不赘述了。 2. Redis 安装本课程是在 vmvare 虚拟机中来安装的 redis （centos 7），学习的时候如果有自己的阿里云服务器，也可以在阿里云中来安装 redis，都可以。只要能 ping 的通云主机或者虚拟机的 ip，然后在虚拟机或者云主机中放行对应的端口（或者关掉防火墙）即可访问 redis。下面来介绍一下 redis 的安装过程： 安装 gcc 编译 因为后面安装redis的时候需要编译，所以事先得先安装gcc编译。阿里云主机已经默认安装了 gcc，如果是自己安装的虚拟机，那么需要先安装一下 gcc： yum install gcc-c++ 下载 redis 有两种方式下载安装包，一种是去官网上下载（https://redis.io），然后将安装包考到 centos 中，另种方法是直接使用 wget 来下载： wget http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-3.2.8.tar.gz 如果没有安装过 wget，可以通过如下命令安装： yum install wget 解压安装 解压安装包： tar –vzxf redis-3.2.8.tar.gz 然后将解压的文件夹 redis-3.2.8 放到 /usr/local/ 下，一般安装软件都放在 /usr/local 下。然后进入 /usr/local/redis-3.2.8/ 文件夹下，执行 make 命令即可完成安装。【注】如果 make 失败，可以尝试如下命令： make MALLOC&#x3D;libcmake install 修改配置文件 安装成功之后，需要修改一下配置文件，包括允许接入的 ip，允许后台执行，设置密码等等。打开 redis 配置文件：vi redis.conf在命令模式下输入 /bind 来查找 bind 配置，按 n 来查找下一个，找到配置后，将 bind 配置成 0.0.0.0，允许任意服务器来访问 redis，即： bind 0.0.0.0 使用同样的方法，将 daemonize 改成 yes （默认为 no），允许 redis 在后台执行。将 requirepass 注释打开，并设置密码为 123456（密码自己设置）。 启动 redis 在 redis-3.2.8 目录下，指定刚刚修改好的配置文件 redis.conf 来启动 redis： redis-server .&#x2F;redis.conf 再启动 redis 客户端： redis-cli 由于我们设置了密码，在启动客户端之后，输入 auth 123456 即可登录进入客户端。然后我们来测试一下，往 redis 中插入一个数据： set name CSDN 然后来获取 name get name 如果正常获取到 CSDN，则说明没有问题。 3. Spring Boot 集成 Redis3.1 依赖导入Spring Boot 集成 redis 很方便，只需要导入一个 redis 的 starter 依赖即可。如下： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--阿里巴巴fastjson --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.35&lt;/version&gt;&lt;/dependency&gt; 这里也导入阿里巴巴的 fastjson 是为了在后面我们要存一个实体，为了方便把实体转换成 json 字符串存进去。 3.2 Redis 配置导入了依赖之后，我们在 application.yml 文件里配置 redis： server: port: 8080spring: #redis相关配置 redis: database: 5 # 配置redis的主机地址，需要修改成自己的 host: 192.168.48.190 port: 6379 password: 123456 timeout: 5000 jedis: pool: # 连接池中的最大空闲连接，默认值也是8。 max-idle: 500 # 连接池中的最小空闲连接，默认值也是0。 min-idle: 50 # 如果赋值为-1，则表示不限制；如果pool已经分配了maxActive个jedis实例，则此时pool的状态为exhausted(耗尽) max-active: 1000 # 等待可用连接的最大时间，单位毫秒，默认值为-1，表示永不超时。如果超过等待时间，则直接抛出JedisConnectionException max-wait: 2000 3.3 常用 api 介绍Spring Boot 对 redis 的支持已经非常完善了，丰富的 api 已经足够我们日常的开发，这里我介绍几个最常用的供大家学习，其他 api 希望大家自己多学习，多研究。用到会去查即可。 有两个 redis 模板：RedisTemplate 和 StringRedisTemplate。我们不使用 RedisTemplate，RedisTemplate 提供给我们操作对象，操作对象的时候，我们通常是以 json 格式存储，但在存储的时候，会使用 Redis 默认的内部序列化器；导致我们存进里面的是乱码之类的东西。当然了，我们可以自己定义序列化，但是比较麻烦，所以使用 StringRedisTemplate 模板。StringRedisTemplate 主要给我们提供字符串操作，我们可以将实体类等转成 json 字符串即可，在取出来后，也可以转成相应的对象，这就是上面我导入了阿里 fastjson 的原因。 3.3.1 redis:string 类型新建一个 RedisService，注入 StringRedisTemplate，使用 stringRedisTemplate.opsForValue() 可以获取 ValueOperations&lt;String, String&gt; 对象，通过该对象即可读写 redis 数据库了。如下： public class RedisService &#123; @Resource private StringRedisTemplate stringRedisTemplate; /** * set redis: string类型 * @param key key * @param value value */ public void setString(String key, String value)&#123; ValueOperations&lt;String, String&gt; valueOperations = stringRedisTemplate.opsForValue(); valueOperations.set(key, value); &#125; /** * get redis: string类型 * @param key key * @return */ public String getString(String key)&#123; return stringRedisTemplate.opsForValue().get(key); &#125; 该对象操作的是 string，我们也可以存实体类，只需要将实体类转换成 json 字符串即可。下面来测试一下： @RunWith(SpringRunner.class)@SpringBootTestpublic class Course14ApplicationTests &#123; private static final Logger logger = LoggerFactory.getLogger(Course14ApplicationTests.class); @Resource private RedisService redisService; @Test public void contextLoads() &#123; //测试redis的string类型 redisService.setString(\"weichat\",\"程序员私房菜\"); logger.info(\"我的微信公众号为：&#123;&#125;\", redisService.getString(\"weichat\")); // 如果是个实体，我们可以使用json工具转成json字符串， User user = new User(\"CSDN\", \"123456\"); redisService.setString(\"userInfo\", JSON.toJSONString(user)); logger.info(\"用户信息：&#123;&#125;\", redisService.getString(\"userInfo\")); &#125;&#125; 先启动 redis，然后运行这个测试用例，观察控制台打印的日志如下： 我的微信公众号为：程序员私房菜用户信息：&#123;&quot;password&quot;:&quot;123456&quot;,&quot;username&quot;:&quot;CSDN&quot;&#125; 3.3.2 redis:hash 类型hash 类型其实原理和 string 一样的，但是有两个 key，使用 stringRedisTemplate.opsForHash() 可以获取 HashOperations&lt;String, Object, Object&gt; 对象。比如我们要存储订单信息，所有订单信息都放在 order 下，针对不同用户的订单实体，可以通过用户的 id 来区分，这就相当于两个 key 了。 @Servicepublic class RedisService &#123; @Resource private StringRedisTemplate stringRedisTemplate; /** * set redis: hash类型 * @param key key * @param filedKey filedkey * @param value value */ public void setHash(String key, String filedKey, String value)&#123; HashOperations&lt;String, Object, Object&gt; hashOperations = stringRedisTemplate.opsForHash(); hashOperations.put(key,filedKey, value); &#125; /** * get redis: hash类型 * @param key key * @param filedkey filedkey * @return */ public String getHash(String key, String filedkey)&#123; return (String) stringRedisTemplate.opsForHash().get(key, filedkey); &#125;&#125; 可以看出，hash 和 string 没啥两样，只不过多了个参数，Spring Boot 中操作 redis 非常简单方便。来测试一下： @SpringBootTestpublic class Course14ApplicationTests &#123; private static final Logger logger = LoggerFactory.getLogger(Course14ApplicationTests.class); @Resource private RedisService redisService; @Test public void contextLoads() &#123; //测试redis的hash类型 redisService.setHash(\"user\", \"name\", JSON.toJSONString(user)); logger.info(\"用户姓名：&#123;&#125;\", redisService.getHash(\"user\",\"name\")); &#125;&#125; 3.3.3 redis:list 类型使用 stringRedisTemplate.opsForList() 可以获取 ListOperations&lt;String, String&gt; listOperations redis 列表对象，该列表是个简单的字符串列表，可以支持从左侧添加，也可以支持从右侧添加，一个列表最多包含 2 ^ 32 -1 个元素。 @Servicepublic class RedisService &#123; @Resource private StringRedisTemplate stringRedisTemplate; /** * set redis:list类型 * @param key key * @param value value * @return */ public long setList(String key, String value)&#123; ListOperations&lt;String, String&gt; listOperations = stringRedisTemplate.opsForList(); return listOperations.leftPush(key, value); &#125; /** * get redis:list类型 * @param key key * @param start start * @param end end * @return */ public List&lt;String&gt; getList(String key, long start, long end)&#123; return stringRedisTemplate.opsForList().range(key, start, end); &#125;&#125; 可以看出，这些 api 都是一样的形式，方便记忆也方便使用。具体的 api 细节我就不展开了，大家可以自己看 api 文档。其实，这些 api 根据参数和返回值也能知道它们是做什么用的。来测试一下： @RunWith(SpringRunner.class)@SpringBootTestpublic class Course14ApplicationTests &#123; private static final Logger logger = LoggerFactory.getLogger(Course14ApplicationTests.class); @Resource private RedisService redisService; @Test public void contextLoads() &#123; //测试redis的list类型 redisService.setList(\"list\", \"football\"); redisService.setList(\"list\", \"basketball\"); List&lt;String&gt; valList = redisService.getList(\"list\",0,-1); for(String value :valList)&#123; logger.info(\"list中有：&#123;&#125;\", value); &#125; &#125;&#125; 4. 总结本节主要介绍了 redis 的使用场景、安装过程，以及 Spring Boot 中集成 redis 的详细步骤。在实际项目中，通常都用 redis 作为缓存，在查询数据库的时候，会先从 redis 中查找，如果有信息，则从 redis 中取；如果没有，则从数据库中查，并且同步到 redis 中，下次 redis 中就有了。更新和删除也是如此，都需要同步到 redis。redis 在高并发场景下运用的很多。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第15课： Spring Boot中集成ActiveMQ1. JMS 和 ActiveMQ 介绍1.1 JMS 是啥百度百科的解释： JMS 即 Java 消息服务（Java Message Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的 API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。Java 消息服务是一个与具体平台无关的 API，绝大多数 MOM 提供商都对 JMS 提供支持。 JMS 只是接口，不同的提供商或者开源组织对其有不同的实现，ActiveMQ 就是其中之一，它支持JMS，是 Apache 推出的。JMS 中有几个对象模型： 连接工厂：ConnectionFactoryJMS连接：ConnectionJMS会话：SessionJMS目的：DestinationJMS生产者：ProducerJMS消费者：ConsumerJMS消息两种类型：点对点和发布/订阅。 可以看出 JMS 实际上和 JDBC 有点类似，JDBC 是可以用来访问许多不同关系数据库的 API，而 JMS 则提供同样与厂商无关的访问方法，以访问消息收发服务。本文主要使用 ActiveMQ。 1.2 ActiveMQActiveMQ 是 Apache 的一个能力强劲的开源消息总线。ActiveMQ 完全支持JMS1.1和J2EE 1.4规范，尽管 JMS 规范出台已经是很久的事情了，但是 JMS 在当今的 Java EE 应用中间仍然扮演着特殊的地位。ActiveMQ 用在异步消息的处理上，所谓异步消息即消息发送者无需等待消息接收者的处理以及返回，甚至无需关心消息是否发送成功。 异步消息主要有两种目的地形式，队列（queue）和主题（topic），队列用于点对点形式的消息通信，主题用于发布/订阅式的消息通信。本章节主要来学习一下在 Spring Boot 中如何使用这两种形式的消息。 2. ActiveMQ安装使用 ActiveMQ 首先需要去官网下载，官网地址为：http://activemq.apache.org/本课程使用的版本是 apache-activemq-5.15.3，下载后解压缩会有一个名为 apache-activemq-5.15.3 的文件夹，没错，这就安装好了，非常简单，开箱即用。打开文件夹会看到里面有个 activemq-all-5.15.3.jar，这个 jar 我们是可以加进工程里的，但是使用 maven 的话，这个 jar 我们不需要。 在使用 ActiveMQ 之前，首先得先启动，刚才解压后的目录中有个 bin 目录，里面有 win32 和 win64 两个目录，根据自己电脑选择其中一个打开运行里面的 activemq.bat 即可启动 ActiveMQ。消息生产者生产消息发布到queue中，然后消息消费者从queue中取出，并且消费消息。这里需要注意：消息被消费者消费以后，queue中不再有存储，所以消息消费者不可消费到已经被消费的消息。Queue支持存在多个消息消费者，但是对一个消息而言，只会有一个消费者可以消费启动完成后，在浏览器中输入 http://127.0.0.1:8161/admin/ 来访问 ActiveMQ 的服务器，用户名和密码是 admin/admin。如下： 我们可以看到有 Queues 和 Topics 这两个选项，这两个选项分别是点对点消息和发布/订阅消息的查看窗口。何为点对点消息和发布/订阅消息呢？ 点对点消息：消息生产者生产消息发布到 queue 中，然后消息消费者从 queue 中取出，并且消费消息。这里需要注意：消息被消费者消费以后，queue 中不再有存储，所以消息消费者不可消费到已经被消费的消息。Queue 支持存在多个消息消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布/订阅消息：消息生产者（发布）将消息发布到 topic 中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic 的消息会被所有订阅者消费。下面分析具体的实现方式。 3. ActiveMQ集成3.1 依赖导入和配置在 Spring Boot 中集成 ActiveMQ 需要导入如下 starter 依赖： &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt;&lt;/dependency&gt; 然后在 application.yml 配置文件中，对 activemq 做一下配置： spring: activemq: # activemq url broker-url: tcp://localhost:61616 in-memory: true pool: # 如果此处设置为true，需要添加activemq-pool的依赖包，否则会自动配置失败，无法注入JmsMessagingTemplate enabled: false 3.2 Queue 和 Topic 的创建首先我们需要创建两种消息 Queue 和 Topic，这两种消息的创建，我们放到 ActiveMqConfig 中来创建，如下： /** * activemq的配置 * @author shengwu ni */@Configurationpublic class ActiveMqConfig &#123; /** * 发布/订阅模式队列名称 */ public static final String TOPIC_NAME = \"activemq.topic\"; /** * 点对点模式队列名称 */ public static final String QUEUE_NAME = \"activemq.queue\"; @Bean public Destination topic() &#123; return new ActiveMQTopic(TOPIC_NAME); &#125; @Bean public Destination queue() &#123; return new ActiveMQQueue(QUEUE_NAME); &#125;&#125; 可以看出创建 Queue 和 Topic 两种消息，分别使用 new ActiveMQQueue 和 new ActiveMQTopic 来创建，分别跟上对应消息的名称即可。这样在其他地方就可以直接将这两种消息作为组件注入进来了。 3.3 消息的发送接口在 Spring Boot 中，我们只要注入 JmsMessagingTemplate 模板即可快速发送消息，如下： /** * 消息发送者 * @author shengwu ni */@Servicepublic class MsgProducer &#123; @Resource private JmsMessagingTemplate jmsMessagingTemplate; public void sendMessage(Destination destination, String msg) &#123; jmsMessagingTemplate.convertAndSend(destination, msg); &#125;&#125; convertAndSend 方法中第一个参数是消息发送的目的地，第二个参数是具体的消息内容。 3.4 点对点消息生产与消费3.4.1 点对点消息的生产消息的生产，我们放到 Controller 中来做，由于上面已经生成了 Queue 消息的组件，所以在 Controller 中我们直接注入进来即可。然后调用上文的消息发送方法 sendMessage 即可成功生产一条消息。 /** * ActiveMQ controller * @author shengwu ni */@RestController@RequestMapping(\"/activemq\")public class ActiveMqController &#123; private static final Logger logger = LoggerFactory.getLogger(ActiveMqController.class); @Resource private MsgProducer producer; @Resource private Destination queue; @GetMapping(\"/send/queue\") public String sendQueueMessage() &#123; logger.info(\"===开始发送点对点消息===\"); producer.sendMessage(queue, \"Queue: hello activemq!\"); return \"success\"; &#125;&#125; 3.4.2 点对点消息的消费点对点消息的消费很简单，只要我们指定目的地即可，jms 监听器一直在监听是否有消息过来，如果有，则消费。 /** * 消息消费者 * @author shengwu ni */@Servicepublic class QueueConsumer &#123; /** * 接收点对点消息 * @param msg */ @JmsListener(destination = ActiveMqConfig.QUEUE_NAME) public void receiveQueueMsg(String msg) &#123; System.out.println(\"收到的消息为：\" + msg); &#125;&#125; 可以看出，使用 @JmsListener 注解来指定要监听的目的地，在消息接收方法内部，我们可以根据具体的业务需求做相应的逻辑处理即可。 3.4.3 测试一下启动项目，在浏览器中输入：http://localhost:8081/activemq/send/queue，观察控制台的输出日志，出现下面的日志说明消息发送和消费成功。 收到的消息为：Queue: hello activemq! 3.5 发布/订阅消息的生产和消费3.5.1 发布/订阅消息的生产和点对点消息一样，我们注入 topic 并调用 producer 的 sendMessage 方法即可发送订阅消息，如下，不再赘述： @RestController@RequestMapping(\"/activemq\")public class ActiveMqController &#123; private static final Logger logger = LoggerFactory.getLogger(ActiveMqController.class); @Resource private MsgProducer producer; @Resource private Destination topic; @GetMapping(\"/send/topic\") public String sendTopicMessage() &#123; logger.info(\"===开始发送订阅消息===\"); producer.sendMessage(topic, \"Topic: hello activemq!\"); return \"success\"; &#125;&#125; 3.5.2 发布/订阅消息的消费发布/订阅消息的消费和点对点不同，订阅消息支持多个消费者一起消费。其次，Spring Boot 中默认的时点对点消息，所以在使用 topic 时，会不起作用，我们需要在配置文件 application.yml 中添加一个配置： spring: jms: pub-sub-domain: true 该配置是 false 的话，则为点对点消息，也是 Spring Boot 默认的。这样是可以解决问题，但是如果这样配置的话，上面提到的点对点消息又不能正常消费了。所以二者不可兼得，这并非一个好的解决办法。 比较好的解决办法是，我们定义一个工厂，@JmsListener 注解默认只接收 queue 消息，如果要接收 topic 消息，需要设置一下 containerFactory。我们还在上面的那个 ActiveMqConfig 配置类中添加： /** * activemq的配置 * * @author shengwu ni */@Configurationpublic class ActiveMqConfig &#123; // 省略其他内容 /** * JmsListener注解默认只接收queue消息,如果要接收topic消息,需要设置containerFactory */ @Bean public JmsListenerContainerFactory topicListenerContainer(ConnectionFactory connectionFactory) &#123; DefaultJmsListenerContainerFactory factory = new DefaultJmsListenerContainerFactory(); factory.setConnectionFactory(connectionFactory); // 相当于在application.yml中配置：spring.jms.pub-sub-domain=true factory.setPubSubDomain(true); return factory; &#125;&#125; 经过这样的配置之后，我们在消费的时候，在 @JmsListener 注解中指定这个容器工厂即可消费 topic 消息。如下： /** * Topic消息消费者 * @author shengwu ni */@Servicepublic class TopicConsumer1 &#123; /** * 接收订阅消息 * @param msg */ @JmsListener(destination = ActiveMqConfig.TOPIC_NAME, containerFactory = \"topicListenerContainer\") public void receiveTopicMsg(String msg) &#123; System.out.println(\"收到的消息为：\" + msg); &#125;&#125; 指定 containerFactory 属性为上面我们自己配置的 topicListenerContainer 即可。由于 topic 消息可以多个消费，所以该消费的类可以拷贝几个一起测试一下，这里我就不贴代码了，可以参考我的源码测试。 3.5.3 测试一下启动项目，在浏览器中输入：http://localhost:8081/activemq/send/topic，观察控制台的输出日志，出现下面的日志说明消息发送和消费成功。 收到的消息为：Topic: hello activemq!收到的消息为：Topic: hello activemq! 4. 总结本章主要介绍了 jms 和 activemq 的相关概念、activemq 的安装与启动。详细分析了 Spring Boot 中点对点消息和发布/订阅消息两种方式的配置、消息生产和消费方式。ActiveMQ 是能力强劲的开源消息总线，在异步消息的处理上很有用，希望大家好好消化一下。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第16课：Spring Boot中集成 ShiroShiro 是一个强大、简单易用的 Java 安全框架，主要用来更便捷的认证，授权，加密，会话管等等，可为任何应用提供安全保障。本课程主要来介绍 Shiro 的认证和授权功能。 1. Shiro 三大核心组件Shiro 有三大核心的组件：Subject、SecurityManager 和 Realm。先来看一下它们之间的关系。 Subject：认证主体。它包含两个信息：Principals 和 Credentials。看一下这两个信息具体是什么。 Principals：身份。可以是用户名，邮件，手机号码等等，用来标识一个登录主体身份；Credentials：凭证。常见有密码，数字证书等等。 说白了，就是需要认证的东西，最常见的就是用户名密码了，比如用户在登录的时候，Shiro 需要去进行身份认证，就需要 Subject 认证主体。 SecurityManager：安全管理员。这是 Shiro 架构的核心，它就像 Shiro 内部所有原件的保护伞一样。我们在项目中一般都会配置 SecurityManager，开发人员大部分精力主要是在 Subject 认证主体上面。我们在与 Subject 进行交互的时候，实际上是 SecurityManager 在背后做一些安全操作。 Realms：Realms 是一个域，它是连接 Shiro 和具体应用的桥梁，当需要与安全数据交互的时候，比如用户账户、访问控制等，Shiro 就会从一个或多个 Realms 中去查找。我们一般会自己定制 Realm，这在下文会详细说明。 1. Shiro 身份和权限认证1.2 Shiro 身份认证我们来分析一下 Shiro 身份认证的过程，看一下官方的一个认证图： Step1：应用程序代码在调用 Subject.login(token) 方法后，传入代表最终用户的身份和凭证的 AuthenticationToken 实例 token。 Step2：将 Subject 实例委托给应用程序的 SecurityManager（Shiro的安全管理）来开始实际的认证工作。这里开始真正的认证工作了。 Step3，4，5：然后 SecurityManager 就会根据具体的 realm 去进行安全认证了。 从图中可以看出，realm 可以自定义（Custom Realm）。 1.3 Shiro 权限认证权限认证，也就是访问控制，即在应用中控制谁能访问哪些资源。在权限认证中，最核心的三个要素是：权限，角色和用户。 权限（permission）：即操作资源的权利，比如访问某个页面，以及对某个模块的数据的添加，修改，删除，查看的权利；角色（role）：指的是用户担任的的角色，一个角色可以有多个权限；用户（user）：在 Shiro 中，代表访问系统的用户，即上面提到的 Subject 认证主体。 它们之间的的关系可以用下图来表示： 一个用户可以有多个角色，而不同的角色可以有不同的权限，也可由有相同的权限。比如说现在有三个角色，1是普通角色，2也是普通角色，3是管理员，角色1只能查看信息，角色2只能添加信息，管理员都可以，而且还可以删除信息，类似于这样。 2. Spring Boot 集成 Shiro 过程2.1 依赖导入Spring Boot 2.0.3 集成 Shiro 需要导入如下 starter 依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-spring&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt; 2.2 数据库表数据初始化这里主要涉及到三张表：用户表、角色表和权限表，其实在 demo 中，我们完全可以自己模拟一下，不用建表，但是为了更加接近实际情况，我们还是加入 mybatis，来操作数据库。下面是数据库表的脚本。 CREATE TABLE `t_role` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键', `rolename` varchar(20) DEFAULT NULL COMMENT '角色名称', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8CREATE TABLE `t_user` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '用户主键', `username` varchar(20) NOT NULL COMMENT '用户名', `password` varchar(20) NOT NULL COMMENT '密码', `role_id` int(11) DEFAULT NULL COMMENT '外键关联role表', PRIMARY KEY (`id`), KEY `role_id` (`role_id`), CONSTRAINT `t_user_ibfk_1` FOREIGN KEY (`role_id`) REFERENCES `t_role` (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8CREATE TABLE `t_permission` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT '主键', `permissionname` varchar(50) NOT NULL COMMENT '权限名', `role_id` int(11) DEFAULT NULL COMMENT '外键关联role', PRIMARY KEY (`id`), KEY `role_id` (`role_id`), CONSTRAINT `t_permission_ibfk_1` FOREIGN KEY (`role_id`) REFERENCES `t_role` (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 其中，t_user，t_role 和 t_permission，分别存储用户信息，角色信息和权限信息，表建立好了之后，我们往表里插入一些测试数据。t_user 表：|id|username|password|role_id||:–:|:–:|:–:|:–:||1|csdn1|123456|1||2|csdn2|123456|2||3|csdn3|123456|3| t_role 表：|id|rolename||:–:|:–:||1|admin||2|teacher||3|student| t_permission 表：|id|permissionname|role_id||:–:|:–:|:–:||1|user:*|1||2|student:*|2| 解释一下这里的权限：user:*表示权限可以是 user:create 或者其他，* 处表示一个占位符，我们可以自己定义，具体的会在下文 Shiro 配置那里说明。 2.2 自定义 Realm有了数据库表和数据之后，我们开始自定义 realm，自定义 realm 需要继承 AuthorizingRealm 类，因为该类封装了很多方法，它也是一步步继承自 Realm 类的，继承了 AuthorizingRealm 类后，需要重写两个方法： doGetAuthenticationInfo() 方法：用来验证当前登录的用户，获取认证信息doGetAuthorizationInfo() 方法：用来为当前登陆成功的用户授予权限和角色 具体实现如下，相关的解释我放在代码的注释中，这样更加方便直观： /** * 自定义realm * @author shengwu ni */public class MyRealm extends AuthorizingRealm &#123; @Resource private UserService userService; @Override protected AuthorizationInfo doGetAuthorizationInfo(PrincipalCollection principalCollection) &#123; // 获取用户名 String username = (String) principalCollection.getPrimaryPrincipal(); SimpleAuthorizationInfo authorizationInfo = new SimpleAuthorizationInfo(); // 给该用户设置角色，角色信息存在t_role表中取 authorizationInfo.setRoles(userService.getRoles(username)); // 给该用户设置权限，权限信息存在t_permission表中取 authorizationInfo.setStringPermissions(userService.getPermissions(username)); return authorizationInfo; &#125; @Override protected AuthenticationInfo doGetAuthenticationInfo(AuthenticationToken authenticationToken) throws AuthenticationException &#123; // 根据token获取用户名，如果您不知道该该token怎么来的，先可以不管，下文会解释 String username = (String) authenticationToken.getPrincipal(); // 根据用户名从数据库中查询该用户 User user = userService.getByUsername(username); if(user != null) &#123; // 把当前用户存到session中 SecurityUtils.getSubject().getSession().setAttribute(\"user\", user); // 传入用户名和密码进行身份认证，并返回认证信息 AuthenticationInfo authcInfo = new SimpleAuthenticationInfo(user.getUsername(), user.getPassword(), \"myRealm\"); return authcInfo; &#125; else &#123; return null; &#125; &#125;&#125; 从上面两个方法中可以看出：验证身份的时候是根据用户输入的用户名先从数据库中查出该用户名对应的用户，这时候并没有涉及到密码，也就是说到这一步的时候，即使用户输入的密码不对，也是可以查出来该用户的，然后将该用户的正确信息封装到 authcInfo 中返回给 Shiro，接下来就是Shiro的事了，它会根据这里面的真实信息与用户前台输入的用户名和密码进行校验， 这个时候也要校验密码了，如果校验通过就让用户登录，否则跳转到指定页面。同理，权限验证的时候也是先根据用户名从数据库中获取与该用户名有关的角色和权限，然后封装到 authorizationInfo 中返回给 Shiro。 2.3 Shiro 配置自定义的 realm 写好了，接下来需要对 Shiro 进行配置了。我们主要配置三个东西：自定义 realm、安全管理器 SecurityManager 和 Shiro 过滤器。如下： 配置自定义 realm： @Configurationpublic class ShiroConfig &#123; private static final Logger logger = LoggerFactory.getLogger(ShiroConfig.class); /** * 注入自定义的realm * @return MyRealm */ @Bean public MyRealm myAuthRealm() &#123; MyRealm myRealm = new MyRealm(); logger.info(\"====myRealm注册完成=====\"); return myRealm; &#125;&#125; 配置安全管理器 SecurityManager： @Configurationpublic class ShiroConfig &#123; private static final Logger logger = LoggerFactory.getLogger(ShiroConfig.class); /** * 注入安全管理器 * @return SecurityManager */ @Bean public SecurityManager securityManager() &#123; // 将自定义realm加进来 DefaultWebSecurityManager securityManager = new DefaultWebSecurityManager(myAuthRealm()); logger.info(\"====securityManager注册完成====\"); return securityManager; &#125;&#125; 配置 SecurityManager 时，需要将上面的自定义 realm 添加进来，这样的话 Shiro 才会走到自定义的 realm 中。 配置 Shiro 过滤器： @Configurationpublic class ShiroConfig &#123; private static final Logger logger = LoggerFactory.getLogger(ShiroConfig.class); /** * 注入Shiro过滤器 * @param securityManager 安全管理器 * @return ShiroFilterFactoryBean */ @Bean public ShiroFilterFactoryBean shiroFilter(SecurityManager securityManager) &#123; // 定义shiroFactoryBean ShiroFilterFactoryBean shiroFilterFactoryBean=new ShiroFilterFactoryBean(); // 设置自定义的securityManager shiroFilterFactoryBean.setSecurityManager(securityManager); // 设置默认登录的url，身份认证失败会访问该url shiroFilterFactoryBean.setLoginUrl(\"/login\"); // 设置成功之后要跳转的链接 shiroFilterFactoryBean.setSuccessUrl(\"/success\"); // 设置未授权界面，权限认证失败会访问该url shiroFilterFactoryBean.setUnauthorizedUrl(\"/unauthorized\"); // LinkedHashMap是有序的，进行顺序拦截器配置 Map&lt;String,String&gt; filterChainMap = new LinkedHashMap&lt;&gt;(); // 配置可以匿名访问的地址，可以根据实际情况自己添加，放行一些静态资源等，anon表示放行 filterChainMap.put(\"/css/**\", \"anon\"); filterChainMap.put(\"/imgs/**\", \"anon\"); filterChainMap.put(\"/js/**\", \"anon\"); filterChainMap.put(\"/swagger-*/**\", \"anon\"); filterChainMap.put(\"/swagger-ui.html/**\", \"anon\"); // 登录url 放行 filterChainMap.put(\"/login\", \"anon\"); // “/user/admin” 开头的需要身份认证，authc表示要身份认证 filterChainMap.put(\"/user/admin*\", \"authc\"); // “/user/student” 开头的需要角色认证，是“admin”才允许 filterChainMap.put(\"/user/student*/**\", \"roles[admin]\"); // “/user/teacher” 开头的需要权限认证，是“user:create”才允许 filterChainMap.put(\"/user/teacher*/**\", \"perms[\\\"user:create\\\"]\"); // 配置logout过滤器 filterChainMap.put(\"/logout\", \"logout\"); // 设置shiroFilterFactoryBean的FilterChainDefinitionMap shiroFilterFactoryBean.setFilterChainDefinitionMap(filterChainMap); logger.info(\"====shiroFilterFactoryBean注册完成====\"); return shiroFilterFactoryBean; &#125;&#125; 配置 Shiro 过滤器时会传入一个安全管理器，可以看出，这是一环套一环，reaml -&gt; SecurityManager -&gt; filter。在过滤器中，我们需要定义一个 shiroFactoryBean，然后将 SecurityManager 添加进来，结合上面代码可以看出，要配置的东西主要有： 默认登录的 url：身份认证失败会访问该 url认证成功之后要跳转的 url权限认证失败会访问该 url需要拦截或者放行的 url：这些都放在一个 map 中 从上述代码中可以看出，在 map 中，针对不同的 url，有不同的权限要求，这里总结一下常用的几个权限。|Filter|说明||:–:|:–:||anon|开放权限，可以理解为匿名用户或游客，可以直接访问的||authc|需要身份认证的||logout|注销，执行后会直接跳转到 shiroFilterFactoryBean.setLoginUrl(); 设置的 url，即登录页面||roles[admin]|参数可写多个，表示是某个或某些角色才能通过，多个参数时写 roles[“admin，user”]，当有多个参数时必须每个参数都通过才算通过||perms[user]|参数可写多个，表示需要某个或某些权限才能通过，多个参数时写 perms[“user, admin”]，当有多个参数时必须每个参数都通过才算通过| 2.4 使用 Shiro 进行认证到这里，我们对 Shiro 的准备工作都做完了，接下来开始使用 Shiro 进行认证工作。我们首先来设计几个接口： 接口一： 使用 http://localhost:8080/user/admin 来验证身份认证接口二： 使用 http://localhost:8080/user/student 来验证角色认证接口三： 使用 http://localhost:8080/user/teacher 来验证权限认证接口四： 使用 http://localhost:8080/user/login 来实现用户登录 然后来一下认证的流程： 流程一： 直接访问接口一（此时还未登录），认证失败，跳转到 login.html 页面让用户登录，登录会请求接口四，实现用户登录功能，此时 Shiro 已经保存了用户信息了。流程二： 再次访问接口一（此时用户已经登录），认证成功，跳转到 success.html 页面，展示用户信息。流程三： 访问接口二，测试角色认证是否成功。流程四： 访问接口三，测试权限认证是否成功。 2.4.1 身份、角色、权限认证接口@Controller@RequestMapping(\"/user\")public class UserController &#123; /** * 身份认证测试接口 * @param request * @return */ @RequestMapping(\"/admin\") public String admin(HttpServletRequest request) &#123; Object user = request.getSession().getAttribute(\"user\"); return \"success\"; &#125; /** * 角色认证测试接口 * @param request * @return */ @RequestMapping(\"/student\") public String student(HttpServletRequest request) &#123; return \"success\"; &#125; /** * 权限认证测试接口 * @param request * @return */ @RequestMapping(\"/teacher\") public String teacher(HttpServletRequest request) &#123; return \"success\"; &#125;&#125; 这三个接口很简单，直接返回到指定页面展示即可，只要认证成功就会正常跳转，如果认证失败，就会跳转到上文 ShrioConfig 中配置的页面进行展示。 2.4.2 用户登录接口@Controller@RequestMapping(\"/user\")public class UserController &#123; /** * 用户登录接口 * @param user user * @param request request * @return string */ @PostMapping(\"/login\") public String login(User user, HttpServletRequest request) &#123; // 根据用户名和密码创建token UsernamePasswordToken token = new UsernamePasswordToken(user.getUsername(), user.getPassword()); // 获取subject认证主体 Subject subject = SecurityUtils.getSubject(); try&#123; // 开始认证，这一步会跳到我们自定义的realm中 subject.login(token); request.getSession().setAttribute(\"user\", user); return \"success\"; &#125;catch(Exception e)&#123; e.printStackTrace(); request.getSession().setAttribute(\"user\", user); request.setAttribute(\"error\", \"用户名或密码错误！\"); return \"login\"; &#125; &#125;&#125; 我们重点分析一下这个登录接口，首先会根据前端传过来的用户名和密码，创建一个 token，然后使用 SecurityUtils 来创建一个认证主体，接下来开始调用 subject.login(token) 开始进行身份认证了，注意这里传了刚刚创建的 token，就如注释中所述，这一步会跳转到我们自定义的 realm 中，进入 doGetAuthenticationInfo 方法，所以到这里，您就会明白该方法中那个参数 token 了。然后就是上文分析的那样，开始进行身份认证。 2.4.3 测试一下最后，启动项目，测试一下：浏览器请求 http://localhost:8080/user/admin 会进行身份认证，因为此时未登录，所以会跳转到 IndexController 中的 /login 接口，然后跳转到 login.html 页面让我们登录，使用用户名密码为 csdn/123456 登录之后，我们在浏览器中请求 http://localhost:8080/user/student 接口，会进行角色认证，因为数据库中 csdn1 的用户角色是 admin，所以和配置中的吻合，认证通过；我们再请求 http://localhost:8080/user/teacher 接口，会进行权限认证，因为数据库中 csdn1 的用户权限为 user:*，满足配置中的 user:create，所以认证通过。 接下来，我们点退出，系统会注销重新让我们登录，我们使用 csdn2 这个用户来登录，重复上述操作，当在进行角色认证和权限认证这两步时，就认证不通过了，因为数据库中 csdn2 这个用户存的角色和权限与配置中的不同，所以认证不通过。 3. 总结本节主要介绍了 Shiro 安全框架与 Spring Boot 的整合。先介绍了 Shiro 的三大核心组件已经它们的作用；然后介绍了 Shiro 的身份认证、角色认证和权限认证；最后结合代码，详细介绍了 Spring Boot 中是如何整合 Shiro 的，并设计了一套测试流程，逐步分析 Shiro 的工作流程和原理，让读者更直观地体会出 Shiro 的整套工作流程。Shiro 使用的很广泛，希望读者将其掌握，并能运用到实际项目中。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第17课：Spring Boot中集成Lucence1. Lucence 和全文检索Lucene 是什么？看一下百度百科： Lucene是一套用于全文检索和搜寻的开源程式库，由 Apache 软件基金会支持和提供。Lucene 提供了一个简单却强大的应用程式接口，能够做全文索引和搜寻。在 Java 开发环境里 Lucene 是一个成熟的免费开源工具。就其本身而言，Lucene 是当前以及最近几年最受欢迎的免费 Java 信息检索程序库。——《百度百科》 1.1 全文检索这里提到了全文检索的概念，我们先来分析一下什么是全文检索，理解了全文检索之后，再理解 Lucene 的原理就非常简单了。 何为全文检索？举个例子，比如现在要在一个文件中查找某个字符串，最直接的想法就是从头开始检索，查到了就OK，这种对于小数据量的文件来说，很实用，但是对于大数据量的文件来说，就有点吃力了。或者说找包含某个字符串的文件，也是这样，如果在一个拥有几十个 G 的硬盘中找那效率可想而知，是很低的。 文件中的数据是属于非结构化数据，也就是说它没有什么结构可言，要解决上面提到的效率问题，首先我们得将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对这些有一定结构的数据进行搜索，从而达到搜索相对较快的目的。这就叫全文搜索。即先建立索引，再对索引进行搜索的过程。 1.2 Lucene 建立索引的方式那么 Lucene 中是如何建立索引的呢？假设现在有两篇文章，内容如下： 文章1的内容为：Tom lives in Guangzhou, I live in Guangzhou too.文章2的内容为：He once lived in Shanghai. 首先第一步是将文档传给分词组件（Tokenizer），分词组件会将文档分成一个个单词，并去除标点符号和停词。所谓的停词指的是没有特别意义的词，比如英文中的 a，the，too 等。经过分词后，得到词元（Token） 。如下： 文章1经过分词后的结果：[Tom] [lives] [Guangzhou] [I] [live] [Guangzhou]文章2经过分词后的结果：[He] [lives] [Shanghai] 然后将词元传给语言处理组件（Linguistic Processor），对于英语，语言处理组件一般会将字母变为小写，将单词缩减为词根形式，如 ”lives” 到 ”live” 等，将单词转变为词根形式，如 ”drove” 到 ”drive” 等。然后得到词（Term）。如下： 文章1经过处理后的结果：[tom] [live] [guangzhou] [i] [live] [guangzhou]文章2经过处理后的结果：[he] [live] [shanghai] 最后将得到的词传给索引组件（Indexer），索引组件经过处理，得到下面的索引结构： 关键词 文章号[出现频率] 出现位置 guangzhou 1[2] 3,6 he 2[1] 1 i 1[1] 4 live 1[2],2[1] 2,5,2 shanghai 2[1] 3 tom 1[1] 1 以上就是Lucene 索引结构中最核心的部分。它的关键字是按字符顺序排列的，因此 Lucene 可以用二元搜索算法快速定位关键词。实现时 Lucene 将上面三列分别作为词典文件（Term Dictionary）、频率文件（frequencies）和位置文件（positions）保存。其中词典文件不仅保存有每个关键词，还保留了指向频率文件和位置文件的指针，通过指针可以找到该关键字的频率信息和位置信息。搜索的过程是先对词典二元查找、找到该词，通过指向频率文件的指针读出所有文章号，然后返回结果，然后就可以在具体的文章中根据出现位置找到该词了。所以 Lucene 在第一次建立索引的时候可能会比较慢，但是以后就不需要每次都建立索引了，就快了。 理解了 Lucene 的分词原理，接下来我们在 Spring Boot 中集成 Lucene 并实现索引和搜索的功能。 2. Spring Boot 中集成 Lucence2.1 依赖导入首先需要导入 Lucene 的依赖，它的依赖有好几个，如下： &lt;!-- Lucence核心包 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Lucene查询解析包 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 常规的分词（英文） --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!--支持分词高亮 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-highlighter&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt;&lt;/dependency&gt;&lt;!--支持中文分词 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-smartcn&lt;/artifactId&gt; &lt;version&gt;5.3.1&lt;/version&gt;&lt;/dependency&gt; 最后一个依赖是用来支持中文分词的，因为默认是支持英文的。那个高亮的分词依赖是最后我要做一个搜索，然后将搜到的内容高亮显示，模拟当前互联网上的做法，大家可以运用到实际项目中去。 2.2 快速入门根据上文的分析，全文检索有两个步骤，先建立索引，再检索。所以为了测试这个过程，我新建两个 java 类，一个用来建立索引的，另一个用来检索。 2.2.1 建立索引我们自己弄几个文件，放到 D:\\lucene\\data 目录下，新建一个 Indexer 类来实现建立索引功能。首先在构造方法中初始化标准分词器和写索引实例。 public class Indexer &#123; /** * 写索引实例 */ private IndexWriter writer; /** * 构造方法，实例化IndexWriter * @param indexDir * @throws Exception */ public Indexer(String indexDir) throws Exception &#123; Directory dir = FSDirectory.open(Paths.get(indexDir)); //标准分词器，会自动去掉空格啊，is a the等单词 Analyzer analyzer = new StandardAnalyzer(); //将标准分词器配到写索引的配置中 IndexWriterConfig config = new IndexWriterConfig(analyzer); //实例化写索引对象 writer = new IndexWriter(dir, config); &#125;&#125; 在构造放发中传一个存放索引的文件夹路径，然后构建标准分词器（这是英文的），再使用标准分词器来实例化写索引对象。接下来就开始建立索引了，我将解释放到代码注释里，方便大家跟进。 /** * 索引指定目录下的所有文件 * @param dataDir * @return * @throws Exception */public int indexAll(String dataDir) throws Exception &#123; // 获取该路径下的所有文件 File[] files = new File(dataDir).listFiles(); if (null != files) &#123; for (File file : files) &#123; //调用下面的indexFile方法，对每个文件进行索引 indexFile(file); &#125; &#125; //返回索引的文件数 return writer.numDocs();&#125;/** * 索引指定的文件 * @param file * @throws Exception */private void indexFile(File file) throws Exception &#123; System.out.println(\"索引文件的路径：\" + file.getCanonicalPath()); //调用下面的getDocument方法，获取该文件的document Document doc = getDocument(file); //将doc添加到索引中 writer.addDocument(doc);&#125;/** * 获取文档，文档里再设置每个字段，就类似于数据库中的一行记录 * @param file * @return * @throws Exception */private Document getDocument(File file) throws Exception &#123; Document doc = new Document(); //开始添加字段 //添加内容 doc.add(new TextField(\"contents\", new FileReader(file))); //添加文件名，并把这个字段存到索引文件里 doc.add(new TextField(\"fileName\", file.getName(), Field.Store.YES)); //添加文件路径 doc.add(new TextField(\"fullPath\", file.getCanonicalPath(), Field.Store.YES)); return doc;&#125; 这样就建立好索引了，我们在该类中写一个 main 方法测试一下： public static void main(String[] args) &#123; //索引保存到的路径 String indexDir = \"D:\\\\lucene\"; //需要索引的文件数据存放的目录 String dataDir = \"D:\\\\lucene\\\\data\"; Indexer indexer = null; int indexedNum = 0; //记录索引开始时间 long startTime = System.currentTimeMillis(); try &#123; // 开始构建索引 indexer = new Indexer(indexDir); indexedNum = indexer.indexAll(dataDir); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if (null != indexer) &#123; indexer.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; //记录索引结束时间 long endTime = System.currentTimeMillis(); System.out.println(\"索引耗时\" + (endTime - startTime) + \"毫秒\"); System.out.println(\"共索引了\" + indexedNum + \"个文件\"); &#125; 我搞了两个 tomcat 相关的文件放到 D:\\lucene\\data 下了，执行完之后，看到控制台输出： 索引文件的路径：D:\\lucene\\data\\catalina.properties索引文件的路径：D:\\lucene\\data\\logging.properties索引耗时882毫秒共索引了2个文件 然后我们去 D:\\lucene\\ 目录下可以看到一些索引文件，这些文件不能删除，删除了就需要重新构建索引，否则没了索引，就无法去检索内容了。 ####2.2.2 检索内容 上面把这两个文件的索引建立好了，接下来我们就可以写检索程序了，在这两个文件中查找特定的词。 public class Searcher &#123; public static void search(String indexDir, String q) throws Exception &#123; //获取要查询的路径，也就是索引所在的位置 Directory dir = FSDirectory.open(Paths.get(indexDir)); IndexReader reader = DirectoryReader.open(dir); //构建IndexSearcher IndexSearcher searcher = new IndexSearcher(reader); //标准分词器，会自动去掉空格啊，is a the等单词 Analyzer analyzer = new StandardAnalyzer(); //查询解析器 QueryParser parser = new QueryParser(\"contents\", analyzer); //通过解析要查询的String，获取查询对象，q为传进来的待查的字符串 Query query = parser.parse(q); //记录索引开始时间 long startTime = System.currentTimeMillis(); //开始查询，查询前10条数据，将记录保存在docs中 TopDocs docs = searcher.search(query, 10); //记录索引结束时间 long endTime = System.currentTimeMillis(); System.out.println(\"匹配\" + q + \"共耗时\" + (endTime-startTime) + \"毫秒\"); System.out.println(\"查询到\" + docs.totalHits + \"条记录\"); //取出每条查询结果 for(ScoreDoc scoreDoc : docs.scoreDocs) &#123; //scoreDoc.doc相当于docID,根据这个docID来获取文档 Document doc = searcher.doc(scoreDoc.doc); //fullPath是刚刚建立索引的时候我们定义的一个字段，表示路径。也可以取其他的内容，只要我们在建立索引时有定义即可。 System.out.println(doc.get(\"fullPath\")); &#125; reader.close(); &#125;&#125; ok，这样我们检索的代码就写完了，每一步解释我写在代码中的注释上了，下面写个 main 方法来测试一下： public static void main(String[] args) &#123; String indexDir = \"D:\\\\lucene\"; //查询这个字符串 String q = \"security\"; try &#123; search(indexDir, q); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 查一下 security 这个字符串，执行一下看控制台打印的结果： 匹配security共耗时23毫秒查询到1条记录D:\\lucene\\data\\catalina.properties 可以看出，耗时了23毫秒在两个文件中找到了 security 这个字符串，并输出了文件的名称。上面的代码我写的很详细，这个代码已经比较全了，可以用在生产环境上。 2.3 中文分词检索高亮实战上文已经写了建立索引和检索的代码，但是在实际项目中，我们往往是结合页面做一些查询结果的展示，比如我要查某个关键字，查到了之后，将相关的信息点展示出来，并将查询的关键字高亮等等。这种需求在实际项目中非常常见，而且大多数网站中都会有这种效果。所以这一小节我们就使用 Lucene 来实现这种效果。 2.3.1 中文分词我们新建一个 ChineseIndexer 类来建立中文索引，建立过程和英文索引一样的，不同的地方在于使用的是中文分词器。除此之外，这里我们不用通过读取文件去建立索引，我们模拟一下用字符串来建立，因为在实际项目中，绝大部分情况是获取到一些文本字符串，然后根据一些关键字去查询相关内容等等。代码如下： public class ChineseIndexer &#123; /** * 存放索引的位置 */ private Directory dir; //准备一下用来测试的数据 //用来标识文档 private Integer ids[] = &#123;1, 2, 3&#125;; private String citys[] = &#123;\"上海\", \"南京\", \"青岛\"&#125;; private String descs[] = &#123; \"上海是个繁华的城市。\", \"南京是一个文化的城市南京，简称宁，是江苏省会，地处中国东部地区，长江下游，濒江近海。全市下辖11个区，总面积6597平方公里，2013年建成区面积752.83平方公里，常住人口818.78万，其中城镇人口659.1万人。[1-4] “江南佳丽地，金陵帝王州”，南京拥有着6000多年文明史、近2600年建城史和近500年的建都史，是中国四大古都之一，有“六朝古都”、“十朝都会”之称，是中华文明的重要发祥地，历史上曾数次庇佑华夏之正朔，长期是中国南方的政治、经济、文化中心，拥有厚重的文化底蕴和丰富的历史遗存。[5-7] 南京是国家重要的科教中心，自古以来就是一座崇文重教的城市，有“天下文枢”、“东南第一学”的美誉。截至2013年，南京有高等院校75所，其中211高校8所，仅次于北京上海；国家重点实验室25所、国家重点学科169个、两院院士83人，均居中国第三。[8-10] 。\", \"青岛是一个美丽的城市。\" &#125;; /** * 生成索引 * @param indexDir * @throws Exception */ public void index(String indexDir) throws Exception &#123; dir = FSDirectory.open(Paths.get(indexDir)); // 先调用 getWriter 获取IndexWriter对象 IndexWriter writer = getWriter(); for(int i = 0; i &lt; ids.length; i++) &#123; Document doc = new Document(); // 把上面的数据都生成索引，分别用id、city和desc来标识 doc.add(new IntField(\"id\", ids[i], Field.Store.YES)); doc.add(new StringField(\"city\", citys[i], Field.Store.YES)); doc.add(new TextField(\"desc\", descs[i], Field.Store.YES)); //添加文档 writer.addDocument(doc); &#125; //close了才真正写到文档中 writer.close(); &#125; /** * 获取IndexWriter实例 * @return * @throws Exception */ private IndexWriter getWriter() throws Exception &#123; //使用中文分词器 SmartChineseAnalyzer analyzer = new SmartChineseAnalyzer(); //将中文分词器配到写索引的配置中 IndexWriterConfig config = new IndexWriterConfig(analyzer); //实例化写索引对象 IndexWriter writer = new IndexWriter(dir, config); return writer; &#125; public static void main(String[] args) throws Exception &#123; new ChineseIndexer().index(\"D:\\\\lucene2\"); &#125;&#125; 这里我们用 id、city、desc 分别代表 id、城市名称和城市描述，用他们作为关键字来建立索引，后面我们获取内容的时候，主要来获取城市描述。南京的描述我故意写的长一点，因为下文检索的时候，根据不同的关键字会检索到不同部分的信息，有个权重的概念在里面。然后执行一下 main 方法，将索引保存到 D:\\lucene2\\ 中。 2.3.2 中文分词查询中文分词查询代码逻辑和默认的查询差不多，有一些区别在于，我们需要将查询出来的关键字标红加粗等需要处理，需要计算出一个得分片段，这是什么意思呢？比如我搜索 “南京文化” 跟搜索 “南京文明”，这两个搜索结果应该根据关键字出现的位置，返回的结果不一样才对，这在下文会测试。我们先看一下代码和注释： public class ChineseSearch &#123; private static final Logger logger = LoggerFactory.getLogger(ChineseSearch.class); public static List&lt;String&gt; search(String indexDir, String q) throws Exception &#123; //获取要查询的路径，也就是索引所在的位置 Directory dir = FSDirectory.open(Paths.get(indexDir)); IndexReader reader = DirectoryReader.open(dir); IndexSearcher searcher = new IndexSearcher(reader); //使用中文分词器 SmartChineseAnalyzer analyzer = new SmartChineseAnalyzer(); //由中文分词器初始化查询解析器 QueryParser parser = new QueryParser(\"desc\", analyzer); //通过解析要查询的String，获取查询对象 Query query = parser.parse(q); //记录索引开始时间 long startTime = System.currentTimeMillis(); //开始查询，查询前10条数据，将记录保存在docs中 TopDocs docs = searcher.search(query, 10); //记录索引结束时间 long endTime = System.currentTimeMillis(); logger.info(\"匹配&#123;&#125;共耗时&#123;&#125;毫秒\", q, (endTime - startTime)); logger.info(\"查询到&#123;&#125;条记录\", docs.totalHits); //如果不指定参数的话，默认是加粗，即&lt;b&gt;&lt;b/&gt; SimpleHTMLFormatter simpleHTMLFormatter = new SimpleHTMLFormatter(\"&lt;b&gt;&lt;font color=red&gt;\",\"&lt;/font&gt;&lt;/b&gt;\"); //根据查询对象计算得分，会初始化一个查询结果最高的得分 QueryScorer scorer = new QueryScorer(query); //根据这个得分计算出一个片段 Fragmenter fragmenter = new SimpleSpanFragmenter(scorer); //将这个片段中的关键字用上面初始化好的高亮格式高亮 Highlighter highlighter = new Highlighter(simpleHTMLFormatter, scorer); //设置一下要显示的片段 highlighter.setTextFragmenter(fragmenter); //取出每条查询结果 List&lt;String&gt; list = new ArrayList&lt;&gt;(); for(ScoreDoc scoreDoc : docs.scoreDocs) &#123; //scoreDoc.doc相当于docID,根据这个docID来获取文档 Document doc = searcher.doc(scoreDoc.doc); logger.info(\"city:&#123;&#125;\", doc.get(\"city\")); logger.info(\"desc:&#123;&#125;\", doc.get(\"desc\")); String desc = doc.get(\"desc\"); //显示高亮 if(desc != null) &#123; TokenStream tokenStream = analyzer.tokenStream(\"desc\", new StringReader(desc)); String summary = highlighter.getBestFragment(tokenStream, desc); logger.info(\"高亮后的desc:&#123;&#125;\", summary); list.add(summary); &#125; &#125; reader.close(); return list; &#125;&#125; 每一步的注释我写的很详细，在这就不赘述了。接下来我们来测试一下效果。 2.3.3 测试一下这里我们使用 thymeleaf 来写个简单的页面来展示获取到的数据，并高亮展示。在 controller 中我们指定索引的目录和需要查询的字符串，如下： @Controller@RequestMapping(\"/lucene\")public class IndexController &#123; @GetMapping(\"/test\") public String test(Model model) &#123; // 索引所在的目录 String indexDir = \"D:\\\\lucene2\"; // 要查询的字符// String q = \"南京文明\"; String q = \"南京文化\"; try &#123; List&lt;String&gt; list = ChineseSearch.search(indexDir, q); model.addAttribute(\"list\", list); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return \"result\"; &#125;&#125; 直接返回到 result.html 页面，该页面主要来展示一下 model 中的数据即可。 &lt;!DOCTYPE html&gt;&lt;html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div th:each=\"desc : $&#123;list&#125;\"&gt; &lt;div th:utext=\"$&#123;desc&#125;\"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 这里注意一下，不能使用 th:test，否则字符串中的 html 标签都会被转义，不会被渲染到页面。下面启动服务，在浏览器中输入 http://localhost:8080/lucene/test，测试一下效果，我们搜索的是 “南京文化”。 再将 controller 中的搜索关键字改成 “南京文明”，看下命中的效果。 可以看出，不同的关键词，它会计算一个得分片段，也就是说不同的关键字会命中不同位置的内容，然后将关键字根据我们自己设定的形式高亮显示。从结果中可以看出，Lucene 也可以很智能的将关键字拆分命中，这在实际项目中会很好用。 3. 总结本节课首先详细的分析了全文检索的理论规则，然后结合 Lucene，系统的讲述了在 Spring Boot 的集成步骤，首先快速带领大家从直观上感受 Lucene 如何建立索引已经如果检索，其次通过中文检索的具体实例，展示了 Lucene 在全文检索中的广泛应用。Lucene 不难，主要就是步骤比较多，代码不用死记硬背，拿到项目中根据实际情况做对应的修改即可。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 第18课：Spring Boot搭建实际项目开发中的架构前面的课程中，我主要给大家讲解了 Spring Boot 中常用的一些技术点，这些技术点在实际项目中可能不会全部用得到，因为不同的项目可能使用的技术不同，但是希望大家都能掌握如何使用，并能自己根据实际项目中的需求进行相应的扩展。 不知道大家了不了解单片机，单片机里有个最小系统，这个最小系统搭建好了之后，就可以在此基础上进行人为的扩展。这节课我们要做的就是搭建一个 “Spring Boot 最小系统架构” 。拿着这个架构，可以在此基础上根据实际需求做相应的扩展。 从零开始搭建一个环境，主要要考虑几点：统一封装的数据结构、可调式的接口、json的处理、模板引擎的使用（本文不写该项，因为现在大部分项目都前后端分离了，但是考虑到也还有非前后端分离的项目，所以我在源代码里也加上了 thymeleaf）、持久层的集成、拦截器（这个也是可选的）和全局异常处理。一般包括这些东西的话，基本上一个 Spring Boot 项目环境就差不多了，然后就是根据具体情况来扩展了。 结合前面的课程和以上的这些点，本节课手把手带领大家搭建一个实际项目开发中可用的 Spring Boot 架构。整个项目工程如下图所示，学习的时候，可以结合我的源码，这样效果会更好。 1. 统一的数据封装由于封装的 json 数据的类型不确定，所以在定义统一的 json 结构时，我们需要用到泛型。统一的 json 结构中属性包括数据、状态码、提示信息即可，构造方法可以根据实际业务需求做相应的添加即可，一般来说，应该有默认的返回结构，也应该有用户指定的返回结构。如下： /** * 统一返回对象 * @author shengwu ni * @param &lt;T&gt; */public class JsonResult&lt;T&gt; &#123; private T data; private String code; private String msg; /** * 若没有数据返回，默认状态码为0，提示信息为：操作成功！ */ public JsonResult() &#123; this.code = \"0\"; this.msg = \"操作成功！\"; &#125; /** * 若没有数据返回，可以人为指定状态码和提示信息 * @param code * @param msg */ public JsonResult(String code, String msg) &#123; this.code = code; this.msg = msg; &#125; /** * 有数据返回时，状态码为0，默认提示信息为：操作成功！ * @param data */ public JsonResult(T data) &#123; this.data = data; this.code = \"0\"; this.msg = \"操作成功！\"; &#125; /** * 有数据返回，状态码为0，人为指定提示信息 * @param data * @param msg */ public JsonResult(T data, String msg) &#123; this.data = data; this.code = \"0\"; this.msg = msg; &#125; /** * 使用自定义异常作为参数传递状态码和提示信息 * @param msgEnum */ public JsonResult(BusinessMsgEnum msgEnum) &#123; this.code = msgEnum.code(); this.msg = msgEnum.msg(); &#125; // 省去get和set方法&#125; 大家可以根据自己项目中所需要的一些东西，合理的修改统一结构中的字段信息。 2. json的处理Json 处理工具很多，比如阿里巴巴的 fastjson，不过 fastjson 对有些未知类型的 null 无法转成空字符串，这可能是 fastjson 自身的缺陷，可扩展性也不是太好，但是使用起来方便，使用的人也蛮多的。这节课里面我们主要集成 Spring Boot 自带的 jackson。主要是对 jackson 做一下对 null 的配置即可，然后就可以在项目中使用了。 /** * jacksonConfig * @author shengwu ni */@Configurationpublic class JacksonConfig &#123; @Bean @Primary @ConditionalOnMissingBean(ObjectMapper.class) public ObjectMapper jacksonObjectMapper(Jackson2ObjectMapperBuilder builder) &#123; ObjectMapper objectMapper = builder.createXmlMapper(false).build(); objectMapper.getSerializerProvider().setNullValueSerializer(new JsonSerializer&lt;Object&gt;() &#123; @Override public void serialize(Object o, JsonGenerator jsonGenerator, SerializerProvider serializerProvider) throws IOException &#123; jsonGenerator.writeString(\"\"); &#125; &#125;); return objectMapper; &#125;&#125; 这里先不测试，等下面 swagger2 配置好了之后，我们一起来测试一下。 3. swagger2在线可调式接口有了 swagger，开发人员不需要给其他人员提供接口文档，只要告诉他们一个 Swagger 地址，即可展示在线的 API 接口文档，除此之外，调用接口的人员还可以在线测试接口数据，同样地，开发人员在开发接口时，同样也可以利用 Swagger 在线接口文档测试接口数据，这给开发人员提供了便利。使用 swagger 需要对其进行配置： /** * swagger配置 * @author shengwu ni */@Configuration@EnableSwagger2public class SwaggerConfig &#123; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType.SWAGGER_2) // 指定构建api文档的详细信息的方法：apiInfo() .apiInfo(apiInfo()) .select() // 指定要生成api接口的包路径，这里把controller作为包路径，生成controller中的所有接口 .apis(RequestHandlerSelectors.basePackage(\"com.itcodai.course18.controller\")) .paths(PathSelectors.any()) .build(); &#125; /** * 构建api文档的详细信息 * @return */ private ApiInfo apiInfo() &#123; return new ApiInfoBuilder() // 设置页面标题 .title(\"Spring Boot搭建实际项目中开发的架构\") // 设置接口描述 .description(\"跟武哥一起学Spring Boot第18课\") // 设置联系方式 .contact(\"倪升武，\" + \"微信公众号：程序员私房菜\") // 设置版本 .version(\"1.0\") // 构建 .build(); &#125;&#125; 到这里，可以先测试一下，写一个 Controller，弄一个静态的接口测试一下上面集成的内容。 @RestController@Api(value = \"用户信息接口\")public class UserController &#123; @Resource private UserService userService; @GetMapping(\"/getUser/&#123;id&#125;\") @ApiOperation(value = \"根据用户唯一标识获取用户信息\") public JsonResult&lt;User&gt; getUserInfo(@PathVariable @ApiParam(value = \"用户唯一标识\") Long id) &#123; User user = new User(id, \"倪升武\", \"123456\"); return new JsonResult&lt;&gt;(user); &#125;&#125; 然后启动项目，在浏览器中输入 localhost:8080/swagger-ui.html 即可看到 swagger 接口文档页面，调用一下上面这个接口，即可看到返回的 json 数据。 4. 持久层集成每个项目中是必须要有持久层的，与数据库交互，这里我们主要来集成 mybatis，集成 mybatis 首先要在 application.yml 中进行配置。 # 服务端口号server: port: 8080# 数据库地址datasource: url: localhost:3306/blog_testspring: datasource: # 数据库配置 driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://$&#123;datasource.url&#125;?useSSL=false&amp;useUnicode=true&amp;characterEncoding=utf-8&amp;allowMultiQueries=true&amp;autoReconnect=true&amp;failOverReadOnly=false&amp;maxReconnects=10 username: root password: 123456 hikari: maximum-pool-size: 10 # 最大连接池数 max-lifetime: 1770000mybatis: # 指定别名设置的包为所有entity type-aliases-package: com.itcodai.course18.entity configuration: map-underscore-to-camel-case: true # 驼峰命名规范 mapper-locations: # mapper映射文件位置 - classpath:mapper/*.xml 配置好了之后，接下来我们来写一下 dao 层，实际中我们使用注解比较多，因为比较方便，当然也可以使用 xml 的方式，甚至两种同时使用都行，这里我们主要使用注解的方式来集成，关于 xml 的方式，大家可以查看前面课程，实际中根据项目情况来定。 public interface UserMapper &#123; @Select(\"select * from user where id = #&#123;id&#125;\") @Results(&#123; @Result(property = \"username\", column = \"user_name\"), @Result(property = \"password\", column = \"password\") &#125;) User getUser(Long id); @Select(\"select * from user where id = #&#123;id&#125; and user_name=#&#123;name&#125;\") User getUserByIdAndName(@Param(\"id\") Long id, @Param(\"name\") String username); @Select(\"select * from user\") List&lt;User&gt; getAll();&#125; 关于 service 层我就不在文章中写代码了，大家可以结合我的源代码学习，这一节主要带领大家来搭建一个 Spring Boot 空架构。最后别忘了在启动类上添加注解扫描 @MapperScan(&quot;com.itcodai.course18.dao&quot;) 5. 拦截器拦截器在项目中使用的是非常多的（但不是绝对的），比如拦截一些置顶的 url，做一些判断和处理等等。除此之外，还需要将常用的静态页面或者 swagger 页面放行，不能将这些静态资源给拦截了。首先先自定义一个拦截器。 public class MyInterceptor implements HandlerInterceptor &#123; private static final Logger logger = LoggerFactory.getLogger(MyInterceptor.class); @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; logger.info(\"执行方法之前执行(Controller方法调用之前)\"); return true; &#125; @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; logger.info(\"执行完方法之后进执行(Controller方法调用之后)，但是此时还没进行视图渲染\"); &#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; logger.info(\"整个请求都处理完咯，DispatcherServlet也渲染了对应的视图咯，此时我可以做一些清理的工作了\"); &#125;&#125; 然后将自定义的拦截器加入到拦截器配置中。 @Configurationpublic class MyInterceptorConfig implements WebMvcConfigurer &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 实现WebMvcConfigurer不会导致静态资源被拦截 registry.addInterceptor(new MyInterceptor()) // 拦截所有url .addPathPatterns(\"/**\") // 放行swagger .excludePathPatterns(\"/swagger-resources/**\"); &#125;&#125; 在 Spring Boot 中，我们通常会在如下目录里存放一些静态资源： classpath:/staticclasspath:/publicclasspath:/resourcesclasspath:/META-INF/resources 上面代码中配置的 /** 是对所有 url 都进行了拦截，但我们实现了 WebMvcConfigurer 接口，不会导致 Spring Boot 对上面这些目录下的静态资源实施拦截。但是我们平时访问的 swagger 会被拦截，所以要将其放行。swagger 页面在 swagger-resources 目录下，放行该目录下所有文件即可。 然后在浏览器中输入一下 swagger 页面，若能正常显示 swagger，说明放行成功。同时可以根据后台打印的日志判断代码执行的顺序。 6. 全局异常处理全局异常处理是每个项目中必须用到的东西，在具体的异常中，我们可能会做具体的处理，但是对于没有处理的异常，一般会有一个统一的全局异常处理。在异常处理之前，最好维护一个异常提示信息枚举类，专门用来保存异常提示信息的。如下： public enum BusinessMsgEnum &#123; /** 参数异常 */ PARMETER_EXCEPTION(\"102\", \"参数异常!\"), /** 等待超时 */ SERVICE_TIME_OUT(\"103\", \"服务调用超时！\"), /** 参数过大 */ PARMETER_BIG_EXCEPTION(\"102\", \"输入的图片数量不能超过50张!\"), /** 500 : 发生异常 */ UNEXPECTED_EXCEPTION(\"500\", \"系统发生异常，请联系管理员！\"); /** * 消息码 */ private String code; /** * 消息内容 */ private String msg; private BusinessMsgEnum(String code, String msg) &#123; this.code = code; this.msg = msg; &#125; public String code() &#123; return code; &#125; public String msg() &#123; return msg; &#125;&#125; 在全局统一异常处理类中，我们一般会对自定义的业务异常最先处理，然后去处理一些常见的系统异常，最后会来一个一劳永逸（Exception 异常）。 @ControllerAdvice@ResponseBodypublic class GlobalExceptionHandler &#123; private static final Logger logger = LoggerFactory.getLogger(GlobalExceptionHandler.class); /** * 拦截业务异常，返回业务异常信息 * @param ex * @return */ @ExceptionHandler(BusinessErrorException.class) @ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR) public JsonResult handleBusinessError(BusinessErrorException ex) &#123; String code = ex.getCode(); String message = ex.getMessage(); return new JsonResult(code, message); &#125; /** * 空指针异常 * @param ex NullPointerException * @return */ @ExceptionHandler(NullPointerException.class) @ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR) public JsonResult handleTypeMismatchException(NullPointerException ex) &#123; logger.error(\"空指针异常，&#123;&#125;\", ex.getMessage()); return new JsonResult(\"500\", \"空指针异常了\"); &#125; /** * 系统异常 预期以外异常 * @param ex * @return */ @ExceptionHandler(Exception.class) @ResponseStatus(value = HttpStatus.INTERNAL_SERVER_ERROR) public JsonResult handleUnexpectedServer(Exception ex) &#123; logger.error(\"系统异常：\", ex); return new JsonResult(BusinessMsgEnum.UNEXPECTED_EXCEPTION); &#125;&#125; 其中，BusinessErrorException 是自定义的业务异常，继承一下 RuntimeException 即可，具体可以看我的源代码，文章中就不贴代码了。在 UserController 中有个 testException 方法，用来测试全局异常的，打开 swagger 页面，调用一下该接口，可以看出返回用户提示信息：”系统发生异常，请联系管理员！“。当然了，实际情况中，需要根据不同的业务提示不同的信息。 7. 总结本文主要手把手带领大家快速搭建一个项目中可以使用的 Spring Boot 空架构，主要从统一封装的数据结构、可调式的接口、json的处理、模板引擎的使用（代码中体现）、持久层的集成、拦截器和全局异常处理。一般包括这些东西的话，基本上一个 Spring Boot 项目环境就差不多了，然后就是根据具体情况来扩展了。 课程源代码下载地址：戳我下载 欢迎关注我的为微信公众号：武哥聊编程 附：作者信息本课程首发于 CSDN GitChat 达人课，该文档为课程详细笔记作者：倪升武（武哥）微信公众号：武哥聊编程二维码： 版权申明：本课程已经免费，但版权属于 CSDN 和作者，未经允许，不得将该笔记用于商业用途，否则追究法律责任。","categories":[{"name":"后端框架","slug":"后端框架","permalink":"http://hofe.work/categories/%E5%90%8E%E7%AB%AF%E6%A1%86%E6%9E%B6/"}],"tags":[{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://hofe.work/tags/SpringBoot/"}]},{"title":"一文详解kmp算法","slug":"数据结构与算法/一文详解kmp算法","date":"2020-04-05T08:28:00.000Z","updated":"2020-05-01T14:59:45.724Z","comments":true,"path":"2020/04/05/数据结构与算法/一文详解kmp算法/","link":"","permalink":"http://hofe.work/2020/04/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/%E4%B8%80%E6%96%87%E8%AF%A6%E8%A7%A3kmp%E7%AE%97%E6%B3%95/","excerpt":"","text":"一、KMP算法是什么？kmp算法是用于解决字符串匹配的算法;先来看一道例题link 本文用约定用 pat 表示模式串，长度为 M，txt 表示文本串，长度为 N。KMP 算法是在 txt 中查找子串 pat，如果存在，返回这个子串的起始索引，否则返回 -1 首先来看一道例题： 题目描述 字符串旋转: 给定两字符串A和B，如果能将A从中间某个位置分割为左右两部分字符串（都不为空串），并将左边的字符串移动到右边字符串后面组成新的字符串可以变为字符串B时返回true。 例如：如果A=‘youzan’，B=‘zanyou’，A按‘you’‘zan’切割换位后得到‘zanyou’和B相同返回true。 输入描述: 2个不为空的字符串(说明:输入一个字符串以英文分号&quot;;&quot;分割为2个字符串)例如:youzan;zanyou 即为A&#x3D;‘youzan’，B&#x3D;‘zanyou’ 输出描述: 输出true或false(表示是否能按要求匹配两个字符串) 示例1 输入 youzan;zanyou 输出 true 这里先给个取巧的方法 import java.util.Scanner;public class Main&#123; public static void main(String[] args)&#123; Scanner s = new Scanner(System.in); String a = s.nextLine(); s.close(); String[] m = a.split(\";\"); if(m[0].length()!=m[1].length())&#123; System.out.print(false); return; &#125; m[0] = m[0]+m[0]; System.out.print(m[0].contains(m[1])); &#125;&#125; 二、为什么要用KMP算法？我们知道从字符串中寻找子串的问题一般可以用暴力遍历来解决，每一次只能往后移动一个位置，且遇到不匹配的字符时，指针需要回溯，时间复杂度为O(n*m) int search(String pat, String txt) &#123; int M = pat.length; int N = txt.length; for (int i = 0; i &lt; N - M; i++) &#123; int j; for (j = 0; j &lt; M; j++) &#123; if (pat[j] != txt[i+j]) break; &#125; // pat 全都匹配了 if (j == M) return i; &#125; // txt 中不存在 pat 子串 return -1;&#125; KMP算法具有两个特性： 仅仅后移模式串 指针不回溯 什么意思呢？请看以下两种情况下的kmp算法 (1)txt = “aaaaaaab” pat = “aaab” (2) txt = “aaacaaab” pat = “aaab”： 到此为止大家应该已经理解为什么要用kmp算法代替暴力遍历找子串了 三、KMP怎么解决问题？现在来看看kmp如何实现 首先我们可以知道显著的区别是遇到失配的情况主串不必再回退到当前的下一个字符开始匹配，而是保持不变，不进行回溯。由子串进行回溯重新匹配，而且回退之后，回退点之前的元素需要和主串匹配才行，这样主串才不用回退。那关键就在于子串回退的位置，它该回退多少的问题。 要保证一个模式串进行移动j位之后，回退点之前的元素仍然和主串匹配，说明模式串的(真)前后缀有一段是相同的。 步骤1：解决回退几位的问题这里引入next[j]表示失配点j，对于字符串aaab它有以下几种情况(这里表示的都是真前后缀) next[0]表示失配点在pat[0]=a这个位置，也就是它前面的元素为””，没有前缀与后缀，令next[0] = -1； next[1]表示失配点在pat[1]=a这个位置，也就是它前面的元素为a，没有前缀与后缀，故next[0] = 0； next[2]表示失配点在pat[2]=a这个位置，也就是它前面的元素为aa，有前缀与后缀a，故next[1] = 1； next[3]表示失配点在pat[3]=b这个位置，也就是它前面的元素为aaa，有前缀与后缀aa，故next[2] = 2； 那就可以得到这串子串的部分匹配表 i 0 1 2 3 pat[] a a a b next[] -1 0 1 2 那现在用这个匹配表验证前面两种主串 (1)txt = “aaaaaaab” pat = “aaab” 失配点 j = 3， pat[j] = b， next[j] = 2；也就是说j回退到2的位置，往回走1位， 即pat[2] = a (2)txt = “aaacaaab” pat = “aaab”： 图片是经过优化算法得出的步骤，因为知道子串中未出现过c，所以可以直接回退到起点，但算法优化前是按照下面流程走的： 第一次: 失配点 j = 3， pat[j] = b， next[j] = 2；j回退到2的位置， 即pat[2] = a 第二次: 失配点 j = 2， pat[j] = a， next[j] = 1；j回退到1的位置， 即pat[1] = a 第三次: 失配点 j = 1， pat[j] = a， next[j] = 0；j回退到next[j]的位置， 即pat[0] = a 第四次: 失配点 j = 0， pat[j] = a， next[j] = -1; j回退到next[j]的位置， 即pat[-1] = “” 代码如下： public static int kmp(char[] txt, char[] pat)&#123; int[] next = getNext(pat); int i = 0, j = 0; while (i &lt; txt.length &amp;&amp; j &lt; pat.length) &#123; if (txt[i] == pat[j]) &#123; i++; j++; &#125; else if (next[j] != -1) &#123; j = next[j]; &#125; else &#123; // next[j] = -1说明这时已经在头部位置之前了 i++; &#125; &#125; return j == pat.length ? i-j : -1; &#125; 步骤2：解决求next数组的问题使用双指针遍历该位置前的串中前后缀相同的值，可令next[0] = -1，代表无匹配，回退到该字符串前一位置； 令next[1] = 0，真前后缀是不包含自身的。 图片链接：https://upload-images.jianshu.io/upload_images/11023579-8e5da803cfa21d91.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp 代码如下: public class static int[] getNext(char[] pat)&#123; int[] next = new int[arr.length]; next[0] = -1; next[1] = 0; int i = 2; int j = 0; while(i &lt; pat.length)&#123; if(pat[i-1] == pat[j])&#123; next[i] = j + 1; i++; j++; &#125;else if(j &gt; 0)&#123; // 首尾匹配，但次前缀和次后缀不匹配； // 那该位置的值就等于子串次前缀位置的值 j = next[j]; &#125;else&#123; // 首尾不匹配 next[i] = 0; i++; &#125; &#125; return next; &#125; 完整代码： import java.util.*;public class Main&#123; public static int kmp(char[] txt, char[] pat)&#123; int[] next = getNext(pat); int i = 0, j = 0; while (i &lt; txt.length &amp;&amp; j &lt; pat.length) &#123; if (txt[i] == pat[j]) &#123; i++; j++; &#125; else if (next[j] != -1) &#123; j = next[j]; &#125; else &#123; // next[j] = -1说明这时已经在头部位置之前了 i++; &#125; &#125; return j == pat.length ? i-j : -1; &#125; public static int[] getNext(char[] pat)&#123; int[] next = new int[pat.length]; next[0] = -1; next[1] = 0; int i = 2; int j = 0; while(i &lt; pat.length)&#123; if(pat[i-1] == pat[j])&#123; next[i] = j + 1; i++; j++; &#125;else if(j &gt; 0)&#123; // 首尾匹配，但次前缀和次后缀不匹配； // 那该位置的值就等于子串次前缀位置的值 j = next[j]; &#125;else&#123; // 首尾不匹配 next[i] = 0; i++; &#125; &#125; return next; &#125; public static void main(String[] args)&#123; Scanner sc = new Scanner(System.in); String str = sc.nextLine(); String s1 = str.split(\";\")[0]; String s2 = str.split(\";\")[1]; if (s1.length() != s2.length()) &#123; System.out.println(\"false\"); return; &#125; // 转化为判断txt是否包含sub String txt = s1 + s1; String sub = s2; System.out.println( kmp(txt.toCharArray(), sub.toCharArray())==-1?false:true); &#125;&#125;","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"kmp","slug":"kmp","permalink":"http://hofe.work/tags/kmp/"}]},{"title":"Git基本使用命令","slug":"工具/Git基本使用命令","date":"2020-02-17T16:00:00.000Z","updated":"2020-05-17T10:56:16.841Z","comments":true,"path":"2020/02/18/工具/Git基本使用命令/","link":"","permalink":"http://hofe.work/2020/02/18/%E5%B7%A5%E5%85%B7/Git%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"基本操作git init #第一步 git branch #查看分支，init之后add\\commit之后才会出现git branch dev #创建分支git checkout dev #切换分支git checkout -b dev #快速创建并切换分支 #查看文件状态git status #增加git add filename.txt#提交git commit -m \"branch dev add filename.txt\"#删除git rm filename.txtgit commit filename .txt -m \"delete filename\" git log #查看日志git log --oneline -2 #简洁日志显示最近两条git log --oneline --graph #图形化显示分支走向 git reset --hard HEAD~2 #回退到上两个版本git reset --hard 4e763k #回退到上版本4e763k 撤销工作区撤销git checkout--he1109.txt #可以撒销到最近一次 git add或 git commit的状态#即：如果暂存区有此文件，则将暂存区中的文件内容恢复到工作区。#如果暂存区没有此文件，则将分支中的文件内容恢复到工作区。 暂存区撤销(已经commit)#如果在工作区中修改了文件并发送到暂存区中，但文件中有需要撤销的内容则可以执行：git reset HEAD he1109.txt#将hello.txt在暂存区的内容清除然后可执行：git checkout--hel1o9.txt回退到上一个版本 关联远程仓库git remote add origin git@github.com:zanghongjiu99/repo #添加远程库远程库别名库地址git remote-V #查看关联的所有远程库git remote show origin #关联远程库后，本地分支和远程分支的对应关系git remote remove origin #删除关联git remote rename origin origin2#重命名 push操作#本地的 master分支上传到与之有跟踪关系的远程分支中，（克隆时就会建立跟踪关系），如果远程分支不存在，则会建立远程分支git push origin master#本地存在分支dev，上传到远程库 origin的分支dev，如果没有dev，将建立远程分支dev git push origin dev:dev#本地库dev：远程库deν 本地库dev2：远程库dev2git push origin dev:dev dev2：dev2 pull=fetch+merge#git pull&lt;远程主机名&gt;&lt;远程分支名&gt;∷&lt;本地分支名&gt;#git pull origin master:mastergit pull origin master #省略本地分支名= master：当前分支（缺省）git pull origin dev #省略本地分支名=dev：当前分支 fetch&amp;&amp;merge#拉取远程 master分支中本地没有的内容（即其他开发者push的内容）git fetch origin master #拉取的分支名为\" origin/原始分支名“git merge origin/ master #把拉取下来的 master分支的内容合并到本地库中的分支上#拉取所有分支的的内容（本地没有的，其他开发者push的内容）（假定有分支：dev2，dev3）git fetch origin git checkout dev2 并 git merge origin/dev2 #切换到dev2分支，并合并拉取下来的内容git checkout dev3 并 git merge origin/dev3git checkout dev2 并 git diff origin/dev2 #切换到dev2分支，比较拉取的内容中的dev2分支和本地dev2分支的不同","categories":[{"name":"工具","slug":"工具","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"工具/Git","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://hofe.work/tags/Git/"}]},{"title":"一篇入门docker","slug":"微服务分布式集群/一篇入门docker","date":"2019-10-05T16:00:00.000Z","updated":"2020-04-27T06:52:28.094Z","comments":true,"path":"2019/10/06/微服务分布式集群/一篇入门docker/","link":"","permalink":"http://hofe.work/2019/10/06/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4/%E4%B8%80%E7%AF%87%E5%85%A5%E9%97%A8docker/","excerpt":"","text":"一篇学会Docker 这里给出几个好的学习资源分享给大家一起学习 本文也是参考如下课程编写，转载请注明出处 docker入门：https://www.bilibili.com/video/av55377411/ docker详细：https://www.bilibili.com/video/av27122140/ Docker简介1.1 什么是DockerDocker将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了Docker，就不用担心环境问题。 总体来说，Docker的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。1.2 容器与虚拟机比较下面的图片比较了 Docker 和传统虚拟化方式的不同之处，可见 容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，而传统方式则是在硬件层面实现。与传统的虚拟机相比，Docker优势体现为启动速度快、占用体积小。 启动快：启动容器相当于启动本机的一个进程，而不是启动一个操作系统，速度就快很多。 资源占用少：容器只占用需要的资源，不占用那些没有用到的资源；虚拟机由于是完整的操作系统，不可避免要占用所以资源。另外，多个容器可以共享资源，虚拟机都是独享资源。 体积小：容器只要包含用到的组件即可，而虚拟机是整个操作系统的打包，所以容器文件比虚拟机文件要小很多。 1.3 Docker 组件1.3.1 Docker镜像与容器、仓库镜像是构建Docker的基石。用户基于镜像来运行自己的容器。镜像也是Docker生命周期中的“构建”部分。 容器是基于镜像启动起来的，容器中可以运行一个或多个进程。我们可以认为，镜像是Docker生命周期中的构建或者打包阶段，而容器则是启动或者执行阶段。 docker 面向对象 容器 对象 镜像 类 概念 解释 镜像(Images) Docker 镜像是用于创建 Docker 容器的模板。 容器(Container) 容器是独立运行的一个或一组应用。 仓库(Registry) Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。Docker Hub(https://hub.docker.com) 提供了庞大的镜像集合供使用。 Docker安装与启动2.1 安装Docker Docker官方建议在Ubuntu中安装，因为Docker是基于Ubuntu发布的，而且一般Docker出现的问题Ubuntu是最先更新或者打补丁的。在很多版本的CentOS中是不支持更新最新的一些补丁包的。 这部分我会单独提供一份基于Ubuntu的Nvidia-docker安装与启动 可以点击这里查看 2.3 Docker的启动与停止systemctl命令是系统服务管理器指令启动docker： systemctl start docker 停止docker： systemctl stop docker 重启docker： systemctl restart docker 查看docker状态： systemctl status docker 开机启动： systemctl enable docker 查看docker概要信息 docker info 查看docker帮助文档 docker --help 常用命令3.1 镜像相关命令3.1.1 查看镜像docker images REPOSITORY：镜像名称 TAG：镜像标签 IMAGE ID：镜像ID CREATED：镜像的创建日期（不是获取该镜像的日期） SIZE：镜像大小 这些镜像都是存储在Docker宿主机的/var/lib/docker目录下 3.1.2 搜索镜像如果你需要从网络中查找需要的镜像，可以通过以下命令搜索 docker search 镜像名称 也可以从镜像库中查找： https://hub.docker.com/ NAME：仓库名称 DESCRIPTION：镜像描述 STARS：用户评价，反应一个镜像的受欢迎程度 OFFICIAL：是否官方 AUTOMATED：自动构建，表示该镜像由Docker Hub自动构建流程创建的 3.1.3 拉取镜像拉取镜像就是从中央仓库中下载镜像到本地 docker pull 镜像名称(默认最新) 例如，我要下载centos7镜像(指定版本) docker pull centos:7 3.1.4 删除镜像按镜像ID删除镜像 docker rmi 镜像ID 删除所有镜像 docker rmi &#96;docker images -q&#96; docker images –q:查询所有镜像的ID，并作为rmi的参数 3.2 容器相关命令3.2.1 查看容器查看正在运行的容器 docker ps 查看所有容器 docker ps –a 查看最后一次运行的容器 docker ps –l 查看停止的容器 docker ps -f status&#x3D;exited 3.2.2 创建与启动容器创建容器常用的参数说明：创建容器命令： docker run -i：表示运行容器 -t：表示容器启动后会进入其命令行。加入这两个参数后，容器创建就能登录进去。即分配一个伪终端。 –name :为创建的容器命名。 -v：表示目录映射关系（前者是宿主机目录，后者是映射到宿主机上的目录），可以使用多个－v做多个目录或文件映射。注意：最好做目录映射，在宿主机上做修改，然后共享到容器上。 -d：在run后面加上-d参数,则会创建一个守护式容器在后台运行（这样创建容器后不会自动登录容器，如果只加-i -t两个参数，创建后就会自动进去容器）。 -p：表示端口映射，前者是宿主机端口，后者是容器内的映射端口。可以使用多个-p做多个端口映射 （1）交互式方式创建容器交互式：前台打开，exit时关机 docker run -it --name&#x3D;容器名称 镜像名称:标签 &#x2F;bin&#x2F;bash（进入命令行） 这时我们通过ps命令查看，发现可以看到启动的容器，状态为启动状态 退出当前容器 exit （2）守护式方式创建容器：守护式：后台运行 docker run -di --name&#x3D;容器名称 镜像名称:标签 登录守护式容器方式： docker exec -it 容器名称 (或者容器ID) &#x2F;bin&#x2F;bash exit不会直接关机而是后台继续运行 3.2.3 停止与启动容器停止容器： docker stop 容器名称（或者容器ID） 启动容器：docker start 容器名称（或者容器ID） 3.2.4 文件拷贝如果我们需要将文件拷贝到容器内可以使用cp命令 docker cp 需要拷贝的文件或目录 容器名称:容器目录 也可以将文件从容器内拷贝出来 docker cp 容器名称:容器目录 需要拷贝的文件或目录 3.2.5 目录挂载（共享目录）我们可以在创建容器的时候，将宿主机的目录与容器内的目录进行映射，这样我们就可以通过修改宿主机某个目录的文件从而去影响容器。创建容器 添加-v参数 后边为 宿主机目录:容器目录，例如： docker run -di -v &#x2F;usr&#x2F;local&#x2F;myhtml:&#x2F;usr&#x2F;local&#x2F;myhtml --name&#x3D;mycentos3 centos:7 容器目录：宿主机目录 3.2.6 查看容器IP地址我们可以通过以下命令查看容器运行的各种数据 docker inspect 容器名称（容器ID） 也可以直接执行下面的命令直接输出IP地址 docker inspect --format&#x3D;&#39;&#123;&#123;.NetworkSettings.IPAddress&#125;&#125;&#39; 容器名称（容器ID） 3.2.7 删除容器删除指定的容器：（需先停止） docker rm 容器名称（容器ID） 应用部署4.1 MySQL部署（1）拉取mysql镜像 docker pull centos&#x2F;mysql-57-centos7 （2）创建容器 docker run -di --name&#x3D;tensquare_mysql -p 33306:3306 -e MYSQL_ROOT_PASSWORD&#x3D;123456 mysql -p 代表端口映射，格式为 宿主机映射端口:容器运行端口 -e 代表添加环境变量 MYSQL_ROOT_PASSWORD 是root用户的登陆密码 （3）远程登录mysql连接宿主机的IP ,指定端口为33306 4.2 tomcat部署（1）拉取镜像 docker pull tomcat:7-jre7 （2）创建容器创建容器 -p表示地址映射 docker run -di --name&#x3D;mytomcat -p 9000:8080 -v &#x2F;usr&#x2F;local&#x2F;webapps:&#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;webapps tomcat:7-jre7 4.3 Nginx部署（1）拉取镜像 docker pull nginx （2）创建Nginx容器 docker run -di --name&#x3D;mynginx -p 80:80 nginx 4.4 Redis部署（1）拉取镜像 docker pull redis （2）创建容器 docker run -di --name&#x3D;myredis -p 6379:6379 redis 迁移与备份5.1 容器保存为镜像我们可以通过以下命令将容器保存为镜像 docker commit mynginx mynginx_i 通过保存的mynginx_i镜像创建基于mynginx_i的容器mynginx2 5.2 镜像备份我们可以通过以下命令将镜像保存为tar 文件 docker save -o mynginx.tar mynginx_i 5.3 镜像恢复与迁移首先我们先删除掉mynginx_img镜像 然后执行此命令进行恢复 docker load -i mynginx.tar -i 输入的文件 执行后再次查看镜像，可以看到镜像已经恢复 Dockerfile6.1 什么是DockerfileDockerfile是由一系列命令和参数构成的脚本，这些命令应用于基础镜像并最终创建一个新的镜像。 对于开发人员：可以为开发团队提供一个完全一致的开发环境； 对于测试人员：可以直接拿开发时所构建的镜像或者通过Dockerfile文件构建一个新的镜像开始工作了； 对于运维人员：在部署时，可以实现应用的无缝移植。 6.2 常用命令 6.1 使用脚本创建镜像步骤：（1）创建目录 mkdir –p &#x2F;usr&#x2F;local&#x2F;dockerjdk8 （2）下载jdk-8u171-linux-x64.tar.gz并上传到服务器（虚拟机）中的/usr/local/dockerjdk8目录 （3）创建文件Dockerfile vi Dockerfile #依赖镜像名称和IDFROM centos:7#指定镜像创建者信息MAINTAINER ITCAST#切换工作目录WORKDIR /usrRUN mkdir /usr/local/java#ADD 是相对路径jar,把java添加到容器中ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/#配置java环境变量ENV JAVA_HOME /usr/local/java/jdk1.8.0_171ENV JRE_HOME $JAVA_HOME/jreENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATHENV PATH $JAVA_HOME/bin:$PATH （4）执行命令构建镜像 docker build -t&#x3D;&#39;jdk1.8&#39; . 注意后边的空格和点，不要省略 （5）查看镜像是否建立完成 docker images Docker私有仓库7.1 私有仓库搭建与配置（1）拉取私有仓库镜像（企业仓库） docker pull registry （2）启动私有仓库容器 docker run -di --name&#x3D;registry -p 5000:5000 registry （3）打开浏览器 输入地址http://192.168.184.141:5000/v2/_catalog看到{&quot;repositories&quot;:[]} 表示私有仓库搭建成功并且内容为空 （4）修改daemon.json vi &#x2F;etc&#x2F;docker&#x2F;daemon.json 添加以下内容，保存退出。{“insecure-registries”:[“192.168.184.141:5000”]} 此步用于让 docker信任私有仓库地址（5）重启docker 服务 systemctl restart docker 7.2 镜像上传至私有仓库（1）标记此镜像为私有仓库的镜像 docker tag jdk1.8 192.168.184.141:5000&#x2F;jdk1.8 （2）再次启动私服容器 docker start registry （3）上传标记的镜像 docker push 192.168.184.141:5000&#x2F;jdk1.8 最后，本文也参考了一些文章，因为也是第一次写博客，忘记将之前参考过的文章保存起来，如果大家有看到我引用没有标明的地方，请告知，谢谢！在学习的道路上，与君共勉。","categories":[{"name":"集群","slug":"集群","permalink":"http://hofe.work/categories/%E9%9B%86%E7%BE%A4/"},{"name":"docker","slug":"集群/docker","permalink":"http://hofe.work/categories/%E9%9B%86%E7%BE%A4/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://hofe.work/tags/docker/"}]}],"categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"项目","slug":"项目","permalink":"http://hofe.work/categories/%E9%A1%B9%E7%9B%AE/"},{"name":"小程序","slug":"项目/小程序","permalink":"http://hofe.work/categories/%E9%A1%B9%E7%9B%AE/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"Java","slug":"Java","permalink":"http://hofe.work/categories/Java/"},{"name":"List","slug":"Java/List","permalink":"http://hofe.work/categories/Java/List/"},{"name":"Python","slug":"Python","permalink":"http://hofe.work/categories/Python/"},{"name":"数据分析","slug":"Python/数据分析","permalink":"http://hofe.work/categories/Python/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"},{"name":"基础","slug":"Java/基础","permalink":"http://hofe.work/categories/Java/%E5%9F%BA%E7%A1%80/"},{"name":"分布式","slug":"分布式","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"MQ","slug":"分布式/MQ","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/MQ/"},{"name":"锁","slug":"Java/锁","permalink":"http://hofe.work/categories/Java/%E9%94%81/"},{"name":"SpringBoot","slug":"Java/SpringBoot","permalink":"http://hofe.work/categories/Java/SpringBoot/"},{"name":"Spring","slug":"Java/Spring","permalink":"http://hofe.work/categories/Java/Spring/"},{"name":"IO","slug":"Java/IO","permalink":"http://hofe.work/categories/Java/IO/"},{"name":"算法","slug":"算法","permalink":"http://hofe.work/categories/%E7%AE%97%E6%B3%95/"},{"name":"工具","slug":"工具","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/"},{"name":"Git","slug":"工具/Git","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/Git/"},{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Oracle","slug":"数据库/Oracle","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Oracle/"},{"name":"网络","slug":"网络","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/"},{"name":"TCP/IP","slug":"网络/TCP-IP","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/TCP-IP/"},{"name":"HTTP","slug":"网络/HTTP","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/HTTP/"},{"name":"设计模式","slug":"Java/设计模式","permalink":"http://hofe.work/categories/Java/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"秒杀","slug":"分布式/秒杀","permalink":"http://hofe.work/categories/%E5%88%86%E5%B8%83%E5%BC%8F/%E7%A7%92%E6%9D%80/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL/"},{"name":"Redis","slug":"数据库/Redis","permalink":"http://hofe.work/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/Redis/"},{"name":"JVM","slug":"Java/JVM","permalink":"http://hofe.work/categories/Java/JVM/"},{"name":"Map","slug":"Java/Map","permalink":"http://hofe.work/categories/Java/Map/"},{"name":"markdown","slug":"工具/markdown","permalink":"http://hofe.work/categories/%E5%B7%A5%E5%85%B7/markdown/"},{"name":"HTTPS","slug":"网络/HTTPS","permalink":"http://hofe.work/categories/%E7%BD%91%E7%BB%9C/HTTPS/"},{"name":"多线程","slug":"Java/多线程","permalink":"http://hofe.work/categories/Java/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"微服务","slug":"微服务","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Dubbo","slug":"微服务/Dubbo","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Dubbo/"},{"name":"入门实战","slug":"微服务/Dubbo/入门实战","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Dubbo/%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98/"},{"name":"Spring Cloud","slug":"微服务/Spring-Cloud","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/"},{"name":"入门实战","slug":"微服务/Spring-Cloud/入门实战","permalink":"http://hofe.work/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/Spring-Cloud/%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98/"},{"name":"后端框架","slug":"后端框架","permalink":"http://hofe.work/categories/%E5%90%8E%E7%AB%AF%E6%A1%86%E6%9E%B6/"},{"name":"集群","slug":"集群","permalink":"http://hofe.work/categories/%E9%9B%86%E7%BE%A4/"},{"name":"docker","slug":"集群/docker","permalink":"http://hofe.work/categories/%E9%9B%86%E7%BE%A4/docker/"}],"tags":[{"name":"进制","slug":"进制","permalink":"http://hofe.work/tags/%E8%BF%9B%E5%88%B6/"},{"name":"小程序","slug":"小程序","permalink":"http://hofe.work/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"ArrayList","slug":"ArrayList","permalink":"http://hofe.work/tags/ArrayList/"},{"name":"时间序列","slug":"时间序列","permalink":"http://hofe.work/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"},{"name":"基础","slug":"基础","permalink":"http://hofe.work/tags/%E5%9F%BA%E7%A1%80/"},{"name":"线程","slug":"线程","permalink":"http://hofe.work/tags/%E7%BA%BF%E7%A8%8B/"},{"name":"进程","slug":"进程","permalink":"http://hofe.work/tags/%E8%BF%9B%E7%A8%8B/"},{"name":"C","slug":"C","permalink":"http://hofe.work/tags/C/"},{"name":"转载","slug":"转载","permalink":"http://hofe.work/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"MQ","slug":"MQ","permalink":"http://hofe.work/tags/MQ/"},{"name":"Lock","slug":"Lock","permalink":"http://hofe.work/tags/Lock/"},{"name":"AQS","slug":"AQS","permalink":"http://hofe.work/tags/AQS/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://hofe.work/tags/SpringBoot/"},{"name":"Spring","slug":"Spring","permalink":"http://hofe.work/tags/Spring/"},{"name":"数组","slug":"数组","permalink":"http://hofe.work/tags/%E6%95%B0%E7%BB%84/"},{"name":"链表","slug":"链表","permalink":"http://hofe.work/tags/%E9%93%BE%E8%A1%A8/"},{"name":"SSO","slug":"SSO","permalink":"http://hofe.work/tags/SSO/"},{"name":"深浅拷贝","slug":"深浅拷贝","permalink":"http://hofe.work/tags/%E6%B7%B1%E6%B5%85%E6%8B%B7%E8%B4%9D/"},{"name":"IO","slug":"IO","permalink":"http://hofe.work/tags/IO/"},{"name":"synchronized","slug":"synchronized","permalink":"http://hofe.work/tags/synchronized/"},{"name":"快排","slug":"快排","permalink":"http://hofe.work/tags/%E5%BF%AB%E6%8E%92/"},{"name":"迭代","slug":"迭代","permalink":"http://hofe.work/tags/%E8%BF%AD%E4%BB%A3/"},{"name":"递归","slug":"递归","permalink":"http://hofe.work/tags/%E9%80%92%E5%BD%92/"},{"name":"动态规划","slug":"动态规划","permalink":"http://hofe.work/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"Git","slug":"Git","permalink":"http://hofe.work/tags/Git/"},{"name":"Oracle","slug":"Oracle","permalink":"http://hofe.work/tags/Oracle/"},{"name":"DTree","slug":"DTree","permalink":"http://hofe.work/tags/DTree/"},{"name":"KNN","slug":"KNN","permalink":"http://hofe.work/tags/KNN/"},{"name":"回归","slug":"回归","permalink":"http://hofe.work/tags/%E5%9B%9E%E5%BD%92/"},{"name":"秒杀","slug":"秒杀","permalink":"http://hofe.work/tags/%E7%A7%92%E6%9D%80/"},{"name":"TCP","slug":"TCP","permalink":"http://hofe.work/tags/TCP/"},{"name":"UDP","slug":"UDP","permalink":"http://hofe.work/tags/UDP/"},{"name":"Session","slug":"Session","permalink":"http://hofe.work/tags/Session/"},{"name":"Cookie","slug":"Cookie","permalink":"http://hofe.work/tags/Cookie/"},{"name":"设计模式","slug":"设计模式","permalink":"http://hofe.work/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"MySQL","slug":"MySQL","permalink":"http://hofe.work/tags/MySQL/"},{"name":"Redis","slug":"Redis","permalink":"http://hofe.work/tags/Redis/"},{"name":"贪心","slug":"贪心","permalink":"http://hofe.work/tags/%E8%B4%AA%E5%BF%83/"},{"name":"栈","slug":"栈","permalink":"http://hofe.work/tags/%E6%A0%88/"},{"name":"队列","slug":"队列","permalink":"http://hofe.work/tags/%E9%98%9F%E5%88%97/"},{"name":"JVM","slug":"JVM","permalink":"http://hofe.work/tags/JVM/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","permalink":"http://hofe.work/tags/ConcurrentHashMap/"},{"name":"HashMap","slug":"HashMap","permalink":"http://hofe.work/tags/HashMap/"},{"name":"排序","slug":"排序","permalink":"http://hofe.work/tags/%E6%8E%92%E5%BA%8F/"},{"name":"markdown","slug":"markdown","permalink":"http://hofe.work/tags/markdown/"},{"name":"数据库","slug":"数据库","permalink":"http://hofe.work/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"HTTPS","slug":"HTTPS","permalink":"http://hofe.work/tags/HTTPS/"},{"name":"树","slug":"树","permalink":"http://hofe.work/tags/%E6%A0%91/"},{"name":"HTTP","slug":"HTTP","permalink":"http://hofe.work/tags/HTTP/"},{"name":"多线程","slug":"多线程","permalink":"http://hofe.work/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Dubbo","slug":"Dubbo","permalink":"http://hofe.work/tags/Dubbo/"},{"name":"Ribbon","slug":"Ribbon","permalink":"http://hofe.work/tags/Ribbon/"},{"name":"Spring Cloud","slug":"Spring-Cloud","permalink":"http://hofe.work/tags/Spring-Cloud/"},{"name":"Eureka","slug":"Eureka","permalink":"http://hofe.work/tags/Eureka/"},{"name":"生产者消费者","slug":"生产者消费者","permalink":"http://hofe.work/tags/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85/"},{"name":"kmp","slug":"kmp","permalink":"http://hofe.work/tags/kmp/"},{"name":"docker","slug":"docker","permalink":"http://hofe.work/tags/docker/"}]}